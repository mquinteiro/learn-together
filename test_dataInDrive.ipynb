{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test-dataInDrive.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mquinteiro/learn-together/blob/master/test_dataInDrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52aD8uaa93yn",
        "colab_type": "text"
      },
      "source": [
        "#Import modules and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZvLqYSywTqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
        "config.gpu_options.allow_growth=True\n",
        "set_session(tf.Session(config=config))\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzvvQsoy-CPp",
        "colab_type": "text"
      },
      "source": [
        "# Load kaggle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOusdsNFynjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()  #this will prompt you to upload the kaggle.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhQYewtv4Ait",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "32bbe0b9-99e5-46e6-a446-ab967d90304b"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission\n",
        "!kaggle competitions download -c learn-together -p input/\n",
        "\n",
        "!unzip input/test.csv.zip -d input/\n",
        "!unzip input/train.csv.zip -d input/\n",
        "\n",
        "print(os.listdir('./input'))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "['train.csv.zip', 'sample_submission.csv.zip', 'test.csv.zip']\n",
            "Archive:  input/sample_submission.csv.zip\n",
            "caution: filename not matched:  input/test.csv.zip\n",
            "caution: filename not matched:  input/train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKeVQKb9-KN5",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVkS24-j5YL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the data\n",
        "X = pd.read_csv('./input/train.csv', index_col='Id') \n",
        "X_test = pd.read_csv('./input/test.csv', index_col='Id')\n",
        "\n",
        "\n",
        "y_enc = X['Cover_Type']\n",
        "X.drop('Cover_Type', axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc3LPqya8PEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove unwanted columns\n",
        "to_drop_columns = ['Soil_Type7','Soil_Type8','Soil_Type8','Soil_Type15','Soil_Type27','Soil_Type28','Soil_Type36']\n",
        "X.drop(to_drop_columns, axis=1, inplace=True)\n",
        "X_test.drop(to_drop_columns, axis=1, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRKfP-mF8XTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1094a26-65c6-4a95-a87f-58e5407df4f2"
      },
      "source": [
        "# normalize data input\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer,MaxAbsScaler\n",
        "#scaler = StandardScaler()\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X_norm = scaler.fit_transform(X)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# create encoder to encode labels\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_enc)\n",
        "y = np_utils.to_categorical(encoder.transform(y_enc))\n",
        "print(y.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15120, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU185uJR8ZBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7ee2bab2-45e5-4ea5-92dc-b5974837675c"
      },
      "source": [
        "print(np.min(X_norm[:3],axis=0))\n",
        "print(y[0:3])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.26787513 -0.71666667 -0.92307692 -0.68428891 -0.6        -0.88679245\n",
            "  0.73228346  0.71612903  0.08870968  0.75060775  1.         -1.\n",
            " -1.         -1.         -1.         -1.         -1.         -1.\n",
            " -1.         -1.         -1.         -1.         -1.         -1.\n",
            " -1.         -1.         -1.         -1.         -1.         -1.\n",
            " -1.         -1.         -1.         -1.         -1.         -1.\n",
            " -1.         -1.         -1.         -1.         -1.         -1.\n",
            " -1.         -1.         -1.         -1.         -1.         -1.        ]\n",
            "[[0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRNPdJ9a8dVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split in  training and validating data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_norm, y,\n",
        "                                                      train_size=0.8, test_size=0.2,\n",
        "                                                      random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcKkgmqM-T_G",
        "colab_type": "text"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-SY-GvN9GhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    drop_val=0.2\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100,  activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(drop_val))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(drop_val))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(drop_val))\n",
        "    model.add(Dense(7, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #8515\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy']) #\n",
        "    return model\n",
        "\n",
        "\n",
        "estimator = KerasClassifier(\n",
        "  build_fn=create_model, epochs=500, batch_size=2500, verbose=1)\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "kfold = KFold(n_splits=4, shuffle=True, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ_8I4yH9Hfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping \n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=1000)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQx120yj-ZiK",
        "colab_type": "text"
      },
      "source": [
        "# Execute model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyZkwm8L9LkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17005091-d63b-43fd-b42f-ee6ba85bba65"
      },
      "source": [
        "# Note: even TPU is so slowwwwwww around 5 times slower than my GPU. Verbose=0 could speed it up?\n",
        "\n",
        "model = create_model()\n",
        "history2=model.fit(x=X_train, y=y_train,epochs=7500, validation_data=(X_valid, y_valid),batch_size=12096,callbacks=callbacks)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0902 17:27:09.196075 140021476185984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0902 17:27:09.204369 140021476185984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0902 17:27:09.225627 140021476185984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0902 17:27:09.227793 140021476185984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0902 17:27:09.299590 140021476185984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0902 17:27:09.322335 140021476185984 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0902 17:27:09.550425 140021476185984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0902 17:27:09.658582 140021476185984 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 12096 samples, validate on 3024 samples\n",
            "Epoch 1/7500\n",
            "12096/12096 [==============================] - 1s 118us/step - loss: 2.8458 - acc: 0.1415 - val_loss: 2.1617 - val_acc: 0.2097\n",
            "Epoch 2/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 2.6181 - acc: 0.1814 - val_loss: 1.8903 - val_acc: 0.2735\n",
            "Epoch 3/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 2.3638 - acc: 0.2181 - val_loss: 1.6663 - val_acc: 0.3912\n",
            "Epoch 4/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 2.1425 - acc: 0.2636 - val_loss: 1.4961 - val_acc: 0.4673\n",
            "Epoch 5/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.9486 - acc: 0.3131 - val_loss: 1.3744 - val_acc: 0.4987\n",
            "Epoch 6/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.7912 - acc: 0.3642 - val_loss: 1.2921 - val_acc: 0.5205\n",
            "Epoch 7/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.6739 - acc: 0.4131 - val_loss: 1.2352 - val_acc: 0.5360\n",
            "Epoch 8/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.5655 - acc: 0.4430 - val_loss: 1.1958 - val_acc: 0.5453\n",
            "Epoch 9/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.4970 - acc: 0.4632 - val_loss: 1.1672 - val_acc: 0.5618\n",
            "Epoch 10/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.4469 - acc: 0.4794 - val_loss: 1.1457 - val_acc: 0.5675\n",
            "Epoch 11/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.4098 - acc: 0.4946 - val_loss: 1.1287 - val_acc: 0.5685\n",
            "Epoch 12/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.3602 - acc: 0.5070 - val_loss: 1.1135 - val_acc: 0.5718\n",
            "Epoch 13/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.3316 - acc: 0.5123 - val_loss: 1.1002 - val_acc: 0.5780\n",
            "Epoch 14/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.3041 - acc: 0.5233 - val_loss: 1.0868 - val_acc: 0.5827\n",
            "Epoch 15/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.2644 - acc: 0.5379 - val_loss: 1.0739 - val_acc: 0.5893\n",
            "Epoch 16/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.2544 - acc: 0.5343 - val_loss: 1.0614 - val_acc: 0.5966\n",
            "Epoch 17/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.2307 - acc: 0.5451 - val_loss: 1.0498 - val_acc: 0.5992\n",
            "Epoch 18/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.2179 - acc: 0.5411 - val_loss: 1.0391 - val_acc: 0.6052\n",
            "Epoch 19/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.1970 - acc: 0.5463 - val_loss: 1.0292 - val_acc: 0.6055\n",
            "Epoch 20/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.1871 - acc: 0.5504 - val_loss: 1.0203 - val_acc: 0.6075\n",
            "Epoch 21/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.1641 - acc: 0.5531 - val_loss: 1.0120 - val_acc: 0.6104\n",
            "Epoch 22/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.1463 - acc: 0.5570 - val_loss: 1.0046 - val_acc: 0.6065\n",
            "Epoch 23/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.1318 - acc: 0.5622 - val_loss: 0.9979 - val_acc: 0.6108\n",
            "Epoch 24/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.1202 - acc: 0.5634 - val_loss: 0.9922 - val_acc: 0.6101\n",
            "Epoch 25/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 1.1129 - acc: 0.5686 - val_loss: 0.9859 - val_acc: 0.6118\n",
            "Epoch 26/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 1.0976 - acc: 0.5681 - val_loss: 0.9792 - val_acc: 0.6157\n",
            "Epoch 27/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 1.0918 - acc: 0.5618 - val_loss: 0.9722 - val_acc: 0.6187\n",
            "Epoch 28/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0771 - acc: 0.5692 - val_loss: 0.9657 - val_acc: 0.6247\n",
            "Epoch 29/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.0791 - acc: 0.5689 - val_loss: 0.9595 - val_acc: 0.6283\n",
            "Epoch 30/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 1.0732 - acc: 0.5702 - val_loss: 0.9532 - val_acc: 0.6323\n",
            "Epoch 31/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.0580 - acc: 0.5728 - val_loss: 0.9474 - val_acc: 0.6339\n",
            "Epoch 32/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0405 - acc: 0.5774 - val_loss: 0.9417 - val_acc: 0.6349\n",
            "Epoch 33/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0416 - acc: 0.5804 - val_loss: 0.9362 - val_acc: 0.6359\n",
            "Epoch 34/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0349 - acc: 0.5783 - val_loss: 0.9310 - val_acc: 0.6376\n",
            "Epoch 35/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.0141 - acc: 0.5880 - val_loss: 0.9256 - val_acc: 0.6399\n",
            "Epoch 36/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 1.0267 - acc: 0.5874 - val_loss: 0.9204 - val_acc: 0.6415\n",
            "Epoch 37/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0227 - acc: 0.5832 - val_loss: 0.9152 - val_acc: 0.6432\n",
            "Epoch 38/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0109 - acc: 0.5852 - val_loss: 0.9100 - val_acc: 0.6452\n",
            "Epoch 39/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 1.0066 - acc: 0.5879 - val_loss: 0.9050 - val_acc: 0.6462\n",
            "Epoch 40/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9906 - acc: 0.5957 - val_loss: 0.9003 - val_acc: 0.6498\n",
            "Epoch 41/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.9948 - acc: 0.5935 - val_loss: 0.8961 - val_acc: 0.6501\n",
            "Epoch 42/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9910 - acc: 0.5887 - val_loss: 0.8920 - val_acc: 0.6505\n",
            "Epoch 43/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9835 - acc: 0.5931 - val_loss: 0.8880 - val_acc: 0.6515\n",
            "Epoch 44/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9809 - acc: 0.5990 - val_loss: 0.8842 - val_acc: 0.6528\n",
            "Epoch 45/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9644 - acc: 0.6014 - val_loss: 0.8806 - val_acc: 0.6531\n",
            "Epoch 46/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9788 - acc: 0.5978 - val_loss: 0.8772 - val_acc: 0.6531\n",
            "Epoch 47/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9648 - acc: 0.6033 - val_loss: 0.8744 - val_acc: 0.6561\n",
            "Epoch 48/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9622 - acc: 0.6016 - val_loss: 0.8717 - val_acc: 0.6541\n",
            "Epoch 49/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9620 - acc: 0.5982 - val_loss: 0.8693 - val_acc: 0.6558\n",
            "Epoch 50/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9575 - acc: 0.6047 - val_loss: 0.8673 - val_acc: 0.6554\n",
            "Epoch 51/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9555 - acc: 0.6071 - val_loss: 0.8656 - val_acc: 0.6564\n",
            "Epoch 52/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9485 - acc: 0.6078 - val_loss: 0.8642 - val_acc: 0.6551\n",
            "Epoch 53/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.9454 - acc: 0.6091 - val_loss: 0.8630 - val_acc: 0.6534\n",
            "Epoch 54/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9411 - acc: 0.6110 - val_loss: 0.8624 - val_acc: 0.6544\n",
            "Epoch 55/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.9413 - acc: 0.6143 - val_loss: 0.8614 - val_acc: 0.6558\n",
            "Epoch 56/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9347 - acc: 0.6114 - val_loss: 0.8603 - val_acc: 0.6564\n",
            "Epoch 57/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9303 - acc: 0.6148 - val_loss: 0.8591 - val_acc: 0.6558\n",
            "Epoch 58/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9235 - acc: 0.6147 - val_loss: 0.8577 - val_acc: 0.6561\n",
            "Epoch 59/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9216 - acc: 0.6141 - val_loss: 0.8562 - val_acc: 0.6581\n",
            "Epoch 60/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9244 - acc: 0.6149 - val_loss: 0.8545 - val_acc: 0.6594\n",
            "Epoch 61/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9239 - acc: 0.6160 - val_loss: 0.8523 - val_acc: 0.6581\n",
            "Epoch 62/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9140 - acc: 0.6179 - val_loss: 0.8501 - val_acc: 0.6571\n",
            "Epoch 63/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.9140 - acc: 0.6193 - val_loss: 0.8475 - val_acc: 0.6581\n",
            "Epoch 64/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.9166 - acc: 0.6179 - val_loss: 0.8445 - val_acc: 0.6594\n",
            "Epoch 65/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8963 - acc: 0.6274 - val_loss: 0.8420 - val_acc: 0.6587\n",
            "Epoch 66/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.9159 - acc: 0.6149 - val_loss: 0.8392 - val_acc: 0.6614\n",
            "Epoch 67/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8976 - acc: 0.6209 - val_loss: 0.8368 - val_acc: 0.6640\n",
            "Epoch 68/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8908 - acc: 0.6289 - val_loss: 0.8350 - val_acc: 0.6644\n",
            "Epoch 69/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.9004 - acc: 0.6201 - val_loss: 0.8339 - val_acc: 0.6667\n",
            "Epoch 70/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8940 - acc: 0.6260 - val_loss: 0.8330 - val_acc: 0.6663\n",
            "Epoch 71/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8791 - acc: 0.6286 - val_loss: 0.8326 - val_acc: 0.6650\n",
            "Epoch 72/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8925 - acc: 0.6220 - val_loss: 0.8323 - val_acc: 0.6644\n",
            "Epoch 73/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8774 - acc: 0.6326 - val_loss: 0.8313 - val_acc: 0.6634\n",
            "Epoch 74/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8751 - acc: 0.6361 - val_loss: 0.8299 - val_acc: 0.6637\n",
            "Epoch 75/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.8792 - acc: 0.6316 - val_loss: 0.8269 - val_acc: 0.6657\n",
            "Epoch 76/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.8835 - acc: 0.6335 - val_loss: 0.8231 - val_acc: 0.6677\n",
            "Epoch 77/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8751 - acc: 0.6351 - val_loss: 0.8189 - val_acc: 0.6680\n",
            "Epoch 78/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8730 - acc: 0.6321 - val_loss: 0.8156 - val_acc: 0.6693\n",
            "Epoch 79/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.8727 - acc: 0.6337 - val_loss: 0.8130 - val_acc: 0.6690\n",
            "Epoch 80/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8584 - acc: 0.6365 - val_loss: 0.8110 - val_acc: 0.6696\n",
            "Epoch 81/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8657 - acc: 0.6385 - val_loss: 0.8094 - val_acc: 0.6723\n",
            "Epoch 82/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8578 - acc: 0.6390 - val_loss: 0.8073 - val_acc: 0.6746\n",
            "Epoch 83/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8627 - acc: 0.6350 - val_loss: 0.8043 - val_acc: 0.6753\n",
            "Epoch 84/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.8574 - acc: 0.6361 - val_loss: 0.8018 - val_acc: 0.6772\n",
            "Epoch 85/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8581 - acc: 0.6381 - val_loss: 0.7991 - val_acc: 0.6782\n",
            "Epoch 86/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.8538 - acc: 0.6451 - val_loss: 0.7966 - val_acc: 0.6812\n",
            "Epoch 87/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.8551 - acc: 0.6391 - val_loss: 0.7947 - val_acc: 0.6835\n",
            "Epoch 88/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.8483 - acc: 0.6412 - val_loss: 0.7931 - val_acc: 0.6835\n",
            "Epoch 89/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8437 - acc: 0.6462 - val_loss: 0.7923 - val_acc: 0.6832\n",
            "Epoch 90/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8464 - acc: 0.6419 - val_loss: 0.7912 - val_acc: 0.6835\n",
            "Epoch 91/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.8356 - acc: 0.6492 - val_loss: 0.7903 - val_acc: 0.6825\n",
            "Epoch 92/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8370 - acc: 0.6467 - val_loss: 0.7873 - val_acc: 0.6839\n",
            "Epoch 93/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8394 - acc: 0.6482 - val_loss: 0.7839 - val_acc: 0.6862\n",
            "Epoch 94/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8305 - acc: 0.6485 - val_loss: 0.7808 - val_acc: 0.6875\n",
            "Epoch 95/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8378 - acc: 0.6470 - val_loss: 0.7778 - val_acc: 0.6905\n",
            "Epoch 96/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8235 - acc: 0.6536 - val_loss: 0.7757 - val_acc: 0.6911\n",
            "Epoch 97/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8291 - acc: 0.6466 - val_loss: 0.7749 - val_acc: 0.6921\n",
            "Epoch 98/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8266 - acc: 0.6497 - val_loss: 0.7740 - val_acc: 0.6918\n",
            "Epoch 99/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8255 - acc: 0.6477 - val_loss: 0.7730 - val_acc: 0.6898\n",
            "Epoch 100/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8220 - acc: 0.6502 - val_loss: 0.7720 - val_acc: 0.6911\n",
            "Epoch 101/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.8250 - acc: 0.6510 - val_loss: 0.7698 - val_acc: 0.6938\n",
            "Epoch 102/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8206 - acc: 0.6531 - val_loss: 0.7671 - val_acc: 0.6954\n",
            "Epoch 103/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8105 - acc: 0.6570 - val_loss: 0.7649 - val_acc: 0.6964\n",
            "Epoch 104/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8227 - acc: 0.6535 - val_loss: 0.7635 - val_acc: 0.6987\n",
            "Epoch 105/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8158 - acc: 0.6553 - val_loss: 0.7623 - val_acc: 0.6997\n",
            "Epoch 106/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8080 - acc: 0.6577 - val_loss: 0.7624 - val_acc: 0.7011\n",
            "Epoch 107/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8137 - acc: 0.6553 - val_loss: 0.7620 - val_acc: 0.7027\n",
            "Epoch 108/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8035 - acc: 0.6595 - val_loss: 0.7609 - val_acc: 0.7021\n",
            "Epoch 109/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7995 - acc: 0.6634 - val_loss: 0.7591 - val_acc: 0.7030\n",
            "Epoch 110/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8050 - acc: 0.6642 - val_loss: 0.7570 - val_acc: 0.7040\n",
            "Epoch 111/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8024 - acc: 0.6627 - val_loss: 0.7545 - val_acc: 0.7044\n",
            "Epoch 112/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8064 - acc: 0.6585 - val_loss: 0.7521 - val_acc: 0.7040\n",
            "Epoch 113/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7952 - acc: 0.6611 - val_loss: 0.7497 - val_acc: 0.7047\n",
            "Epoch 114/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7951 - acc: 0.6654 - val_loss: 0.7474 - val_acc: 0.7047\n",
            "Epoch 115/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.8007 - acc: 0.6646 - val_loss: 0.7452 - val_acc: 0.7060\n",
            "Epoch 116/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7966 - acc: 0.6603 - val_loss: 0.7430 - val_acc: 0.7080\n",
            "Epoch 117/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7950 - acc: 0.6640 - val_loss: 0.7407 - val_acc: 0.7080\n",
            "Epoch 118/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7978 - acc: 0.6628 - val_loss: 0.7389 - val_acc: 0.7090\n",
            "Epoch 119/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7936 - acc: 0.6641 - val_loss: 0.7371 - val_acc: 0.7097\n",
            "Epoch 120/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7867 - acc: 0.6662 - val_loss: 0.7352 - val_acc: 0.7130\n",
            "Epoch 121/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7855 - acc: 0.6666 - val_loss: 0.7338 - val_acc: 0.7149\n",
            "Epoch 122/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7883 - acc: 0.6686 - val_loss: 0.7337 - val_acc: 0.7149\n",
            "Epoch 123/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.7720 - acc: 0.6683 - val_loss: 0.7339 - val_acc: 0.7126\n",
            "Epoch 124/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7807 - acc: 0.6633 - val_loss: 0.7336 - val_acc: 0.7130\n",
            "Epoch 125/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7824 - acc: 0.6682 - val_loss: 0.7323 - val_acc: 0.7126\n",
            "Epoch 126/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7801 - acc: 0.6663 - val_loss: 0.7301 - val_acc: 0.7130\n",
            "Epoch 127/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7789 - acc: 0.6734 - val_loss: 0.7276 - val_acc: 0.7130\n",
            "Epoch 128/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7780 - acc: 0.6679 - val_loss: 0.7251 - val_acc: 0.7146\n",
            "Epoch 129/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7711 - acc: 0.6731 - val_loss: 0.7233 - val_acc: 0.7136\n",
            "Epoch 130/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7700 - acc: 0.6739 - val_loss: 0.7209 - val_acc: 0.7143\n",
            "Epoch 131/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.7763 - acc: 0.6684 - val_loss: 0.7185 - val_acc: 0.7133\n",
            "Epoch 132/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7688 - acc: 0.6742 - val_loss: 0.7177 - val_acc: 0.7126\n",
            "Epoch 133/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7708 - acc: 0.6729 - val_loss: 0.7172 - val_acc: 0.7136\n",
            "Epoch 134/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7669 - acc: 0.6773 - val_loss: 0.7166 - val_acc: 0.7143\n",
            "Epoch 135/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7558 - acc: 0.6792 - val_loss: 0.7158 - val_acc: 0.7130\n",
            "Epoch 136/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7548 - acc: 0.6833 - val_loss: 0.7146 - val_acc: 0.7130\n",
            "Epoch 137/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7632 - acc: 0.6768 - val_loss: 0.7125 - val_acc: 0.7140\n",
            "Epoch 138/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7633 - acc: 0.6786 - val_loss: 0.7103 - val_acc: 0.7176\n",
            "Epoch 139/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7564 - acc: 0.6793 - val_loss: 0.7082 - val_acc: 0.7176\n",
            "Epoch 140/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7551 - acc: 0.6797 - val_loss: 0.7066 - val_acc: 0.7173\n",
            "Epoch 141/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.7500 - acc: 0.6830 - val_loss: 0.7058 - val_acc: 0.7196\n",
            "Epoch 142/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.7461 - acc: 0.6892 - val_loss: 0.7061 - val_acc: 0.7199\n",
            "Epoch 143/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7504 - acc: 0.6844 - val_loss: 0.7072 - val_acc: 0.7202\n",
            "Epoch 144/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7443 - acc: 0.6837 - val_loss: 0.7072 - val_acc: 0.7196\n",
            "Epoch 145/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.7516 - acc: 0.6811 - val_loss: 0.7059 - val_acc: 0.7176\n",
            "Epoch 146/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7505 - acc: 0.6821 - val_loss: 0.7043 - val_acc: 0.7163\n",
            "Epoch 147/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7487 - acc: 0.6848 - val_loss: 0.7017 - val_acc: 0.7169\n",
            "Epoch 148/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7484 - acc: 0.6838 - val_loss: 0.6985 - val_acc: 0.7169\n",
            "Epoch 149/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7423 - acc: 0.6872 - val_loss: 0.6960 - val_acc: 0.7189\n",
            "Epoch 150/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7375 - acc: 0.6891 - val_loss: 0.6952 - val_acc: 0.7183\n",
            "Epoch 151/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7391 - acc: 0.6913 - val_loss: 0.6950 - val_acc: 0.7202\n",
            "Epoch 152/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.7346 - acc: 0.6916 - val_loss: 0.6949 - val_acc: 0.7173\n",
            "Epoch 153/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.7310 - acc: 0.6926 - val_loss: 0.6940 - val_acc: 0.7169\n",
            "Epoch 154/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7379 - acc: 0.6899 - val_loss: 0.6930 - val_acc: 0.7176\n",
            "Epoch 155/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7355 - acc: 0.6906 - val_loss: 0.6923 - val_acc: 0.7176\n",
            "Epoch 156/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.7310 - acc: 0.6896 - val_loss: 0.6920 - val_acc: 0.7186\n",
            "Epoch 157/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7350 - acc: 0.6910 - val_loss: 0.6921 - val_acc: 0.7176\n",
            "Epoch 158/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7247 - acc: 0.6939 - val_loss: 0.6916 - val_acc: 0.7169\n",
            "Epoch 159/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7299 - acc: 0.6938 - val_loss: 0.6898 - val_acc: 0.7169\n",
            "Epoch 160/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.7255 - acc: 0.6918 - val_loss: 0.6869 - val_acc: 0.7149\n",
            "Epoch 161/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7321 - acc: 0.6878 - val_loss: 0.6844 - val_acc: 0.7173\n",
            "Epoch 162/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7304 - acc: 0.6900 - val_loss: 0.6841 - val_acc: 0.7169\n",
            "Epoch 163/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7194 - acc: 0.6948 - val_loss: 0.6845 - val_acc: 0.7156\n",
            "Epoch 164/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7168 - acc: 0.6954 - val_loss: 0.6845 - val_acc: 0.7159\n",
            "Epoch 165/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7190 - acc: 0.6958 - val_loss: 0.6840 - val_acc: 0.7159\n",
            "Epoch 166/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7193 - acc: 0.6948 - val_loss: 0.6823 - val_acc: 0.7173\n",
            "Epoch 167/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.7192 - acc: 0.6911 - val_loss: 0.6803 - val_acc: 0.7186\n",
            "Epoch 168/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7105 - acc: 0.7040 - val_loss: 0.6794 - val_acc: 0.7199\n",
            "Epoch 169/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7158 - acc: 0.6977 - val_loss: 0.6792 - val_acc: 0.7196\n",
            "Epoch 170/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7066 - acc: 0.7018 - val_loss: 0.6800 - val_acc: 0.7186\n",
            "Epoch 171/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7154 - acc: 0.7007 - val_loss: 0.6803 - val_acc: 0.7173\n",
            "Epoch 172/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7114 - acc: 0.7044 - val_loss: 0.6809 - val_acc: 0.7183\n",
            "Epoch 173/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7065 - acc: 0.7045 - val_loss: 0.6807 - val_acc: 0.7192\n",
            "Epoch 174/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7067 - acc: 0.7015 - val_loss: 0.6805 - val_acc: 0.7206\n",
            "Epoch 175/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.7139 - acc: 0.6956 - val_loss: 0.6807 - val_acc: 0.7169\n",
            "Epoch 176/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7056 - acc: 0.7028 - val_loss: 0.6809 - val_acc: 0.7163\n",
            "Epoch 177/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7012 - acc: 0.7074 - val_loss: 0.6826 - val_acc: 0.7136\n",
            "Epoch 178/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7030 - acc: 0.7062 - val_loss: 0.6849 - val_acc: 0.7110\n",
            "Epoch 179/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6979 - acc: 0.7070 - val_loss: 0.6880 - val_acc: 0.7110\n",
            "Epoch 180/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7001 - acc: 0.7040 - val_loss: 0.6943 - val_acc: 0.7067\n",
            "Epoch 181/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.7022 - acc: 0.7024 - val_loss: 0.7003 - val_acc: 0.7057\n",
            "Epoch 182/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7029 - acc: 0.7037 - val_loss: 0.7030 - val_acc: 0.7030\n",
            "Epoch 183/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6960 - acc: 0.7094 - val_loss: 0.7011 - val_acc: 0.7021\n",
            "Epoch 184/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7027 - acc: 0.7046 - val_loss: 0.6977 - val_acc: 0.7047\n",
            "Epoch 185/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.7012 - acc: 0.7039 - val_loss: 0.6974 - val_acc: 0.7057\n",
            "Epoch 186/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6930 - acc: 0.7126 - val_loss: 0.7006 - val_acc: 0.7027\n",
            "Epoch 187/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6955 - acc: 0.7087 - val_loss: 0.7079 - val_acc: 0.6958\n",
            "Epoch 188/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.6911 - acc: 0.7129 - val_loss: 0.7138 - val_acc: 0.6951\n",
            "Epoch 189/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6940 - acc: 0.7076 - val_loss: 0.7212 - val_acc: 0.6931\n",
            "Epoch 190/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6896 - acc: 0.7102 - val_loss: 0.7302 - val_acc: 0.6882\n",
            "Epoch 191/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6842 - acc: 0.7130 - val_loss: 0.7411 - val_acc: 0.6822\n",
            "Epoch 192/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6806 - acc: 0.7181 - val_loss: 0.7586 - val_acc: 0.6743\n",
            "Epoch 193/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.6826 - acc: 0.7140 - val_loss: 0.7783 - val_acc: 0.6663\n",
            "Epoch 194/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6864 - acc: 0.7104 - val_loss: 0.7949 - val_acc: 0.6594\n",
            "Epoch 195/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6845 - acc: 0.7110 - val_loss: 0.8051 - val_acc: 0.6577\n",
            "Epoch 196/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6909 - acc: 0.7122 - val_loss: 0.8059 - val_acc: 0.6581\n",
            "Epoch 197/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6828 - acc: 0.7135 - val_loss: 0.8070 - val_acc: 0.6564\n",
            "Epoch 198/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6789 - acc: 0.7131 - val_loss: 0.8051 - val_acc: 0.6567\n",
            "Epoch 199/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6910 - acc: 0.7108 - val_loss: 0.8097 - val_acc: 0.6561\n",
            "Epoch 200/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6885 - acc: 0.7098 - val_loss: 0.8213 - val_acc: 0.6478\n",
            "Epoch 201/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6747 - acc: 0.7157 - val_loss: 0.8276 - val_acc: 0.6498\n",
            "Epoch 202/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6768 - acc: 0.7183 - val_loss: 0.8325 - val_acc: 0.6515\n",
            "Epoch 203/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6796 - acc: 0.7154 - val_loss: 0.8254 - val_acc: 0.6548\n",
            "Epoch 204/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6754 - acc: 0.7121 - val_loss: 0.8147 - val_acc: 0.6538\n",
            "Epoch 205/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6782 - acc: 0.7137 - val_loss: 0.8107 - val_acc: 0.6534\n",
            "Epoch 206/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6788 - acc: 0.7135 - val_loss: 0.8172 - val_acc: 0.6521\n",
            "Epoch 207/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6727 - acc: 0.7139 - val_loss: 0.8317 - val_acc: 0.6481\n",
            "Epoch 208/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6699 - acc: 0.7195 - val_loss: 0.8164 - val_acc: 0.6508\n",
            "Epoch 209/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6726 - acc: 0.7145 - val_loss: 0.7903 - val_acc: 0.6601\n",
            "Epoch 210/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6713 - acc: 0.7186 - val_loss: 0.7848 - val_acc: 0.6624\n",
            "Epoch 211/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6598 - acc: 0.7264 - val_loss: 0.7974 - val_acc: 0.6581\n",
            "Epoch 212/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6609 - acc: 0.7213 - val_loss: 0.8189 - val_acc: 0.6544\n",
            "Epoch 213/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6734 - acc: 0.7137 - val_loss: 0.8402 - val_acc: 0.6468\n",
            "Epoch 214/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6640 - acc: 0.7174 - val_loss: 0.8284 - val_acc: 0.6541\n",
            "Epoch 215/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6661 - acc: 0.7209 - val_loss: 0.8012 - val_acc: 0.6617\n",
            "Epoch 216/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6652 - acc: 0.7214 - val_loss: 0.7803 - val_acc: 0.6690\n",
            "Epoch 217/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6642 - acc: 0.7267 - val_loss: 0.7753 - val_acc: 0.6706\n",
            "Epoch 218/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6620 - acc: 0.7224 - val_loss: 0.7752 - val_acc: 0.6693\n",
            "Epoch 219/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6648 - acc: 0.7223 - val_loss: 0.7919 - val_acc: 0.6627\n",
            "Epoch 220/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6600 - acc: 0.7226 - val_loss: 0.8077 - val_acc: 0.6567\n",
            "Epoch 221/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6597 - acc: 0.7230 - val_loss: 0.7952 - val_acc: 0.6601\n",
            "Epoch 222/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6584 - acc: 0.7217 - val_loss: 0.7756 - val_acc: 0.6706\n",
            "Epoch 223/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6632 - acc: 0.7173 - val_loss: 0.7603 - val_acc: 0.6749\n",
            "Epoch 224/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6598 - acc: 0.7188 - val_loss: 0.7538 - val_acc: 0.6789\n",
            "Epoch 225/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6571 - acc: 0.7259 - val_loss: 0.7589 - val_acc: 0.6786\n",
            "Epoch 226/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6581 - acc: 0.7239 - val_loss: 0.7783 - val_acc: 0.6753\n",
            "Epoch 227/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6514 - acc: 0.7268 - val_loss: 0.8031 - val_acc: 0.6653\n",
            "Epoch 228/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6532 - acc: 0.7267 - val_loss: 0.8139 - val_acc: 0.6604\n",
            "Epoch 229/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6563 - acc: 0.7269 - val_loss: 0.7977 - val_acc: 0.6624\n",
            "Epoch 230/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6490 - acc: 0.7288 - val_loss: 0.7679 - val_acc: 0.6720\n",
            "Epoch 231/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6478 - acc: 0.7245 - val_loss: 0.7386 - val_acc: 0.6852\n",
            "Epoch 232/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6519 - acc: 0.7262 - val_loss: 0.7237 - val_acc: 0.6944\n",
            "Epoch 233/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6513 - acc: 0.7255 - val_loss: 0.7291 - val_acc: 0.6888\n",
            "Epoch 234/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6477 - acc: 0.7239 - val_loss: 0.7487 - val_acc: 0.6832\n",
            "Epoch 235/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6518 - acc: 0.7247 - val_loss: 0.7666 - val_acc: 0.6713\n",
            "Epoch 236/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6519 - acc: 0.7252 - val_loss: 0.7740 - val_acc: 0.6700\n",
            "Epoch 237/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6489 - acc: 0.7270 - val_loss: 0.7701 - val_acc: 0.6706\n",
            "Epoch 238/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6433 - acc: 0.7269 - val_loss: 0.7506 - val_acc: 0.6796\n",
            "Epoch 239/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6496 - acc: 0.7298 - val_loss: 0.7337 - val_acc: 0.6875\n",
            "Epoch 240/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6476 - acc: 0.7281 - val_loss: 0.7259 - val_acc: 0.6915\n",
            "Epoch 241/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6440 - acc: 0.7285 - val_loss: 0.7246 - val_acc: 0.6901\n",
            "Epoch 242/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6510 - acc: 0.7296 - val_loss: 0.7256 - val_acc: 0.6888\n",
            "Epoch 243/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6451 - acc: 0.7287 - val_loss: 0.7241 - val_acc: 0.6885\n",
            "Epoch 244/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6422 - acc: 0.7328 - val_loss: 0.7227 - val_acc: 0.6882\n",
            "Epoch 245/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6477 - acc: 0.7305 - val_loss: 0.7166 - val_acc: 0.6921\n",
            "Epoch 246/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6384 - acc: 0.7338 - val_loss: 0.7113 - val_acc: 0.6981\n",
            "Epoch 247/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6414 - acc: 0.7348 - val_loss: 0.7099 - val_acc: 0.6984\n",
            "Epoch 248/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6392 - acc: 0.7348 - val_loss: 0.7145 - val_acc: 0.6968\n",
            "Epoch 249/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6420 - acc: 0.7293 - val_loss: 0.7308 - val_acc: 0.6901\n",
            "Epoch 250/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6377 - acc: 0.7354 - val_loss: 0.7524 - val_acc: 0.6812\n",
            "Epoch 251/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6424 - acc: 0.7270 - val_loss: 0.7609 - val_acc: 0.6786\n",
            "Epoch 252/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6410 - acc: 0.7334 - val_loss: 0.7547 - val_acc: 0.6802\n",
            "Epoch 253/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6443 - acc: 0.7309 - val_loss: 0.7396 - val_acc: 0.6839\n",
            "Epoch 254/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6267 - acc: 0.7354 - val_loss: 0.7145 - val_acc: 0.6958\n",
            "Epoch 255/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6379 - acc: 0.7363 - val_loss: 0.7056 - val_acc: 0.6987\n",
            "Epoch 256/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6367 - acc: 0.7330 - val_loss: 0.7086 - val_acc: 0.6991\n",
            "Epoch 257/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6360 - acc: 0.7397 - val_loss: 0.7220 - val_acc: 0.6935\n",
            "Epoch 258/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6358 - acc: 0.7329 - val_loss: 0.7315 - val_acc: 0.6885\n",
            "Epoch 259/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.6291 - acc: 0.7374 - val_loss: 0.7291 - val_acc: 0.6935\n",
            "Epoch 260/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.6378 - acc: 0.7352 - val_loss: 0.7235 - val_acc: 0.6978\n",
            "Epoch 261/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6276 - acc: 0.7371 - val_loss: 0.7115 - val_acc: 0.6987\n",
            "Epoch 262/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6343 - acc: 0.7357 - val_loss: 0.7060 - val_acc: 0.7004\n",
            "Epoch 263/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6293 - acc: 0.7359 - val_loss: 0.7125 - val_acc: 0.6944\n",
            "Epoch 264/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6310 - acc: 0.7345 - val_loss: 0.7280 - val_acc: 0.6901\n",
            "Epoch 265/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6270 - acc: 0.7355 - val_loss: 0.7326 - val_acc: 0.6872\n",
            "Epoch 266/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6280 - acc: 0.7362 - val_loss: 0.7314 - val_acc: 0.6895\n",
            "Epoch 267/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6349 - acc: 0.7307 - val_loss: 0.7214 - val_acc: 0.6944\n",
            "Epoch 268/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6272 - acc: 0.7344 - val_loss: 0.7124 - val_acc: 0.7007\n",
            "Epoch 269/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6295 - acc: 0.7354 - val_loss: 0.7071 - val_acc: 0.7001\n",
            "Epoch 270/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6260 - acc: 0.7355 - val_loss: 0.7063 - val_acc: 0.7021\n",
            "Epoch 271/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6242 - acc: 0.7404 - val_loss: 0.7144 - val_acc: 0.6958\n",
            "Epoch 272/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6208 - acc: 0.7425 - val_loss: 0.7136 - val_acc: 0.6964\n",
            "Epoch 273/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6236 - acc: 0.7420 - val_loss: 0.7091 - val_acc: 0.7017\n",
            "Epoch 274/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6227 - acc: 0.7364 - val_loss: 0.7081 - val_acc: 0.7017\n",
            "Epoch 275/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6261 - acc: 0.7365 - val_loss: 0.7117 - val_acc: 0.7027\n",
            "Epoch 276/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6152 - acc: 0.7451 - val_loss: 0.7154 - val_acc: 0.6971\n",
            "Epoch 277/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6165 - acc: 0.7397 - val_loss: 0.7198 - val_acc: 0.6925\n",
            "Epoch 278/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6161 - acc: 0.7462 - val_loss: 0.7162 - val_acc: 0.6931\n",
            "Epoch 279/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6201 - acc: 0.7418 - val_loss: 0.7115 - val_acc: 0.6961\n",
            "Epoch 280/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6243 - acc: 0.7395 - val_loss: 0.7056 - val_acc: 0.7017\n",
            "Epoch 281/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6209 - acc: 0.7388 - val_loss: 0.7010 - val_acc: 0.7050\n",
            "Epoch 282/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6167 - acc: 0.7415 - val_loss: 0.7006 - val_acc: 0.7070\n",
            "Epoch 283/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6280 - acc: 0.7368 - val_loss: 0.7061 - val_acc: 0.7037\n",
            "Epoch 284/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6222 - acc: 0.7411 - val_loss: 0.6989 - val_acc: 0.7057\n",
            "Epoch 285/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6175 - acc: 0.7436 - val_loss: 0.6890 - val_acc: 0.7100\n",
            "Epoch 286/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6143 - acc: 0.7383 - val_loss: 0.6863 - val_acc: 0.7110\n",
            "Epoch 287/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6227 - acc: 0.7378 - val_loss: 0.6843 - val_acc: 0.7136\n",
            "Epoch 288/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6229 - acc: 0.7384 - val_loss: 0.6885 - val_acc: 0.7100\n",
            "Epoch 289/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6136 - acc: 0.7398 - val_loss: 0.6953 - val_acc: 0.7050\n",
            "Epoch 290/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6225 - acc: 0.7404 - val_loss: 0.6893 - val_acc: 0.7067\n",
            "Epoch 291/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6122 - acc: 0.7443 - val_loss: 0.6739 - val_acc: 0.7153\n",
            "Epoch 292/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.6087 - acc: 0.7442 - val_loss: 0.6621 - val_acc: 0.7255\n",
            "Epoch 293/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6086 - acc: 0.7450 - val_loss: 0.6602 - val_acc: 0.7288\n",
            "Epoch 294/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6225 - acc: 0.7393 - val_loss: 0.6717 - val_acc: 0.7196\n",
            "Epoch 295/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6077 - acc: 0.7440 - val_loss: 0.6881 - val_acc: 0.7163\n",
            "Epoch 296/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6137 - acc: 0.7463 - val_loss: 0.6940 - val_acc: 0.7093\n",
            "Epoch 297/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6093 - acc: 0.7448 - val_loss: 0.6791 - val_acc: 0.7146\n",
            "Epoch 298/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6082 - acc: 0.7457 - val_loss: 0.6617 - val_acc: 0.7222\n",
            "Epoch 299/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.6089 - acc: 0.7426 - val_loss: 0.6577 - val_acc: 0.7269\n",
            "Epoch 300/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6086 - acc: 0.7474 - val_loss: 0.6660 - val_acc: 0.7229\n",
            "Epoch 301/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6025 - acc: 0.7478 - val_loss: 0.6721 - val_acc: 0.7226\n",
            "Epoch 302/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6075 - acc: 0.7451 - val_loss: 0.6751 - val_acc: 0.7199\n",
            "Epoch 303/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6058 - acc: 0.7476 - val_loss: 0.6681 - val_acc: 0.7229\n",
            "Epoch 304/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6088 - acc: 0.7443 - val_loss: 0.6511 - val_acc: 0.7252\n",
            "Epoch 305/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6077 - acc: 0.7452 - val_loss: 0.6367 - val_acc: 0.7348\n",
            "Epoch 306/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6137 - acc: 0.7438 - val_loss: 0.6319 - val_acc: 0.7384\n",
            "Epoch 307/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6052 - acc: 0.7433 - val_loss: 0.6384 - val_acc: 0.7345\n",
            "Epoch 308/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6087 - acc: 0.7448 - val_loss: 0.6518 - val_acc: 0.7278\n",
            "Epoch 309/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6016 - acc: 0.7499 - val_loss: 0.6647 - val_acc: 0.7206\n",
            "Epoch 310/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6044 - acc: 0.7442 - val_loss: 0.6663 - val_acc: 0.7206\n",
            "Epoch 311/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6080 - acc: 0.7481 - val_loss: 0.6639 - val_acc: 0.7232\n",
            "Epoch 312/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5992 - acc: 0.7488 - val_loss: 0.6560 - val_acc: 0.7282\n",
            "Epoch 313/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5998 - acc: 0.7543 - val_loss: 0.6498 - val_acc: 0.7325\n",
            "Epoch 314/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6057 - acc: 0.7455 - val_loss: 0.6557 - val_acc: 0.7298\n",
            "Epoch 315/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6005 - acc: 0.7493 - val_loss: 0.6646 - val_acc: 0.7232\n",
            "Epoch 316/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6053 - acc: 0.7428 - val_loss: 0.6631 - val_acc: 0.7209\n",
            "Epoch 317/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.6019 - acc: 0.7479 - val_loss: 0.6527 - val_acc: 0.7255\n",
            "Epoch 318/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5983 - acc: 0.7502 - val_loss: 0.6414 - val_acc: 0.7321\n",
            "Epoch 319/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5974 - acc: 0.7474 - val_loss: 0.6333 - val_acc: 0.7361\n",
            "Epoch 320/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.6008 - acc: 0.7516 - val_loss: 0.6294 - val_acc: 0.7354\n",
            "Epoch 321/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5997 - acc: 0.7505 - val_loss: 0.6345 - val_acc: 0.7348\n",
            "Epoch 322/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6015 - acc: 0.7501 - val_loss: 0.6450 - val_acc: 0.7312\n",
            "Epoch 323/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6014 - acc: 0.7513 - val_loss: 0.6525 - val_acc: 0.7278\n",
            "Epoch 324/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6003 - acc: 0.7499 - val_loss: 0.6538 - val_acc: 0.7285\n",
            "Epoch 325/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5985 - acc: 0.7497 - val_loss: 0.6532 - val_acc: 0.7288\n",
            "Epoch 326/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5948 - acc: 0.7487 - val_loss: 0.6434 - val_acc: 0.7321\n",
            "Epoch 327/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5942 - acc: 0.7555 - val_loss: 0.6338 - val_acc: 0.7384\n",
            "Epoch 328/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5907 - acc: 0.7569 - val_loss: 0.6280 - val_acc: 0.7388\n",
            "Epoch 329/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5912 - acc: 0.7531 - val_loss: 0.6314 - val_acc: 0.7364\n",
            "Epoch 330/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5935 - acc: 0.7546 - val_loss: 0.6417 - val_acc: 0.7288\n",
            "Epoch 331/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.6020 - acc: 0.7493 - val_loss: 0.6455 - val_acc: 0.7285\n",
            "Epoch 332/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5896 - acc: 0.7532 - val_loss: 0.6454 - val_acc: 0.7269\n",
            "Epoch 333/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5939 - acc: 0.7513 - val_loss: 0.6375 - val_acc: 0.7298\n",
            "Epoch 334/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5961 - acc: 0.7517 - val_loss: 0.6288 - val_acc: 0.7358\n",
            "Epoch 335/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5910 - acc: 0.7525 - val_loss: 0.6247 - val_acc: 0.7374\n",
            "Epoch 336/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5884 - acc: 0.7513 - val_loss: 0.6323 - val_acc: 0.7335\n",
            "Epoch 337/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5904 - acc: 0.7551 - val_loss: 0.6442 - val_acc: 0.7278\n",
            "Epoch 338/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5939 - acc: 0.7507 - val_loss: 0.6432 - val_acc: 0.7312\n",
            "Epoch 339/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5833 - acc: 0.7611 - val_loss: 0.6320 - val_acc: 0.7374\n",
            "Epoch 340/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5942 - acc: 0.7528 - val_loss: 0.6239 - val_acc: 0.7417\n",
            "Epoch 341/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5831 - acc: 0.7568 - val_loss: 0.6221 - val_acc: 0.7391\n",
            "Epoch 342/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5901 - acc: 0.7531 - val_loss: 0.6252 - val_acc: 0.7364\n",
            "Epoch 343/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5904 - acc: 0.7570 - val_loss: 0.6313 - val_acc: 0.7335\n",
            "Epoch 344/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5862 - acc: 0.7554 - val_loss: 0.6312 - val_acc: 0.7341\n",
            "Epoch 345/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5909 - acc: 0.7536 - val_loss: 0.6249 - val_acc: 0.7378\n",
            "Epoch 346/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5855 - acc: 0.7565 - val_loss: 0.6171 - val_acc: 0.7421\n",
            "Epoch 347/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5892 - acc: 0.7569 - val_loss: 0.6090 - val_acc: 0.7477\n",
            "Epoch 348/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5857 - acc: 0.7548 - val_loss: 0.6060 - val_acc: 0.7503\n",
            "Epoch 349/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5838 - acc: 0.7530 - val_loss: 0.6046 - val_acc: 0.7480\n",
            "Epoch 350/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5841 - acc: 0.7567 - val_loss: 0.6077 - val_acc: 0.7464\n",
            "Epoch 351/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5843 - acc: 0.7562 - val_loss: 0.6135 - val_acc: 0.7437\n",
            "Epoch 352/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5864 - acc: 0.7536 - val_loss: 0.6181 - val_acc: 0.7421\n",
            "Epoch 353/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5854 - acc: 0.7599 - val_loss: 0.6188 - val_acc: 0.7437\n",
            "Epoch 354/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5828 - acc: 0.7599 - val_loss: 0.6128 - val_acc: 0.7457\n",
            "Epoch 355/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5815 - acc: 0.7593 - val_loss: 0.6054 - val_acc: 0.7490\n",
            "Epoch 356/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5846 - acc: 0.7544 - val_loss: 0.6009 - val_acc: 0.7500\n",
            "Epoch 357/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5748 - acc: 0.7626 - val_loss: 0.6034 - val_acc: 0.7460\n",
            "Epoch 358/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5735 - acc: 0.7628 - val_loss: 0.6123 - val_acc: 0.7417\n",
            "Epoch 359/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5745 - acc: 0.7623 - val_loss: 0.6262 - val_acc: 0.7368\n",
            "Epoch 360/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5765 - acc: 0.7606 - val_loss: 0.6344 - val_acc: 0.7338\n",
            "Epoch 361/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5769 - acc: 0.7579 - val_loss: 0.6259 - val_acc: 0.7371\n",
            "Epoch 362/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5792 - acc: 0.7585 - val_loss: 0.6105 - val_acc: 0.7427\n",
            "Epoch 363/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5758 - acc: 0.7631 - val_loss: 0.5982 - val_acc: 0.7517\n",
            "Epoch 364/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5764 - acc: 0.7588 - val_loss: 0.5919 - val_acc: 0.7560\n",
            "Epoch 365/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5823 - acc: 0.7593 - val_loss: 0.5925 - val_acc: 0.7560\n",
            "Epoch 366/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5690 - acc: 0.7694 - val_loss: 0.6041 - val_acc: 0.7507\n",
            "Epoch 367/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5766 - acc: 0.7583 - val_loss: 0.6140 - val_acc: 0.7421\n",
            "Epoch 368/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5771 - acc: 0.7607 - val_loss: 0.6149 - val_acc: 0.7384\n",
            "Epoch 369/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5749 - acc: 0.7628 - val_loss: 0.6081 - val_acc: 0.7388\n",
            "Epoch 370/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5736 - acc: 0.7646 - val_loss: 0.6046 - val_acc: 0.7424\n",
            "Epoch 371/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5742 - acc: 0.7611 - val_loss: 0.6038 - val_acc: 0.7464\n",
            "Epoch 372/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5749 - acc: 0.7595 - val_loss: 0.6049 - val_acc: 0.7467\n",
            "Epoch 373/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5736 - acc: 0.7598 - val_loss: 0.6021 - val_acc: 0.7503\n",
            "Epoch 374/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5818 - acc: 0.7596 - val_loss: 0.5974 - val_acc: 0.7540\n",
            "Epoch 375/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5714 - acc: 0.7619 - val_loss: 0.5965 - val_acc: 0.7520\n",
            "Epoch 376/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5719 - acc: 0.7650 - val_loss: 0.5952 - val_acc: 0.7533\n",
            "Epoch 377/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5764 - acc: 0.7606 - val_loss: 0.5961 - val_acc: 0.7523\n",
            "Epoch 378/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5661 - acc: 0.7646 - val_loss: 0.6028 - val_acc: 0.7474\n",
            "Epoch 379/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5618 - acc: 0.7683 - val_loss: 0.6072 - val_acc: 0.7421\n",
            "Epoch 380/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5682 - acc: 0.7650 - val_loss: 0.6078 - val_acc: 0.7384\n",
            "Epoch 381/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5701 - acc: 0.7620 - val_loss: 0.6046 - val_acc: 0.7437\n",
            "Epoch 382/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5706 - acc: 0.7626 - val_loss: 0.5953 - val_acc: 0.7480\n",
            "Epoch 383/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5714 - acc: 0.7607 - val_loss: 0.5836 - val_acc: 0.7550\n",
            "Epoch 384/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5646 - acc: 0.7674 - val_loss: 0.5781 - val_acc: 0.7589\n",
            "Epoch 385/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5666 - acc: 0.7647 - val_loss: 0.5805 - val_acc: 0.7589\n",
            "Epoch 386/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5645 - acc: 0.7612 - val_loss: 0.5870 - val_acc: 0.7573\n",
            "Epoch 387/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5662 - acc: 0.7618 - val_loss: 0.5955 - val_acc: 0.7517\n",
            "Epoch 388/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5766 - acc: 0.7598 - val_loss: 0.6005 - val_acc: 0.7510\n",
            "Epoch 389/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5636 - acc: 0.7642 - val_loss: 0.5975 - val_acc: 0.7523\n",
            "Epoch 390/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5705 - acc: 0.7641 - val_loss: 0.5928 - val_acc: 0.7526\n",
            "Epoch 391/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5658 - acc: 0.7617 - val_loss: 0.5924 - val_acc: 0.7530\n",
            "Epoch 392/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5635 - acc: 0.7646 - val_loss: 0.5956 - val_acc: 0.7490\n",
            "Epoch 393/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5680 - acc: 0.7630 - val_loss: 0.5946 - val_acc: 0.7460\n",
            "Epoch 394/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5648 - acc: 0.7664 - val_loss: 0.5935 - val_acc: 0.7490\n",
            "Epoch 395/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5625 - acc: 0.7679 - val_loss: 0.5903 - val_acc: 0.7517\n",
            "Epoch 396/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5545 - acc: 0.7698 - val_loss: 0.5826 - val_acc: 0.7573\n",
            "Epoch 397/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5638 - acc: 0.7655 - val_loss: 0.5756 - val_acc: 0.7652\n",
            "Epoch 398/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5689 - acc: 0.7690 - val_loss: 0.5758 - val_acc: 0.7649\n",
            "Epoch 399/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5657 - acc: 0.7622 - val_loss: 0.5787 - val_acc: 0.7622\n",
            "Epoch 400/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5597 - acc: 0.7667 - val_loss: 0.5803 - val_acc: 0.7586\n",
            "Epoch 401/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5679 - acc: 0.7682 - val_loss: 0.5815 - val_acc: 0.7583\n",
            "Epoch 402/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5565 - acc: 0.7715 - val_loss: 0.5823 - val_acc: 0.7596\n",
            "Epoch 403/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5640 - acc: 0.7676 - val_loss: 0.5808 - val_acc: 0.7603\n",
            "Epoch 404/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5633 - acc: 0.7684 - val_loss: 0.5739 - val_acc: 0.7665\n",
            "Epoch 405/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5546 - acc: 0.7689 - val_loss: 0.5725 - val_acc: 0.7646\n",
            "Epoch 406/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5649 - acc: 0.7641 - val_loss: 0.5767 - val_acc: 0.7632\n",
            "Epoch 407/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5533 - acc: 0.7722 - val_loss: 0.5777 - val_acc: 0.7672\n",
            "Epoch 408/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5569 - acc: 0.7682 - val_loss: 0.5810 - val_acc: 0.7649\n",
            "Epoch 409/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5553 - acc: 0.7693 - val_loss: 0.5830 - val_acc: 0.7619\n",
            "Epoch 410/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5604 - acc: 0.7688 - val_loss: 0.5850 - val_acc: 0.7593\n",
            "Epoch 411/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5542 - acc: 0.7736 - val_loss: 0.5803 - val_acc: 0.7685\n",
            "Epoch 412/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5489 - acc: 0.7752 - val_loss: 0.5699 - val_acc: 0.7728\n",
            "Epoch 413/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5525 - acc: 0.7685 - val_loss: 0.5629 - val_acc: 0.7748\n",
            "Epoch 414/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5519 - acc: 0.7724 - val_loss: 0.5616 - val_acc: 0.7731\n",
            "Epoch 415/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5585 - acc: 0.7710 - val_loss: 0.5680 - val_acc: 0.7672\n",
            "Epoch 416/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5483 - acc: 0.7742 - val_loss: 0.5797 - val_acc: 0.7612\n",
            "Epoch 417/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5568 - acc: 0.7736 - val_loss: 0.5873 - val_acc: 0.7586\n",
            "Epoch 418/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5520 - acc: 0.7727 - val_loss: 0.5835 - val_acc: 0.7609\n",
            "Epoch 419/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5518 - acc: 0.7733 - val_loss: 0.5733 - val_acc: 0.7669\n",
            "Epoch 420/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5550 - acc: 0.7711 - val_loss: 0.5647 - val_acc: 0.7728\n",
            "Epoch 421/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5550 - acc: 0.7727 - val_loss: 0.5599 - val_acc: 0.7761\n",
            "Epoch 422/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5600 - acc: 0.7679 - val_loss: 0.5578 - val_acc: 0.7738\n",
            "Epoch 423/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5585 - acc: 0.7661 - val_loss: 0.5622 - val_acc: 0.7722\n",
            "Epoch 424/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5485 - acc: 0.7779 - val_loss: 0.5677 - val_acc: 0.7662\n",
            "Epoch 425/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5547 - acc: 0.7702 - val_loss: 0.5710 - val_acc: 0.7672\n",
            "Epoch 426/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5546 - acc: 0.7714 - val_loss: 0.5695 - val_acc: 0.7705\n",
            "Epoch 427/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5477 - acc: 0.7729 - val_loss: 0.5712 - val_acc: 0.7705\n",
            "Epoch 428/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5494 - acc: 0.7765 - val_loss: 0.5709 - val_acc: 0.7685\n",
            "Epoch 429/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5448 - acc: 0.7774 - val_loss: 0.5679 - val_acc: 0.7741\n",
            "Epoch 430/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5484 - acc: 0.7747 - val_loss: 0.5614 - val_acc: 0.7748\n",
            "Epoch 431/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5496 - acc: 0.7716 - val_loss: 0.5591 - val_acc: 0.7781\n",
            "Epoch 432/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5498 - acc: 0.7732 - val_loss: 0.5616 - val_acc: 0.7768\n",
            "Epoch 433/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5393 - acc: 0.7766 - val_loss: 0.5650 - val_acc: 0.7728\n",
            "Epoch 434/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5488 - acc: 0.7703 - val_loss: 0.5665 - val_acc: 0.7705\n",
            "Epoch 435/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5407 - acc: 0.7764 - val_loss: 0.5618 - val_acc: 0.7722\n",
            "Epoch 436/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5510 - acc: 0.7739 - val_loss: 0.5564 - val_acc: 0.7755\n",
            "Epoch 437/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5450 - acc: 0.7749 - val_loss: 0.5573 - val_acc: 0.7745\n",
            "Epoch 438/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5420 - acc: 0.7746 - val_loss: 0.5608 - val_acc: 0.7712\n",
            "Epoch 439/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5447 - acc: 0.7744 - val_loss: 0.5657 - val_acc: 0.7692\n",
            "Epoch 440/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5428 - acc: 0.7775 - val_loss: 0.5668 - val_acc: 0.7682\n",
            "Epoch 441/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5390 - acc: 0.7770 - val_loss: 0.5623 - val_acc: 0.7722\n",
            "Epoch 442/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5411 - acc: 0.7776 - val_loss: 0.5567 - val_acc: 0.7758\n",
            "Epoch 443/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5391 - acc: 0.7740 - val_loss: 0.5558 - val_acc: 0.7755\n",
            "Epoch 444/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5385 - acc: 0.7775 - val_loss: 0.5575 - val_acc: 0.7758\n",
            "Epoch 445/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5434 - acc: 0.7744 - val_loss: 0.5592 - val_acc: 0.7722\n",
            "Epoch 446/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5418 - acc: 0.7783 - val_loss: 0.5628 - val_acc: 0.7708\n",
            "Epoch 447/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5440 - acc: 0.7757 - val_loss: 0.5624 - val_acc: 0.7705\n",
            "Epoch 448/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5380 - acc: 0.7778 - val_loss: 0.5617 - val_acc: 0.7698\n",
            "Epoch 449/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5403 - acc: 0.7773 - val_loss: 0.5590 - val_acc: 0.7718\n",
            "Epoch 450/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5419 - acc: 0.7741 - val_loss: 0.5586 - val_acc: 0.7728\n",
            "Epoch 451/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5397 - acc: 0.7795 - val_loss: 0.5579 - val_acc: 0.7761\n",
            "Epoch 452/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5389 - acc: 0.7765 - val_loss: 0.5603 - val_acc: 0.7735\n",
            "Epoch 453/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5310 - acc: 0.7812 - val_loss: 0.5579 - val_acc: 0.7741\n",
            "Epoch 454/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5386 - acc: 0.7765 - val_loss: 0.5531 - val_acc: 0.7774\n",
            "Epoch 455/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5345 - acc: 0.7773 - val_loss: 0.5520 - val_acc: 0.7781\n",
            "Epoch 456/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5317 - acc: 0.7851 - val_loss: 0.5532 - val_acc: 0.7804\n",
            "Epoch 457/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5333 - acc: 0.7783 - val_loss: 0.5569 - val_acc: 0.7768\n",
            "Epoch 458/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5407 - acc: 0.7793 - val_loss: 0.5544 - val_acc: 0.7774\n",
            "Epoch 459/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5400 - acc: 0.7762 - val_loss: 0.5541 - val_acc: 0.7768\n",
            "Epoch 460/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5454 - acc: 0.7786 - val_loss: 0.5547 - val_acc: 0.7774\n",
            "Epoch 461/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5375 - acc: 0.7756 - val_loss: 0.5570 - val_acc: 0.7765\n",
            "Epoch 462/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5357 - acc: 0.7780 - val_loss: 0.5558 - val_acc: 0.7778\n",
            "Epoch 463/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5399 - acc: 0.7750 - val_loss: 0.5524 - val_acc: 0.7798\n",
            "Epoch 464/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5317 - acc: 0.7817 - val_loss: 0.5497 - val_acc: 0.7798\n",
            "Epoch 465/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5331 - acc: 0.7780 - val_loss: 0.5525 - val_acc: 0.7755\n",
            "Epoch 466/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5304 - acc: 0.7828 - val_loss: 0.5532 - val_acc: 0.7755\n",
            "Epoch 467/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5397 - acc: 0.7760 - val_loss: 0.5521 - val_acc: 0.7765\n",
            "Epoch 468/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5308 - acc: 0.7813 - val_loss: 0.5468 - val_acc: 0.7841\n",
            "Epoch 469/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5362 - acc: 0.7812 - val_loss: 0.5432 - val_acc: 0.7854\n",
            "Epoch 470/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5333 - acc: 0.7805 - val_loss: 0.5432 - val_acc: 0.7860\n",
            "Epoch 471/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5322 - acc: 0.7779 - val_loss: 0.5451 - val_acc: 0.7857\n",
            "Epoch 472/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5343 - acc: 0.7796 - val_loss: 0.5476 - val_acc: 0.7808\n",
            "Epoch 473/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5267 - acc: 0.7810 - val_loss: 0.5493 - val_acc: 0.7808\n",
            "Epoch 474/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5337 - acc: 0.7807 - val_loss: 0.5519 - val_acc: 0.7788\n",
            "Epoch 475/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5256 - acc: 0.7796 - val_loss: 0.5558 - val_acc: 0.7774\n",
            "Epoch 476/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5315 - acc: 0.7817 - val_loss: 0.5559 - val_acc: 0.7778\n",
            "Epoch 477/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5316 - acc: 0.7819 - val_loss: 0.5515 - val_acc: 0.7794\n",
            "Epoch 478/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5232 - acc: 0.7831 - val_loss: 0.5470 - val_acc: 0.7817\n",
            "Epoch 479/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5266 - acc: 0.7831 - val_loss: 0.5457 - val_acc: 0.7817\n",
            "Epoch 480/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5328 - acc: 0.7804 - val_loss: 0.5481 - val_acc: 0.7808\n",
            "Epoch 481/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5287 - acc: 0.7814 - val_loss: 0.5535 - val_acc: 0.7781\n",
            "Epoch 482/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5310 - acc: 0.7765 - val_loss: 0.5579 - val_acc: 0.7788\n",
            "Epoch 483/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5278 - acc: 0.7823 - val_loss: 0.5515 - val_acc: 0.7811\n",
            "Epoch 484/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5292 - acc: 0.7835 - val_loss: 0.5441 - val_acc: 0.7854\n",
            "Epoch 485/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5262 - acc: 0.7823 - val_loss: 0.5417 - val_acc: 0.7887\n",
            "Epoch 486/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5262 - acc: 0.7814 - val_loss: 0.5401 - val_acc: 0.7837\n",
            "Epoch 487/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5172 - acc: 0.7897 - val_loss: 0.5415 - val_acc: 0.7821\n",
            "Epoch 488/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5237 - acc: 0.7823 - val_loss: 0.5500 - val_acc: 0.7741\n",
            "Epoch 489/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5265 - acc: 0.7813 - val_loss: 0.5535 - val_acc: 0.7731\n",
            "Epoch 490/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.5206 - acc: 0.7819 - val_loss: 0.5426 - val_acc: 0.7817\n",
            "Epoch 491/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5224 - acc: 0.7898 - val_loss: 0.5406 - val_acc: 0.7854\n",
            "Epoch 492/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5243 - acc: 0.7853 - val_loss: 0.5435 - val_acc: 0.7841\n",
            "Epoch 493/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5233 - acc: 0.7854 - val_loss: 0.5451 - val_acc: 0.7827\n",
            "Epoch 494/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5225 - acc: 0.7860 - val_loss: 0.5469 - val_acc: 0.7801\n",
            "Epoch 495/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5179 - acc: 0.7894 - val_loss: 0.5516 - val_acc: 0.7778\n",
            "Epoch 496/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5268 - acc: 0.7851 - val_loss: 0.5512 - val_acc: 0.7791\n",
            "Epoch 497/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5242 - acc: 0.7829 - val_loss: 0.5442 - val_acc: 0.7821\n",
            "Epoch 498/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5226 - acc: 0.7775 - val_loss: 0.5416 - val_acc: 0.7847\n",
            "Epoch 499/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5167 - acc: 0.7870 - val_loss: 0.5413 - val_acc: 0.7841\n",
            "Epoch 500/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5182 - acc: 0.7873 - val_loss: 0.5438 - val_acc: 0.7808\n",
            "Epoch 501/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5221 - acc: 0.7840 - val_loss: 0.5476 - val_acc: 0.7811\n",
            "Epoch 502/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5195 - acc: 0.7839 - val_loss: 0.5534 - val_acc: 0.7774\n",
            "Epoch 503/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5248 - acc: 0.7846 - val_loss: 0.5481 - val_acc: 0.7781\n",
            "Epoch 504/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5144 - acc: 0.7870 - val_loss: 0.5433 - val_acc: 0.7821\n",
            "Epoch 505/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5206 - acc: 0.7855 - val_loss: 0.5421 - val_acc: 0.7821\n",
            "Epoch 506/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5213 - acc: 0.7869 - val_loss: 0.5429 - val_acc: 0.7814\n",
            "Epoch 507/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5256 - acc: 0.7806 - val_loss: 0.5487 - val_acc: 0.7778\n",
            "Epoch 508/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5204 - acc: 0.7873 - val_loss: 0.5461 - val_acc: 0.7794\n",
            "Epoch 509/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5217 - acc: 0.7865 - val_loss: 0.5403 - val_acc: 0.7860\n",
            "Epoch 510/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5160 - acc: 0.7891 - val_loss: 0.5377 - val_acc: 0.7844\n",
            "Epoch 511/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5200 - acc: 0.7845 - val_loss: 0.5357 - val_acc: 0.7860\n",
            "Epoch 512/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5193 - acc: 0.7859 - val_loss: 0.5377 - val_acc: 0.7821\n",
            "Epoch 513/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5210 - acc: 0.7793 - val_loss: 0.5403 - val_acc: 0.7801\n",
            "Epoch 514/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5190 - acc: 0.7891 - val_loss: 0.5488 - val_acc: 0.7745\n",
            "Epoch 515/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5202 - acc: 0.7839 - val_loss: 0.5428 - val_acc: 0.7791\n",
            "Epoch 516/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5173 - acc: 0.7881 - val_loss: 0.5350 - val_acc: 0.7851\n",
            "Epoch 517/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5195 - acc: 0.7836 - val_loss: 0.5337 - val_acc: 0.7877\n",
            "Epoch 518/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5192 - acc: 0.7889 - val_loss: 0.5354 - val_acc: 0.7870\n",
            "Epoch 519/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5242 - acc: 0.7817 - val_loss: 0.5332 - val_acc: 0.7884\n",
            "Epoch 520/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5129 - acc: 0.7875 - val_loss: 0.5364 - val_acc: 0.7874\n",
            "Epoch 521/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5147 - acc: 0.7885 - val_loss: 0.5418 - val_acc: 0.7841\n",
            "Epoch 522/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5100 - acc: 0.7898 - val_loss: 0.5413 - val_acc: 0.7841\n",
            "Epoch 523/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5143 - acc: 0.7870 - val_loss: 0.5425 - val_acc: 0.7837\n",
            "Epoch 524/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5162 - acc: 0.7886 - val_loss: 0.5402 - val_acc: 0.7831\n",
            "Epoch 525/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5106 - acc: 0.7894 - val_loss: 0.5366 - val_acc: 0.7801\n",
            "Epoch 526/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5176 - acc: 0.7884 - val_loss: 0.5372 - val_acc: 0.7824\n",
            "Epoch 527/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5125 - acc: 0.7904 - val_loss: 0.5372 - val_acc: 0.7824\n",
            "Epoch 528/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5156 - acc: 0.7875 - val_loss: 0.5411 - val_acc: 0.7811\n",
            "Epoch 529/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5150 - acc: 0.7895 - val_loss: 0.5483 - val_acc: 0.7755\n",
            "Epoch 530/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5092 - acc: 0.7922 - val_loss: 0.5440 - val_acc: 0.7761\n",
            "Epoch 531/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5100 - acc: 0.7910 - val_loss: 0.5337 - val_acc: 0.7854\n",
            "Epoch 532/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5142 - acc: 0.7896 - val_loss: 0.5324 - val_acc: 0.7864\n",
            "Epoch 533/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5120 - acc: 0.7914 - val_loss: 0.5337 - val_acc: 0.7867\n",
            "Epoch 534/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5129 - acc: 0.7907 - val_loss: 0.5330 - val_acc: 0.7864\n",
            "Epoch 535/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5031 - acc: 0.7939 - val_loss: 0.5337 - val_acc: 0.7864\n",
            "Epoch 536/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5161 - acc: 0.7867 - val_loss: 0.5326 - val_acc: 0.7860\n",
            "Epoch 537/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.5079 - acc: 0.7928 - val_loss: 0.5302 - val_acc: 0.7880\n",
            "Epoch 538/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5079 - acc: 0.7922 - val_loss: 0.5328 - val_acc: 0.7887\n",
            "Epoch 539/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5088 - acc: 0.7907 - val_loss: 0.5361 - val_acc: 0.7874\n",
            "Epoch 540/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5040 - acc: 0.7990 - val_loss: 0.5374 - val_acc: 0.7860\n",
            "Epoch 541/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5133 - acc: 0.7879 - val_loss: 0.5348 - val_acc: 0.7867\n",
            "Epoch 542/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5124 - acc: 0.7860 - val_loss: 0.5339 - val_acc: 0.7890\n",
            "Epoch 543/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5073 - acc: 0.7927 - val_loss: 0.5318 - val_acc: 0.7910\n",
            "Epoch 544/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5121 - acc: 0.7908 - val_loss: 0.5293 - val_acc: 0.7927\n",
            "Epoch 545/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5060 - acc: 0.7946 - val_loss: 0.5287 - val_acc: 0.7930\n",
            "Epoch 546/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5068 - acc: 0.7922 - val_loss: 0.5337 - val_acc: 0.7880\n",
            "Epoch 547/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5153 - acc: 0.7903 - val_loss: 0.5331 - val_acc: 0.7864\n",
            "Epoch 548/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5039 - acc: 0.7908 - val_loss: 0.5300 - val_acc: 0.7884\n",
            "Epoch 549/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5028 - acc: 0.7925 - val_loss: 0.5308 - val_acc: 0.7827\n",
            "Epoch 550/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5068 - acc: 0.7913 - val_loss: 0.5332 - val_acc: 0.7870\n",
            "Epoch 551/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5034 - acc: 0.7948 - val_loss: 0.5331 - val_acc: 0.7870\n",
            "Epoch 552/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5051 - acc: 0.7903 - val_loss: 0.5306 - val_acc: 0.7887\n",
            "Epoch 553/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5025 - acc: 0.7923 - val_loss: 0.5311 - val_acc: 0.7894\n",
            "Epoch 554/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5007 - acc: 0.7946 - val_loss: 0.5312 - val_acc: 0.7910\n",
            "Epoch 555/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4934 - acc: 0.7978 - val_loss: 0.5308 - val_acc: 0.7907\n",
            "Epoch 556/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5034 - acc: 0.7943 - val_loss: 0.5301 - val_acc: 0.7910\n",
            "Epoch 557/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5016 - acc: 0.7917 - val_loss: 0.5283 - val_acc: 0.7900\n",
            "Epoch 558/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.5047 - acc: 0.7880 - val_loss: 0.5267 - val_acc: 0.7890\n",
            "Epoch 559/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5031 - acc: 0.7937 - val_loss: 0.5244 - val_acc: 0.7874\n",
            "Epoch 560/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5010 - acc: 0.7917 - val_loss: 0.5230 - val_acc: 0.7880\n",
            "Epoch 561/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4960 - acc: 0.7959 - val_loss: 0.5223 - val_acc: 0.7894\n",
            "Epoch 562/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5032 - acc: 0.7952 - val_loss: 0.5222 - val_acc: 0.7927\n",
            "Epoch 563/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5027 - acc: 0.7941 - val_loss: 0.5243 - val_acc: 0.7920\n",
            "Epoch 564/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4985 - acc: 0.7965 - val_loss: 0.5289 - val_acc: 0.7930\n",
            "Epoch 565/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5018 - acc: 0.7933 - val_loss: 0.5253 - val_acc: 0.7917\n",
            "Epoch 566/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5013 - acc: 0.7941 - val_loss: 0.5235 - val_acc: 0.7903\n",
            "Epoch 567/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4969 - acc: 0.7978 - val_loss: 0.5217 - val_acc: 0.7910\n",
            "Epoch 568/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4991 - acc: 0.7920 - val_loss: 0.5200 - val_acc: 0.7917\n",
            "Epoch 569/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4979 - acc: 0.7930 - val_loss: 0.5175 - val_acc: 0.7907\n",
            "Epoch 570/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5028 - acc: 0.7894 - val_loss: 0.5140 - val_acc: 0.7946\n",
            "Epoch 571/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5027 - acc: 0.7937 - val_loss: 0.5162 - val_acc: 0.7907\n",
            "Epoch 572/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4977 - acc: 0.7973 - val_loss: 0.5196 - val_acc: 0.7903\n",
            "Epoch 573/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4967 - acc: 0.7962 - val_loss: 0.5187 - val_acc: 0.7927\n",
            "Epoch 574/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5005 - acc: 0.7957 - val_loss: 0.5162 - val_acc: 0.7910\n",
            "Epoch 575/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.5045 - acc: 0.7915 - val_loss: 0.5167 - val_acc: 0.7937\n",
            "Epoch 576/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4974 - acc: 0.7979 - val_loss: 0.5199 - val_acc: 0.7923\n",
            "Epoch 577/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.5028 - acc: 0.7959 - val_loss: 0.5216 - val_acc: 0.7887\n",
            "Epoch 578/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4940 - acc: 0.7964 - val_loss: 0.5198 - val_acc: 0.7913\n",
            "Epoch 579/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4987 - acc: 0.7941 - val_loss: 0.5143 - val_acc: 0.7943\n",
            "Epoch 580/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4930 - acc: 0.7956 - val_loss: 0.5112 - val_acc: 0.7953\n",
            "Epoch 581/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4898 - acc: 0.7985 - val_loss: 0.5116 - val_acc: 0.7933\n",
            "Epoch 582/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4953 - acc: 0.7944 - val_loss: 0.5171 - val_acc: 0.7900\n",
            "Epoch 583/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4969 - acc: 0.7940 - val_loss: 0.5192 - val_acc: 0.7870\n",
            "Epoch 584/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4940 - acc: 0.7957 - val_loss: 0.5159 - val_acc: 0.7894\n",
            "Epoch 585/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4961 - acc: 0.7946 - val_loss: 0.5126 - val_acc: 0.7930\n",
            "Epoch 586/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4974 - acc: 0.7972 - val_loss: 0.5136 - val_acc: 0.7940\n",
            "Epoch 587/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4889 - acc: 0.7984 - val_loss: 0.5200 - val_acc: 0.7920\n",
            "Epoch 588/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4961 - acc: 0.8005 - val_loss: 0.5211 - val_acc: 0.7903\n",
            "Epoch 589/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4945 - acc: 0.7986 - val_loss: 0.5179 - val_acc: 0.7910\n",
            "Epoch 590/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4933 - acc: 0.7999 - val_loss: 0.5163 - val_acc: 0.7903\n",
            "Epoch 591/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4900 - acc: 0.7989 - val_loss: 0.5143 - val_acc: 0.7913\n",
            "Epoch 592/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4953 - acc: 0.7981 - val_loss: 0.5129 - val_acc: 0.7920\n",
            "Epoch 593/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4901 - acc: 0.7951 - val_loss: 0.5105 - val_acc: 0.7956\n",
            "Epoch 594/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4943 - acc: 0.7925 - val_loss: 0.5096 - val_acc: 0.7953\n",
            "Epoch 595/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4917 - acc: 0.7983 - val_loss: 0.5110 - val_acc: 0.7943\n",
            "Epoch 596/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4974 - acc: 0.7957 - val_loss: 0.5112 - val_acc: 0.7907\n",
            "Epoch 597/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4869 - acc: 0.7979 - val_loss: 0.5124 - val_acc: 0.7920\n",
            "Epoch 598/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4893 - acc: 0.7998 - val_loss: 0.5108 - val_acc: 0.7910\n",
            "Epoch 599/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4948 - acc: 0.7981 - val_loss: 0.5097 - val_acc: 0.7927\n",
            "Epoch 600/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4870 - acc: 0.8007 - val_loss: 0.5108 - val_acc: 0.7960\n",
            "Epoch 601/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4907 - acc: 0.8017 - val_loss: 0.5143 - val_acc: 0.7943\n",
            "Epoch 602/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4874 - acc: 0.7965 - val_loss: 0.5178 - val_acc: 0.7976\n",
            "Epoch 603/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4936 - acc: 0.7929 - val_loss: 0.5170 - val_acc: 0.7956\n",
            "Epoch 604/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4932 - acc: 0.7989 - val_loss: 0.5164 - val_acc: 0.7943\n",
            "Epoch 605/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4888 - acc: 0.7960 - val_loss: 0.5152 - val_acc: 0.7933\n",
            "Epoch 606/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4811 - acc: 0.8041 - val_loss: 0.5147 - val_acc: 0.7913\n",
            "Epoch 607/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4940 - acc: 0.7951 - val_loss: 0.5160 - val_acc: 0.7900\n",
            "Epoch 608/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4884 - acc: 0.7986 - val_loss: 0.5149 - val_acc: 0.7910\n",
            "Epoch 609/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4909 - acc: 0.7966 - val_loss: 0.5128 - val_acc: 0.7930\n",
            "Epoch 610/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4921 - acc: 0.7978 - val_loss: 0.5109 - val_acc: 0.7973\n",
            "Epoch 611/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4891 - acc: 0.7952 - val_loss: 0.5109 - val_acc: 0.7956\n",
            "Epoch 612/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4914 - acc: 0.7984 - val_loss: 0.5115 - val_acc: 0.7910\n",
            "Epoch 613/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4879 - acc: 0.7983 - val_loss: 0.5122 - val_acc: 0.7897\n",
            "Epoch 614/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4909 - acc: 0.7985 - val_loss: 0.5134 - val_acc: 0.7917\n",
            "Epoch 615/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4880 - acc: 0.7965 - val_loss: 0.5127 - val_acc: 0.7907\n",
            "Epoch 616/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4850 - acc: 0.8004 - val_loss: 0.5121 - val_acc: 0.7920\n",
            "Epoch 617/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4961 - acc: 0.7980 - val_loss: 0.5111 - val_acc: 0.7943\n",
            "Epoch 618/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4881 - acc: 0.7987 - val_loss: 0.5105 - val_acc: 0.7956\n",
            "Epoch 619/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4884 - acc: 0.7969 - val_loss: 0.5085 - val_acc: 0.7950\n",
            "Epoch 620/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4828 - acc: 0.8029 - val_loss: 0.5061 - val_acc: 0.7966\n",
            "Epoch 621/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4824 - acc: 0.8040 - val_loss: 0.5067 - val_acc: 0.7953\n",
            "Epoch 622/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4876 - acc: 0.8000 - val_loss: 0.5079 - val_acc: 0.7910\n",
            "Epoch 623/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4856 - acc: 0.8015 - val_loss: 0.5056 - val_acc: 0.7933\n",
            "Epoch 624/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4904 - acc: 0.8004 - val_loss: 0.5056 - val_acc: 0.7940\n",
            "Epoch 625/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4819 - acc: 0.7977 - val_loss: 0.5053 - val_acc: 0.7963\n",
            "Epoch 626/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4806 - acc: 0.8004 - val_loss: 0.5062 - val_acc: 0.7996\n",
            "Epoch 627/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4790 - acc: 0.8050 - val_loss: 0.5098 - val_acc: 0.7970\n",
            "Epoch 628/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4892 - acc: 0.8002 - val_loss: 0.5107 - val_acc: 0.7973\n",
            "Epoch 629/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4832 - acc: 0.7986 - val_loss: 0.5065 - val_acc: 0.8003\n",
            "Epoch 630/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4866 - acc: 0.8011 - val_loss: 0.5047 - val_acc: 0.7993\n",
            "Epoch 631/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4860 - acc: 0.8013 - val_loss: 0.5036 - val_acc: 0.8003\n",
            "Epoch 632/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4870 - acc: 0.7993 - val_loss: 0.5040 - val_acc: 0.7993\n",
            "Epoch 633/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4847 - acc: 0.8036 - val_loss: 0.5074 - val_acc: 0.7946\n",
            "Epoch 634/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4817 - acc: 0.8037 - val_loss: 0.5086 - val_acc: 0.7976\n",
            "Epoch 635/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4876 - acc: 0.8018 - val_loss: 0.5075 - val_acc: 0.7999\n",
            "Epoch 636/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4847 - acc: 0.7989 - val_loss: 0.5085 - val_acc: 0.7986\n",
            "Epoch 637/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4876 - acc: 0.7994 - val_loss: 0.5095 - val_acc: 0.7979\n",
            "Epoch 638/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4782 - acc: 0.8013 - val_loss: 0.5078 - val_acc: 0.7996\n",
            "Epoch 639/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4793 - acc: 0.8050 - val_loss: 0.5074 - val_acc: 0.7960\n",
            "Epoch 640/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4775 - acc: 0.8013 - val_loss: 0.5100 - val_acc: 0.7933\n",
            "Epoch 641/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4818 - acc: 0.8007 - val_loss: 0.5048 - val_acc: 0.7950\n",
            "Epoch 642/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4803 - acc: 0.8042 - val_loss: 0.5030 - val_acc: 0.8022\n",
            "Epoch 643/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4775 - acc: 0.8046 - val_loss: 0.5037 - val_acc: 0.8006\n",
            "Epoch 644/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4805 - acc: 0.8003 - val_loss: 0.5036 - val_acc: 0.7996\n",
            "Epoch 645/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4838 - acc: 0.8010 - val_loss: 0.5070 - val_acc: 0.8009\n",
            "Epoch 646/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4749 - acc: 0.8042 - val_loss: 0.5087 - val_acc: 0.7979\n",
            "Epoch 647/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4733 - acc: 0.8071 - val_loss: 0.5049 - val_acc: 0.8009\n",
            "Epoch 648/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4794 - acc: 0.8036 - val_loss: 0.5030 - val_acc: 0.8003\n",
            "Epoch 649/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4752 - acc: 0.8080 - val_loss: 0.5035 - val_acc: 0.7996\n",
            "Epoch 650/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4782 - acc: 0.8043 - val_loss: 0.5023 - val_acc: 0.7993\n",
            "Epoch 651/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4832 - acc: 0.8046 - val_loss: 0.5011 - val_acc: 0.8003\n",
            "Epoch 652/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4730 - acc: 0.8084 - val_loss: 0.5009 - val_acc: 0.8003\n",
            "Epoch 653/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4762 - acc: 0.8023 - val_loss: 0.5021 - val_acc: 0.7973\n",
            "Epoch 654/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4776 - acc: 0.8046 - val_loss: 0.5032 - val_acc: 0.7996\n",
            "Epoch 655/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4753 - acc: 0.8053 - val_loss: 0.5015 - val_acc: 0.8016\n",
            "Epoch 656/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4815 - acc: 0.8046 - val_loss: 0.5021 - val_acc: 0.7993\n",
            "Epoch 657/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4802 - acc: 0.8025 - val_loss: 0.5035 - val_acc: 0.7996\n",
            "Epoch 658/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4721 - acc: 0.8030 - val_loss: 0.5009 - val_acc: 0.8009\n",
            "Epoch 659/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4694 - acc: 0.8066 - val_loss: 0.4984 - val_acc: 0.8022\n",
            "Epoch 660/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4714 - acc: 0.8034 - val_loss: 0.4969 - val_acc: 0.8026\n",
            "Epoch 661/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4761 - acc: 0.8043 - val_loss: 0.4955 - val_acc: 0.8019\n",
            "Epoch 662/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4750 - acc: 0.8023 - val_loss: 0.4951 - val_acc: 0.8039\n",
            "Epoch 663/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4756 - acc: 0.8054 - val_loss: 0.4970 - val_acc: 0.8056\n",
            "Epoch 664/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.4791 - acc: 0.8013 - val_loss: 0.4965 - val_acc: 0.8065\n",
            "Epoch 665/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4745 - acc: 0.8067 - val_loss: 0.4952 - val_acc: 0.8065\n",
            "Epoch 666/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4717 - acc: 0.8059 - val_loss: 0.4942 - val_acc: 0.8085\n",
            "Epoch 667/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4824 - acc: 0.8040 - val_loss: 0.4950 - val_acc: 0.8016\n",
            "Epoch 668/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4737 - acc: 0.8091 - val_loss: 0.4946 - val_acc: 0.8016\n",
            "Epoch 669/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4721 - acc: 0.8024 - val_loss: 0.4940 - val_acc: 0.8062\n",
            "Epoch 670/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4755 - acc: 0.8050 - val_loss: 0.4934 - val_acc: 0.8085\n",
            "Epoch 671/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4805 - acc: 0.7973 - val_loss: 0.4942 - val_acc: 0.8082\n",
            "Epoch 672/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4730 - acc: 0.8051 - val_loss: 0.4944 - val_acc: 0.8075\n",
            "Epoch 673/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4701 - acc: 0.8077 - val_loss: 0.4943 - val_acc: 0.8072\n",
            "Epoch 674/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4692 - acc: 0.8065 - val_loss: 0.4925 - val_acc: 0.8049\n",
            "Epoch 675/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4754 - acc: 0.8027 - val_loss: 0.4931 - val_acc: 0.8042\n",
            "Epoch 676/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4678 - acc: 0.8091 - val_loss: 0.4952 - val_acc: 0.8046\n",
            "Epoch 677/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4748 - acc: 0.8056 - val_loss: 0.4961 - val_acc: 0.8029\n",
            "Epoch 678/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4750 - acc: 0.8070 - val_loss: 0.4953 - val_acc: 0.8062\n",
            "Epoch 679/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4669 - acc: 0.8070 - val_loss: 0.4965 - val_acc: 0.8069\n",
            "Epoch 680/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4724 - acc: 0.8053 - val_loss: 0.4942 - val_acc: 0.8075\n",
            "Epoch 681/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4647 - acc: 0.8095 - val_loss: 0.4924 - val_acc: 0.8075\n",
            "Epoch 682/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4755 - acc: 0.8076 - val_loss: 0.4932 - val_acc: 0.8046\n",
            "Epoch 683/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4714 - acc: 0.8089 - val_loss: 0.4955 - val_acc: 0.8042\n",
            "Epoch 684/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4755 - acc: 0.8070 - val_loss: 0.4949 - val_acc: 0.8046\n",
            "Epoch 685/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4714 - acc: 0.8099 - val_loss: 0.4958 - val_acc: 0.8046\n",
            "Epoch 686/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4734 - acc: 0.8056 - val_loss: 0.4966 - val_acc: 0.8042\n",
            "Epoch 687/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4673 - acc: 0.8054 - val_loss: 0.4955 - val_acc: 0.8046\n",
            "Epoch 688/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4751 - acc: 0.8053 - val_loss: 0.4946 - val_acc: 0.8029\n",
            "Epoch 689/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4656 - acc: 0.8112 - val_loss: 0.4960 - val_acc: 0.8042\n",
            "Epoch 690/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4633 - acc: 0.8107 - val_loss: 0.4944 - val_acc: 0.8065\n",
            "Epoch 691/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4656 - acc: 0.8098 - val_loss: 0.4922 - val_acc: 0.8046\n",
            "Epoch 692/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4714 - acc: 0.8073 - val_loss: 0.4915 - val_acc: 0.8079\n",
            "Epoch 693/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4644 - acc: 0.8108 - val_loss: 0.4910 - val_acc: 0.8052\n",
            "Epoch 694/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4700 - acc: 0.8115 - val_loss: 0.4920 - val_acc: 0.8049\n",
            "Epoch 695/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4748 - acc: 0.8084 - val_loss: 0.4923 - val_acc: 0.8049\n",
            "Epoch 696/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4636 - acc: 0.8100 - val_loss: 0.4926 - val_acc: 0.8072\n",
            "Epoch 697/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4684 - acc: 0.8075 - val_loss: 0.4932 - val_acc: 0.8079\n",
            "Epoch 698/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4608 - acc: 0.8091 - val_loss: 0.4944 - val_acc: 0.8082\n",
            "Epoch 699/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4660 - acc: 0.8074 - val_loss: 0.4927 - val_acc: 0.8046\n",
            "Epoch 700/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4669 - acc: 0.8100 - val_loss: 0.4908 - val_acc: 0.8039\n",
            "Epoch 701/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4717 - acc: 0.8097 - val_loss: 0.4902 - val_acc: 0.8072\n",
            "Epoch 702/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4671 - acc: 0.8094 - val_loss: 0.4913 - val_acc: 0.8059\n",
            "Epoch 703/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4723 - acc: 0.8079 - val_loss: 0.4896 - val_acc: 0.8072\n",
            "Epoch 704/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4650 - acc: 0.8082 - val_loss: 0.4887 - val_acc: 0.8085\n",
            "Epoch 705/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4648 - acc: 0.8095 - val_loss: 0.4885 - val_acc: 0.8105\n",
            "Epoch 706/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4640 - acc: 0.8105 - val_loss: 0.4904 - val_acc: 0.8079\n",
            "Epoch 707/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4678 - acc: 0.8099 - val_loss: 0.4938 - val_acc: 0.8056\n",
            "Epoch 708/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4643 - acc: 0.8082 - val_loss: 0.4921 - val_acc: 0.8075\n",
            "Epoch 709/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4701 - acc: 0.8115 - val_loss: 0.4904 - val_acc: 0.8075\n",
            "Epoch 710/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4629 - acc: 0.8083 - val_loss: 0.4882 - val_acc: 0.8092\n",
            "Epoch 711/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4602 - acc: 0.8080 - val_loss: 0.4873 - val_acc: 0.8075\n",
            "Epoch 712/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4617 - acc: 0.8127 - val_loss: 0.4910 - val_acc: 0.8052\n",
            "Epoch 713/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4629 - acc: 0.8091 - val_loss: 0.4919 - val_acc: 0.8065\n",
            "Epoch 714/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4628 - acc: 0.8118 - val_loss: 0.4915 - val_acc: 0.8092\n",
            "Epoch 715/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4554 - acc: 0.8159 - val_loss: 0.4902 - val_acc: 0.8059\n",
            "Epoch 716/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4651 - acc: 0.8084 - val_loss: 0.4878 - val_acc: 0.8056\n",
            "Epoch 717/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4569 - acc: 0.8125 - val_loss: 0.4860 - val_acc: 0.8056\n",
            "Epoch 718/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4568 - acc: 0.8151 - val_loss: 0.4856 - val_acc: 0.8079\n",
            "Epoch 719/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4621 - acc: 0.8110 - val_loss: 0.4865 - val_acc: 0.8085\n",
            "Epoch 720/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4624 - acc: 0.8086 - val_loss: 0.4900 - val_acc: 0.8059\n",
            "Epoch 721/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4561 - acc: 0.8153 - val_loss: 0.4942 - val_acc: 0.8069\n",
            "Epoch 722/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4584 - acc: 0.8119 - val_loss: 0.4955 - val_acc: 0.8072\n",
            "Epoch 723/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4578 - acc: 0.8104 - val_loss: 0.4942 - val_acc: 0.8052\n",
            "Epoch 724/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4579 - acc: 0.8143 - val_loss: 0.4925 - val_acc: 0.8049\n",
            "Epoch 725/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4676 - acc: 0.8098 - val_loss: 0.4894 - val_acc: 0.8079\n",
            "Epoch 726/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4595 - acc: 0.8151 - val_loss: 0.4896 - val_acc: 0.8075\n",
            "Epoch 727/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4704 - acc: 0.8111 - val_loss: 0.4890 - val_acc: 0.8079\n",
            "Epoch 728/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4591 - acc: 0.8123 - val_loss: 0.4890 - val_acc: 0.8079\n",
            "Epoch 729/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4526 - acc: 0.8150 - val_loss: 0.4876 - val_acc: 0.8089\n",
            "Epoch 730/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4627 - acc: 0.8093 - val_loss: 0.4855 - val_acc: 0.8089\n",
            "Epoch 731/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4641 - acc: 0.8103 - val_loss: 0.4844 - val_acc: 0.8065\n",
            "Epoch 732/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4613 - acc: 0.8139 - val_loss: 0.4845 - val_acc: 0.8112\n",
            "Epoch 733/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4583 - acc: 0.8142 - val_loss: 0.4854 - val_acc: 0.8102\n",
            "Epoch 734/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4576 - acc: 0.8109 - val_loss: 0.4883 - val_acc: 0.8099\n",
            "Epoch 735/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4610 - acc: 0.8099 - val_loss: 0.4894 - val_acc: 0.8085\n",
            "Epoch 736/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4574 - acc: 0.8124 - val_loss: 0.4902 - val_acc: 0.8085\n",
            "Epoch 737/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4557 - acc: 0.8151 - val_loss: 0.4872 - val_acc: 0.8118\n",
            "Epoch 738/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4573 - acc: 0.8104 - val_loss: 0.4850 - val_acc: 0.8142\n",
            "Epoch 739/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4657 - acc: 0.8089 - val_loss: 0.4844 - val_acc: 0.8085\n",
            "Epoch 740/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4611 - acc: 0.8123 - val_loss: 0.4876 - val_acc: 0.8085\n",
            "Epoch 741/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4610 - acc: 0.8094 - val_loss: 0.4871 - val_acc: 0.8085\n",
            "Epoch 742/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4590 - acc: 0.8125 - val_loss: 0.4864 - val_acc: 0.8085\n",
            "Epoch 743/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4595 - acc: 0.8113 - val_loss: 0.4845 - val_acc: 0.8115\n",
            "Epoch 744/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4584 - acc: 0.8134 - val_loss: 0.4824 - val_acc: 0.8118\n",
            "Epoch 745/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4593 - acc: 0.8118 - val_loss: 0.4831 - val_acc: 0.8108\n",
            "Epoch 746/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4549 - acc: 0.8140 - val_loss: 0.4837 - val_acc: 0.8082\n",
            "Epoch 747/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4544 - acc: 0.8166 - val_loss: 0.4840 - val_acc: 0.8089\n",
            "Epoch 748/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4596 - acc: 0.8103 - val_loss: 0.4860 - val_acc: 0.8089\n",
            "Epoch 749/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4566 - acc: 0.8127 - val_loss: 0.4866 - val_acc: 0.8065\n",
            "Epoch 750/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4561 - acc: 0.8162 - val_loss: 0.4875 - val_acc: 0.8089\n",
            "Epoch 751/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4511 - acc: 0.8135 - val_loss: 0.4835 - val_acc: 0.8138\n",
            "Epoch 752/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4580 - acc: 0.8128 - val_loss: 0.4792 - val_acc: 0.8158\n",
            "Epoch 753/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4536 - acc: 0.8144 - val_loss: 0.4790 - val_acc: 0.8135\n",
            "Epoch 754/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4548 - acc: 0.8147 - val_loss: 0.4782 - val_acc: 0.8122\n",
            "Epoch 755/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4433 - acc: 0.8201 - val_loss: 0.4805 - val_acc: 0.8112\n",
            "Epoch 756/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4514 - acc: 0.8161 - val_loss: 0.4841 - val_acc: 0.8112\n",
            "Epoch 757/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4590 - acc: 0.8109 - val_loss: 0.4847 - val_acc: 0.8122\n",
            "Epoch 758/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4582 - acc: 0.8161 - val_loss: 0.4841 - val_acc: 0.8089\n",
            "Epoch 759/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4483 - acc: 0.8166 - val_loss: 0.4829 - val_acc: 0.8092\n",
            "Epoch 760/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4543 - acc: 0.8118 - val_loss: 0.4834 - val_acc: 0.8102\n",
            "Epoch 761/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4507 - acc: 0.8179 - val_loss: 0.4817 - val_acc: 0.8132\n",
            "Epoch 762/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4514 - acc: 0.8164 - val_loss: 0.4798 - val_acc: 0.8151\n",
            "Epoch 763/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4445 - acc: 0.8207 - val_loss: 0.4809 - val_acc: 0.8135\n",
            "Epoch 764/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4557 - acc: 0.8151 - val_loss: 0.4848 - val_acc: 0.8112\n",
            "Epoch 765/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4530 - acc: 0.8186 - val_loss: 0.4864 - val_acc: 0.8128\n",
            "Epoch 766/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4541 - acc: 0.8175 - val_loss: 0.4861 - val_acc: 0.8122\n",
            "Epoch 767/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4528 - acc: 0.8163 - val_loss: 0.4862 - val_acc: 0.8112\n",
            "Epoch 768/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4552 - acc: 0.8115 - val_loss: 0.4830 - val_acc: 0.8128\n",
            "Epoch 769/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4510 - acc: 0.8166 - val_loss: 0.4818 - val_acc: 0.8112\n",
            "Epoch 770/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4511 - acc: 0.8127 - val_loss: 0.4877 - val_acc: 0.8085\n",
            "Epoch 771/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4497 - acc: 0.8119 - val_loss: 0.4828 - val_acc: 0.8105\n",
            "Epoch 772/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4502 - acc: 0.8130 - val_loss: 0.4808 - val_acc: 0.8105\n",
            "Epoch 773/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8113 - val_loss: 0.4804 - val_acc: 0.8161\n",
            "Epoch 774/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4558 - acc: 0.8169 - val_loss: 0.4783 - val_acc: 0.8168\n",
            "Epoch 775/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4560 - acc: 0.8128 - val_loss: 0.4795 - val_acc: 0.8138\n",
            "Epoch 776/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4499 - acc: 0.8175 - val_loss: 0.4812 - val_acc: 0.8122\n",
            "Epoch 777/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4527 - acc: 0.8144 - val_loss: 0.4818 - val_acc: 0.8118\n",
            "Epoch 778/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4446 - acc: 0.8169 - val_loss: 0.4838 - val_acc: 0.8142\n",
            "Epoch 779/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4493 - acc: 0.8189 - val_loss: 0.4832 - val_acc: 0.8151\n",
            "Epoch 780/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4559 - acc: 0.8128 - val_loss: 0.4834 - val_acc: 0.8151\n",
            "Epoch 781/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4550 - acc: 0.8132 - val_loss: 0.4815 - val_acc: 0.8128\n",
            "Epoch 782/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4525 - acc: 0.8120 - val_loss: 0.4785 - val_acc: 0.8132\n",
            "Epoch 783/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4476 - acc: 0.8141 - val_loss: 0.4781 - val_acc: 0.8118\n",
            "Epoch 784/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4472 - acc: 0.8149 - val_loss: 0.4794 - val_acc: 0.8132\n",
            "Epoch 785/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4480 - acc: 0.8203 - val_loss: 0.4849 - val_acc: 0.8072\n",
            "Epoch 786/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4456 - acc: 0.8156 - val_loss: 0.4843 - val_acc: 0.8118\n",
            "Epoch 787/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4471 - acc: 0.8152 - val_loss: 0.4824 - val_acc: 0.8122\n",
            "Epoch 788/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4502 - acc: 0.8204 - val_loss: 0.4820 - val_acc: 0.8128\n",
            "Epoch 789/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4470 - acc: 0.8187 - val_loss: 0.4819 - val_acc: 0.8115\n",
            "Epoch 790/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4494 - acc: 0.8167 - val_loss: 0.4856 - val_acc: 0.8065\n",
            "Epoch 791/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4553 - acc: 0.8163 - val_loss: 0.4812 - val_acc: 0.8112\n",
            "Epoch 792/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4501 - acc: 0.8178 - val_loss: 0.4781 - val_acc: 0.8151\n",
            "Epoch 793/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4529 - acc: 0.8141 - val_loss: 0.4773 - val_acc: 0.8188\n",
            "Epoch 794/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4499 - acc: 0.8154 - val_loss: 0.4782 - val_acc: 0.8218\n",
            "Epoch 795/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4458 - acc: 0.8157 - val_loss: 0.4800 - val_acc: 0.8185\n",
            "Epoch 796/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4461 - acc: 0.8211 - val_loss: 0.4805 - val_acc: 0.8181\n",
            "Epoch 797/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4468 - acc: 0.8201 - val_loss: 0.4783 - val_acc: 0.8181\n",
            "Epoch 798/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4483 - acc: 0.8152 - val_loss: 0.4750 - val_acc: 0.8175\n",
            "Epoch 799/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4475 - acc: 0.8194 - val_loss: 0.4739 - val_acc: 0.8168\n",
            "Epoch 800/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4469 - acc: 0.8226 - val_loss: 0.4746 - val_acc: 0.8171\n",
            "Epoch 801/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4461 - acc: 0.8185 - val_loss: 0.4780 - val_acc: 0.8175\n",
            "Epoch 802/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4498 - acc: 0.8151 - val_loss: 0.4786 - val_acc: 0.8155\n",
            "Epoch 803/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4530 - acc: 0.8138 - val_loss: 0.4775 - val_acc: 0.8151\n",
            "Epoch 804/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4494 - acc: 0.8186 - val_loss: 0.4755 - val_acc: 0.8155\n",
            "Epoch 805/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4384 - acc: 0.8237 - val_loss: 0.4746 - val_acc: 0.8132\n",
            "Epoch 806/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4434 - acc: 0.8188 - val_loss: 0.4740 - val_acc: 0.8161\n",
            "Epoch 807/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4464 - acc: 0.8155 - val_loss: 0.4753 - val_acc: 0.8171\n",
            "Epoch 808/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4432 - acc: 0.8198 - val_loss: 0.4789 - val_acc: 0.8135\n",
            "Epoch 809/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4445 - acc: 0.8171 - val_loss: 0.4774 - val_acc: 0.8148\n",
            "Epoch 810/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4416 - acc: 0.8166 - val_loss: 0.4739 - val_acc: 0.8168\n",
            "Epoch 811/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4488 - acc: 0.8156 - val_loss: 0.4735 - val_acc: 0.8178\n",
            "Epoch 812/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4491 - acc: 0.8155 - val_loss: 0.4726 - val_acc: 0.8191\n",
            "Epoch 813/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4445 - acc: 0.8152 - val_loss: 0.4766 - val_acc: 0.8161\n",
            "Epoch 814/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4469 - acc: 0.8203 - val_loss: 0.4775 - val_acc: 0.8158\n",
            "Epoch 815/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4345 - acc: 0.8238 - val_loss: 0.4759 - val_acc: 0.8175\n",
            "Epoch 816/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4433 - acc: 0.8209 - val_loss: 0.4765 - val_acc: 0.8178\n",
            "Epoch 817/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4472 - acc: 0.8161 - val_loss: 0.4737 - val_acc: 0.8188\n",
            "Epoch 818/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4384 - acc: 0.8184 - val_loss: 0.4729 - val_acc: 0.8188\n",
            "Epoch 819/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4467 - acc: 0.8159 - val_loss: 0.4731 - val_acc: 0.8168\n",
            "Epoch 820/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4465 - acc: 0.8193 - val_loss: 0.4747 - val_acc: 0.8142\n",
            "Epoch 821/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4418 - acc: 0.8201 - val_loss: 0.4736 - val_acc: 0.8135\n",
            "Epoch 822/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4429 - acc: 0.8218 - val_loss: 0.4709 - val_acc: 0.8148\n",
            "Epoch 823/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4455 - acc: 0.8152 - val_loss: 0.4688 - val_acc: 0.8165\n",
            "Epoch 824/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4442 - acc: 0.8185 - val_loss: 0.4688 - val_acc: 0.8158\n",
            "Epoch 825/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4374 - acc: 0.8221 - val_loss: 0.4693 - val_acc: 0.8148\n",
            "Epoch 826/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4410 - acc: 0.8185 - val_loss: 0.4696 - val_acc: 0.8145\n",
            "Epoch 827/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4341 - acc: 0.8244 - val_loss: 0.4701 - val_acc: 0.8151\n",
            "Epoch 828/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4436 - acc: 0.8192 - val_loss: 0.4714 - val_acc: 0.8138\n",
            "Epoch 829/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4440 - acc: 0.8160 - val_loss: 0.4724 - val_acc: 0.8125\n",
            "Epoch 830/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4413 - acc: 0.8170 - val_loss: 0.4714 - val_acc: 0.8135\n",
            "Epoch 831/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4379 - acc: 0.8227 - val_loss: 0.4708 - val_acc: 0.8145\n",
            "Epoch 832/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4442 - acc: 0.8171 - val_loss: 0.4676 - val_acc: 0.8185\n",
            "Epoch 833/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4370 - acc: 0.8197 - val_loss: 0.4662 - val_acc: 0.8201\n",
            "Epoch 834/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4378 - acc: 0.8189 - val_loss: 0.4673 - val_acc: 0.8181\n",
            "Epoch 835/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4433 - acc: 0.8194 - val_loss: 0.4702 - val_acc: 0.8201\n",
            "Epoch 836/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.4402 - acc: 0.8182 - val_loss: 0.4725 - val_acc: 0.8181\n",
            "Epoch 837/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4362 - acc: 0.8238 - val_loss: 0.4717 - val_acc: 0.8181\n",
            "Epoch 838/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4382 - acc: 0.8222 - val_loss: 0.4693 - val_acc: 0.8237\n",
            "Epoch 839/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4382 - acc: 0.8245 - val_loss: 0.4691 - val_acc: 0.8214\n",
            "Epoch 840/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4444 - acc: 0.8206 - val_loss: 0.4692 - val_acc: 0.8204\n",
            "Epoch 841/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4417 - acc: 0.8183 - val_loss: 0.4696 - val_acc: 0.8178\n",
            "Epoch 842/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4364 - acc: 0.8242 - val_loss: 0.4684 - val_acc: 0.8171\n",
            "Epoch 843/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4423 - acc: 0.8209 - val_loss: 0.4722 - val_acc: 0.8188\n",
            "Epoch 844/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4434 - acc: 0.8174 - val_loss: 0.4758 - val_acc: 0.8181\n",
            "Epoch 845/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4288 - acc: 0.8242 - val_loss: 0.4753 - val_acc: 0.8168\n",
            "Epoch 846/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8187 - val_loss: 0.4742 - val_acc: 0.8128\n",
            "Epoch 847/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4453 - acc: 0.8172 - val_loss: 0.4741 - val_acc: 0.8115\n",
            "Epoch 848/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4383 - acc: 0.8228 - val_loss: 0.4750 - val_acc: 0.8171\n",
            "Epoch 849/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4353 - acc: 0.8229 - val_loss: 0.4759 - val_acc: 0.8211\n",
            "Epoch 850/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4337 - acc: 0.8219 - val_loss: 0.4733 - val_acc: 0.8204\n",
            "Epoch 851/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4384 - acc: 0.8218 - val_loss: 0.4715 - val_acc: 0.8198\n",
            "Epoch 852/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4297 - acc: 0.8258 - val_loss: 0.4722 - val_acc: 0.8181\n",
            "Epoch 853/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4331 - acc: 0.8213 - val_loss: 0.4724 - val_acc: 0.8188\n",
            "Epoch 854/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4405 - acc: 0.8210 - val_loss: 0.4726 - val_acc: 0.8208\n",
            "Epoch 855/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4389 - acc: 0.8186 - val_loss: 0.4737 - val_acc: 0.8208\n",
            "Epoch 856/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4358 - acc: 0.8240 - val_loss: 0.4740 - val_acc: 0.8191\n",
            "Epoch 857/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4387 - acc: 0.8223 - val_loss: 0.4706 - val_acc: 0.8198\n",
            "Epoch 858/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4360 - acc: 0.8237 - val_loss: 0.4669 - val_acc: 0.8201\n",
            "Epoch 859/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4323 - acc: 0.8232 - val_loss: 0.4684 - val_acc: 0.8171\n",
            "Epoch 860/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4355 - acc: 0.8245 - val_loss: 0.4699 - val_acc: 0.8178\n",
            "Epoch 861/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4299 - acc: 0.8215 - val_loss: 0.4728 - val_acc: 0.8181\n",
            "Epoch 862/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4336 - acc: 0.8213 - val_loss: 0.4735 - val_acc: 0.8198\n",
            "Epoch 863/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4323 - acc: 0.8244 - val_loss: 0.4713 - val_acc: 0.8218\n",
            "Epoch 864/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4364 - acc: 0.8237 - val_loss: 0.4687 - val_acc: 0.8181\n",
            "Epoch 865/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4351 - acc: 0.8203 - val_loss: 0.4723 - val_acc: 0.8142\n",
            "Epoch 866/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4334 - acc: 0.8236 - val_loss: 0.4718 - val_acc: 0.8165\n",
            "Epoch 867/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4355 - acc: 0.8223 - val_loss: 0.4723 - val_acc: 0.8221\n",
            "Epoch 868/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4392 - acc: 0.8189 - val_loss: 0.4753 - val_acc: 0.8151\n",
            "Epoch 869/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4326 - acc: 0.8272 - val_loss: 0.4742 - val_acc: 0.8158\n",
            "Epoch 870/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4315 - acc: 0.8205 - val_loss: 0.4713 - val_acc: 0.8214\n",
            "Epoch 871/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4382 - acc: 0.8204 - val_loss: 0.4713 - val_acc: 0.8168\n",
            "Epoch 872/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4361 - acc: 0.8218 - val_loss: 0.4696 - val_acc: 0.8175\n",
            "Epoch 873/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4369 - acc: 0.8178 - val_loss: 0.4686 - val_acc: 0.8165\n",
            "Epoch 874/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4315 - acc: 0.8261 - val_loss: 0.4700 - val_acc: 0.8151\n",
            "Epoch 875/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4299 - acc: 0.8242 - val_loss: 0.4704 - val_acc: 0.8188\n",
            "Epoch 876/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4310 - acc: 0.8237 - val_loss: 0.4737 - val_acc: 0.8198\n",
            "Epoch 877/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4348 - acc: 0.8208 - val_loss: 0.4732 - val_acc: 0.8194\n",
            "Epoch 878/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4309 - acc: 0.8191 - val_loss: 0.4725 - val_acc: 0.8178\n",
            "Epoch 879/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4364 - acc: 0.8185 - val_loss: 0.4705 - val_acc: 0.8181\n",
            "Epoch 880/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4351 - acc: 0.8212 - val_loss: 0.4688 - val_acc: 0.8214\n",
            "Epoch 881/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4347 - acc: 0.8237 - val_loss: 0.4683 - val_acc: 0.8218\n",
            "Epoch 882/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4341 - acc: 0.8213 - val_loss: 0.4658 - val_acc: 0.8224\n",
            "Epoch 883/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4379 - acc: 0.8187 - val_loss: 0.4627 - val_acc: 0.8204\n",
            "Epoch 884/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4313 - acc: 0.8240 - val_loss: 0.4609 - val_acc: 0.8234\n",
            "Epoch 885/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4308 - acc: 0.8265 - val_loss: 0.4631 - val_acc: 0.8221\n",
            "Epoch 886/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4312 - acc: 0.8226 - val_loss: 0.4675 - val_acc: 0.8211\n",
            "Epoch 887/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4263 - acc: 0.8264 - val_loss: 0.4726 - val_acc: 0.8214\n",
            "Epoch 888/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4291 - acc: 0.8257 - val_loss: 0.4726 - val_acc: 0.8234\n",
            "Epoch 889/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4243 - acc: 0.8282 - val_loss: 0.4723 - val_acc: 0.8234\n",
            "Epoch 890/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4300 - acc: 0.8306 - val_loss: 0.4712 - val_acc: 0.8247\n",
            "Epoch 891/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4286 - acc: 0.8272 - val_loss: 0.4668 - val_acc: 0.8224\n",
            "Epoch 892/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4300 - acc: 0.8248 - val_loss: 0.4666 - val_acc: 0.8214\n",
            "Epoch 893/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4297 - acc: 0.8213 - val_loss: 0.4691 - val_acc: 0.8228\n",
            "Epoch 894/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4184 - acc: 0.8299 - val_loss: 0.4708 - val_acc: 0.8247\n",
            "Epoch 895/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4308 - acc: 0.8223 - val_loss: 0.4729 - val_acc: 0.8257\n",
            "Epoch 896/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4350 - acc: 0.8191 - val_loss: 0.4733 - val_acc: 0.8228\n",
            "Epoch 897/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4262 - acc: 0.8263 - val_loss: 0.4703 - val_acc: 0.8231\n",
            "Epoch 898/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4246 - acc: 0.8271 - val_loss: 0.4674 - val_acc: 0.8257\n",
            "Epoch 899/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4309 - acc: 0.8258 - val_loss: 0.4643 - val_acc: 0.8264\n",
            "Epoch 900/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4270 - acc: 0.8250 - val_loss: 0.4614 - val_acc: 0.8237\n",
            "Epoch 901/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4321 - acc: 0.8252 - val_loss: 0.4630 - val_acc: 0.8231\n",
            "Epoch 902/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4272 - acc: 0.8294 - val_loss: 0.4645 - val_acc: 0.8251\n",
            "Epoch 903/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4271 - acc: 0.8251 - val_loss: 0.4655 - val_acc: 0.8247\n",
            "Epoch 904/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4278 - acc: 0.8229 - val_loss: 0.4675 - val_acc: 0.8221\n",
            "Epoch 905/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4274 - acc: 0.8261 - val_loss: 0.4671 - val_acc: 0.8218\n",
            "Epoch 906/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4232 - acc: 0.8217 - val_loss: 0.4645 - val_acc: 0.8251\n",
            "Epoch 907/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4259 - acc: 0.8249 - val_loss: 0.4646 - val_acc: 0.8257\n",
            "Epoch 908/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4321 - acc: 0.8242 - val_loss: 0.4656 - val_acc: 0.8271\n",
            "Epoch 909/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4253 - acc: 0.8254 - val_loss: 0.4666 - val_acc: 0.8277\n",
            "Epoch 910/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4259 - acc: 0.8269 - val_loss: 0.4679 - val_acc: 0.8261\n",
            "Epoch 911/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4233 - acc: 0.8283 - val_loss: 0.4678 - val_acc: 0.8244\n",
            "Epoch 912/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4282 - acc: 0.8242 - val_loss: 0.4664 - val_acc: 0.8247\n",
            "Epoch 913/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4250 - acc: 0.8266 - val_loss: 0.4656 - val_acc: 0.8254\n",
            "Epoch 914/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4250 - acc: 0.8273 - val_loss: 0.4635 - val_acc: 0.8267\n",
            "Epoch 915/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4215 - acc: 0.8260 - val_loss: 0.4651 - val_acc: 0.8261\n",
            "Epoch 916/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4261 - acc: 0.8256 - val_loss: 0.4638 - val_acc: 0.8264\n",
            "Epoch 917/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4215 - acc: 0.8274 - val_loss: 0.4618 - val_acc: 0.8251\n",
            "Epoch 918/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4256 - acc: 0.8271 - val_loss: 0.4610 - val_acc: 0.8241\n",
            "Epoch 919/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4221 - acc: 0.8319 - val_loss: 0.4604 - val_acc: 0.8284\n",
            "Epoch 920/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4237 - acc: 0.8280 - val_loss: 0.4626 - val_acc: 0.8277\n",
            "Epoch 921/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4235 - acc: 0.8246 - val_loss: 0.4660 - val_acc: 0.8261\n",
            "Epoch 922/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4166 - acc: 0.8318 - val_loss: 0.4670 - val_acc: 0.8274\n",
            "Epoch 923/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4297 - acc: 0.8262 - val_loss: 0.4651 - val_acc: 0.8280\n",
            "Epoch 924/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4263 - acc: 0.8274 - val_loss: 0.4650 - val_acc: 0.8261\n",
            "Epoch 925/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4286 - acc: 0.8215 - val_loss: 0.4646 - val_acc: 0.8221\n",
            "Epoch 926/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4240 - acc: 0.8266 - val_loss: 0.4636 - val_acc: 0.8221\n",
            "Epoch 927/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4256 - acc: 0.8271 - val_loss: 0.4625 - val_acc: 0.8244\n",
            "Epoch 928/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4192 - acc: 0.8270 - val_loss: 0.4635 - val_acc: 0.8254\n",
            "Epoch 929/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4319 - acc: 0.8247 - val_loss: 0.4631 - val_acc: 0.8234\n",
            "Epoch 930/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4225 - acc: 0.8275 - val_loss: 0.4647 - val_acc: 0.8221\n",
            "Epoch 931/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4257 - acc: 0.8289 - val_loss: 0.4640 - val_acc: 0.8228\n",
            "Epoch 932/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4253 - acc: 0.8231 - val_loss: 0.4646 - val_acc: 0.8214\n",
            "Epoch 933/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4241 - acc: 0.8271 - val_loss: 0.4635 - val_acc: 0.8244\n",
            "Epoch 934/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4241 - acc: 0.8251 - val_loss: 0.4624 - val_acc: 0.8271\n",
            "Epoch 935/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4255 - acc: 0.8275 - val_loss: 0.4621 - val_acc: 0.8261\n",
            "Epoch 936/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4177 - acc: 0.8285 - val_loss: 0.4623 - val_acc: 0.8224\n",
            "Epoch 937/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4196 - acc: 0.8274 - val_loss: 0.4624 - val_acc: 0.8214\n",
            "Epoch 938/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4196 - acc: 0.8307 - val_loss: 0.4612 - val_acc: 0.8237\n",
            "Epoch 939/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4207 - acc: 0.8256 - val_loss: 0.4611 - val_acc: 0.8228\n",
            "Epoch 940/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4158 - acc: 0.8327 - val_loss: 0.4617 - val_acc: 0.8247\n",
            "Epoch 941/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4216 - acc: 0.8310 - val_loss: 0.4619 - val_acc: 0.8237\n",
            "Epoch 942/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4242 - acc: 0.8298 - val_loss: 0.4632 - val_acc: 0.8228\n",
            "Epoch 943/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4220 - acc: 0.8253 - val_loss: 0.4653 - val_acc: 0.8237\n",
            "Epoch 944/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4215 - acc: 0.8272 - val_loss: 0.4638 - val_acc: 0.8204\n",
            "Epoch 945/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4262 - acc: 0.8256 - val_loss: 0.4606 - val_acc: 0.8228\n",
            "Epoch 946/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4312 - acc: 0.8224 - val_loss: 0.4598 - val_acc: 0.8234\n",
            "Epoch 947/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4236 - acc: 0.8256 - val_loss: 0.4570 - val_acc: 0.8254\n",
            "Epoch 948/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4238 - acc: 0.8248 - val_loss: 0.4589 - val_acc: 0.8237\n",
            "Epoch 949/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4256 - acc: 0.8276 - val_loss: 0.4606 - val_acc: 0.8237\n",
            "Epoch 950/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4215 - acc: 0.8245 - val_loss: 0.4620 - val_acc: 0.8257\n",
            "Epoch 951/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4202 - acc: 0.8302 - val_loss: 0.4642 - val_acc: 0.8254\n",
            "Epoch 952/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4166 - acc: 0.8299 - val_loss: 0.4637 - val_acc: 0.8254\n",
            "Epoch 953/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4218 - acc: 0.8254 - val_loss: 0.4630 - val_acc: 0.8247\n",
            "Epoch 954/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4204 - acc: 0.8278 - val_loss: 0.4640 - val_acc: 0.8251\n",
            "Epoch 955/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4165 - acc: 0.8307 - val_loss: 0.4651 - val_acc: 0.8257\n",
            "Epoch 956/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4288 - acc: 0.8274 - val_loss: 0.4671 - val_acc: 0.8290\n",
            "Epoch 957/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4237 - acc: 0.8254 - val_loss: 0.4689 - val_acc: 0.8294\n",
            "Epoch 958/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4189 - acc: 0.8283 - val_loss: 0.4698 - val_acc: 0.8294\n",
            "Epoch 959/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4196 - acc: 0.8276 - val_loss: 0.4677 - val_acc: 0.8290\n",
            "Epoch 960/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4193 - acc: 0.8292 - val_loss: 0.4627 - val_acc: 0.8254\n",
            "Epoch 961/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4223 - acc: 0.8305 - val_loss: 0.4631 - val_acc: 0.8247\n",
            "Epoch 962/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4185 - acc: 0.8305 - val_loss: 0.4638 - val_acc: 0.8241\n",
            "Epoch 963/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4178 - acc: 0.8333 - val_loss: 0.4648 - val_acc: 0.8290\n",
            "Epoch 964/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4190 - acc: 0.8285 - val_loss: 0.4685 - val_acc: 0.8267\n",
            "Epoch 965/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4171 - acc: 0.8309 - val_loss: 0.4703 - val_acc: 0.8228\n",
            "Epoch 966/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4136 - acc: 0.8342 - val_loss: 0.4664 - val_acc: 0.8237\n",
            "Epoch 967/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4205 - acc: 0.8261 - val_loss: 0.4610 - val_acc: 0.8247\n",
            "Epoch 968/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4178 - acc: 0.8270 - val_loss: 0.4599 - val_acc: 0.8244\n",
            "Epoch 969/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4208 - acc: 0.8294 - val_loss: 0.4620 - val_acc: 0.8237\n",
            "Epoch 970/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4118 - acc: 0.8326 - val_loss: 0.4648 - val_acc: 0.8257\n",
            "Epoch 971/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4154 - acc: 0.8261 - val_loss: 0.4656 - val_acc: 0.8261\n",
            "Epoch 972/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4147 - acc: 0.8288 - val_loss: 0.4651 - val_acc: 0.8274\n",
            "Epoch 973/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4136 - acc: 0.8262 - val_loss: 0.4638 - val_acc: 0.8257\n",
            "Epoch 974/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4128 - acc: 0.8325 - val_loss: 0.4621 - val_acc: 0.8287\n",
            "Epoch 975/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4179 - acc: 0.8302 - val_loss: 0.4612 - val_acc: 0.8271\n",
            "Epoch 976/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4103 - acc: 0.8348 - val_loss: 0.4621 - val_acc: 0.8264\n",
            "Epoch 977/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4168 - acc: 0.8281 - val_loss: 0.4629 - val_acc: 0.8261\n",
            "Epoch 978/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4217 - acc: 0.8312 - val_loss: 0.4631 - val_acc: 0.8257\n",
            "Epoch 979/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4154 - acc: 0.8286 - val_loss: 0.4656 - val_acc: 0.8247\n",
            "Epoch 980/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4086 - acc: 0.8311 - val_loss: 0.4678 - val_acc: 0.8261\n",
            "Epoch 981/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4097 - acc: 0.8342 - val_loss: 0.4697 - val_acc: 0.8254\n",
            "Epoch 982/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4060 - acc: 0.8352 - val_loss: 0.4698 - val_acc: 0.8251\n",
            "Epoch 983/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4183 - acc: 0.8331 - val_loss: 0.4685 - val_acc: 0.8287\n",
            "Epoch 984/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4080 - acc: 0.8343 - val_loss: 0.4668 - val_acc: 0.8297\n",
            "Epoch 985/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4095 - acc: 0.8326 - val_loss: 0.4625 - val_acc: 0.8277\n",
            "Epoch 986/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4132 - acc: 0.8339 - val_loss: 0.4594 - val_acc: 0.8264\n",
            "Epoch 987/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4122 - acc: 0.8346 - val_loss: 0.4596 - val_acc: 0.8257\n",
            "Epoch 988/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4127 - acc: 0.8285 - val_loss: 0.4615 - val_acc: 0.8241\n",
            "Epoch 989/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4134 - acc: 0.8283 - val_loss: 0.4639 - val_acc: 0.8294\n",
            "Epoch 990/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4096 - acc: 0.8372 - val_loss: 0.4698 - val_acc: 0.8254\n",
            "Epoch 991/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4132 - acc: 0.8294 - val_loss: 0.4708 - val_acc: 0.8231\n",
            "Epoch 992/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4146 - acc: 0.8338 - val_loss: 0.4675 - val_acc: 0.8244\n",
            "Epoch 993/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4123 - acc: 0.8363 - val_loss: 0.4625 - val_acc: 0.8254\n",
            "Epoch 994/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4116 - acc: 0.8349 - val_loss: 0.4615 - val_acc: 0.8224\n",
            "Epoch 995/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4108 - acc: 0.8323 - val_loss: 0.4600 - val_acc: 0.8257\n",
            "Epoch 996/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4128 - acc: 0.8267 - val_loss: 0.4599 - val_acc: 0.8247\n",
            "Epoch 997/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4106 - acc: 0.8363 - val_loss: 0.4625 - val_acc: 0.8277\n",
            "Epoch 998/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4138 - acc: 0.8295 - val_loss: 0.4645 - val_acc: 0.8271\n",
            "Epoch 999/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4150 - acc: 0.8335 - val_loss: 0.4624 - val_acc: 0.8267\n",
            "Epoch 1000/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4128 - acc: 0.8371 - val_loss: 0.4596 - val_acc: 0.8271\n",
            "Epoch 1001/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4115 - acc: 0.8317 - val_loss: 0.4599 - val_acc: 0.8274\n",
            "Epoch 1002/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4071 - acc: 0.8328 - val_loss: 0.4628 - val_acc: 0.8234\n",
            "Epoch 1003/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4027 - acc: 0.8373 - val_loss: 0.4683 - val_acc: 0.8231\n",
            "Epoch 1004/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4076 - acc: 0.8291 - val_loss: 0.4686 - val_acc: 0.8218\n",
            "Epoch 1005/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4094 - acc: 0.8348 - val_loss: 0.4659 - val_acc: 0.8218\n",
            "Epoch 1006/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4109 - acc: 0.8327 - val_loss: 0.4634 - val_acc: 0.8228\n",
            "Epoch 1007/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4062 - acc: 0.8366 - val_loss: 0.4623 - val_acc: 0.8241\n",
            "Epoch 1008/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4141 - acc: 0.8302 - val_loss: 0.4605 - val_acc: 0.8244\n",
            "Epoch 1009/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.4089 - acc: 0.8381 - val_loss: 0.4601 - val_acc: 0.8261\n",
            "Epoch 1010/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4116 - acc: 0.8322 - val_loss: 0.4596 - val_acc: 0.8271\n",
            "Epoch 1011/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4079 - acc: 0.8347 - val_loss: 0.4630 - val_acc: 0.8228\n",
            "Epoch 1012/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4083 - acc: 0.8332 - val_loss: 0.4662 - val_acc: 0.8244\n",
            "Epoch 1013/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4119 - acc: 0.8321 - val_loss: 0.4624 - val_acc: 0.8234\n",
            "Epoch 1014/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4103 - acc: 0.8329 - val_loss: 0.4609 - val_acc: 0.8274\n",
            "Epoch 1015/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4139 - acc: 0.8297 - val_loss: 0.4611 - val_acc: 0.8261\n",
            "Epoch 1016/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4083 - acc: 0.8356 - val_loss: 0.4614 - val_acc: 0.8251\n",
            "Epoch 1017/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4091 - acc: 0.8336 - val_loss: 0.4633 - val_acc: 0.8284\n",
            "Epoch 1018/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4110 - acc: 0.8342 - val_loss: 0.4666 - val_acc: 0.8231\n",
            "Epoch 1019/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4122 - acc: 0.8329 - val_loss: 0.4637 - val_acc: 0.8247\n",
            "Epoch 1020/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4072 - acc: 0.8380 - val_loss: 0.4605 - val_acc: 0.8257\n",
            "Epoch 1021/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4109 - acc: 0.8321 - val_loss: 0.4581 - val_acc: 0.8267\n",
            "Epoch 1022/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4154 - acc: 0.8300 - val_loss: 0.4586 - val_acc: 0.8280\n",
            "Epoch 1023/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4121 - acc: 0.8349 - val_loss: 0.4598 - val_acc: 0.8304\n",
            "Epoch 1024/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4094 - acc: 0.8331 - val_loss: 0.4584 - val_acc: 0.8290\n",
            "Epoch 1025/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4153 - acc: 0.8308 - val_loss: 0.4561 - val_acc: 0.8264\n",
            "Epoch 1026/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3986 - acc: 0.8396 - val_loss: 0.4573 - val_acc: 0.8247\n",
            "Epoch 1027/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4021 - acc: 0.8392 - val_loss: 0.4611 - val_acc: 0.8237\n",
            "Epoch 1028/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4167 - acc: 0.8297 - val_loss: 0.4652 - val_acc: 0.8228\n",
            "Epoch 1029/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4067 - acc: 0.8353 - val_loss: 0.4700 - val_acc: 0.8234\n",
            "Epoch 1030/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4051 - acc: 0.8377 - val_loss: 0.4714 - val_acc: 0.8234\n",
            "Epoch 1031/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4118 - acc: 0.8330 - val_loss: 0.4704 - val_acc: 0.8251\n",
            "Epoch 1032/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4064 - acc: 0.8340 - val_loss: 0.4679 - val_acc: 0.8257\n",
            "Epoch 1033/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3986 - acc: 0.8382 - val_loss: 0.4620 - val_acc: 0.8274\n",
            "Epoch 1034/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4040 - acc: 0.8351 - val_loss: 0.4597 - val_acc: 0.8261\n",
            "Epoch 1035/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4059 - acc: 0.8342 - val_loss: 0.4597 - val_acc: 0.8251\n",
            "Epoch 1036/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4082 - acc: 0.8305 - val_loss: 0.4607 - val_acc: 0.8267\n",
            "Epoch 1037/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4039 - acc: 0.8344 - val_loss: 0.4635 - val_acc: 0.8221\n",
            "Epoch 1038/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4105 - acc: 0.8344 - val_loss: 0.4645 - val_acc: 0.8234\n",
            "Epoch 1039/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4051 - acc: 0.8330 - val_loss: 0.4614 - val_acc: 0.8244\n",
            "Epoch 1040/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4071 - acc: 0.8337 - val_loss: 0.4569 - val_acc: 0.8290\n",
            "Epoch 1041/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4058 - acc: 0.8372 - val_loss: 0.4542 - val_acc: 0.8290\n",
            "Epoch 1042/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4077 - acc: 0.8329 - val_loss: 0.4547 - val_acc: 0.8287\n",
            "Epoch 1043/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4110 - acc: 0.8339 - val_loss: 0.4575 - val_acc: 0.8271\n",
            "Epoch 1044/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4071 - acc: 0.8328 - val_loss: 0.4632 - val_acc: 0.8271\n",
            "Epoch 1045/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4046 - acc: 0.8375 - val_loss: 0.4664 - val_acc: 0.8271\n",
            "Epoch 1046/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3980 - acc: 0.8403 - val_loss: 0.4653 - val_acc: 0.8284\n",
            "Epoch 1047/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4021 - acc: 0.8358 - val_loss: 0.4622 - val_acc: 0.8277\n",
            "Epoch 1048/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3973 - acc: 0.8406 - val_loss: 0.4576 - val_acc: 0.8280\n",
            "Epoch 1049/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4019 - acc: 0.8365 - val_loss: 0.4561 - val_acc: 0.8264\n",
            "Epoch 1050/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4067 - acc: 0.8344 - val_loss: 0.4542 - val_acc: 0.8257\n",
            "Epoch 1051/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4012 - acc: 0.8348 - val_loss: 0.4550 - val_acc: 0.8287\n",
            "Epoch 1052/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4054 - acc: 0.8385 - val_loss: 0.4567 - val_acc: 0.8310\n",
            "Epoch 1053/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3995 - acc: 0.8370 - val_loss: 0.4588 - val_acc: 0.8310\n",
            "Epoch 1054/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4052 - acc: 0.8354 - val_loss: 0.4596 - val_acc: 0.8310\n",
            "Epoch 1055/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4003 - acc: 0.8379 - val_loss: 0.4582 - val_acc: 0.8284\n",
            "Epoch 1056/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4084 - acc: 0.8336 - val_loss: 0.4552 - val_acc: 0.8304\n",
            "Epoch 1057/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3991 - acc: 0.8359 - val_loss: 0.4532 - val_acc: 0.8264\n",
            "Epoch 1058/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4023 - acc: 0.8392 - val_loss: 0.4536 - val_acc: 0.8244\n",
            "Epoch 1059/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4007 - acc: 0.8371 - val_loss: 0.4550 - val_acc: 0.8261\n",
            "Epoch 1060/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3949 - acc: 0.8385 - val_loss: 0.4597 - val_acc: 0.8244\n",
            "Epoch 1061/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4016 - acc: 0.8349 - val_loss: 0.4612 - val_acc: 0.8267\n",
            "Epoch 1062/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4056 - acc: 0.8362 - val_loss: 0.4590 - val_acc: 0.8257\n",
            "Epoch 1063/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4011 - acc: 0.8383 - val_loss: 0.4578 - val_acc: 0.8294\n",
            "Epoch 1064/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4016 - acc: 0.8364 - val_loss: 0.4561 - val_acc: 0.8261\n",
            "Epoch 1065/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4009 - acc: 0.8333 - val_loss: 0.4582 - val_acc: 0.8254\n",
            "Epoch 1066/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4022 - acc: 0.8349 - val_loss: 0.4598 - val_acc: 0.8261\n",
            "Epoch 1067/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3985 - acc: 0.8408 - val_loss: 0.4577 - val_acc: 0.8271\n",
            "Epoch 1068/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4036 - acc: 0.8352 - val_loss: 0.4604 - val_acc: 0.8267\n",
            "Epoch 1069/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4048 - acc: 0.8405 - val_loss: 0.4625 - val_acc: 0.8284\n",
            "Epoch 1070/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4079 - acc: 0.8328 - val_loss: 0.4634 - val_acc: 0.8297\n",
            "Epoch 1071/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4023 - acc: 0.8328 - val_loss: 0.4632 - val_acc: 0.8271\n",
            "Epoch 1072/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4020 - acc: 0.8411 - val_loss: 0.4570 - val_acc: 0.8271\n",
            "Epoch 1073/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3994 - acc: 0.8360 - val_loss: 0.4566 - val_acc: 0.8257\n",
            "Epoch 1074/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3982 - acc: 0.8399 - val_loss: 0.4549 - val_acc: 0.8271\n",
            "Epoch 1075/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4054 - acc: 0.8379 - val_loss: 0.4535 - val_acc: 0.8261\n",
            "Epoch 1076/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4034 - acc: 0.8380 - val_loss: 0.4581 - val_acc: 0.8254\n",
            "Epoch 1077/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3967 - acc: 0.8380 - val_loss: 0.4633 - val_acc: 0.8297\n",
            "Epoch 1078/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4051 - acc: 0.8383 - val_loss: 0.4639 - val_acc: 0.8284\n",
            "Epoch 1079/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3967 - acc: 0.8392 - val_loss: 0.4600 - val_acc: 0.8300\n",
            "Epoch 1080/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4000 - acc: 0.8347 - val_loss: 0.4547 - val_acc: 0.8310\n",
            "Epoch 1081/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4008 - acc: 0.8411 - val_loss: 0.4529 - val_acc: 0.8284\n",
            "Epoch 1082/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4076 - acc: 0.8353 - val_loss: 0.4530 - val_acc: 0.8280\n",
            "Epoch 1083/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4044 - acc: 0.8364 - val_loss: 0.4567 - val_acc: 0.8280\n",
            "Epoch 1084/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4012 - acc: 0.8399 - val_loss: 0.4591 - val_acc: 0.8254\n",
            "Epoch 1085/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4022 - acc: 0.8342 - val_loss: 0.4580 - val_acc: 0.8294\n",
            "Epoch 1086/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3934 - acc: 0.8385 - val_loss: 0.4571 - val_acc: 0.8284\n",
            "Epoch 1087/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4043 - acc: 0.8352 - val_loss: 0.4569 - val_acc: 0.8264\n",
            "Epoch 1088/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3950 - acc: 0.8399 - val_loss: 0.4579 - val_acc: 0.8274\n",
            "Epoch 1089/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4043 - acc: 0.8359 - val_loss: 0.4586 - val_acc: 0.8274\n",
            "Epoch 1090/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3922 - acc: 0.8381 - val_loss: 0.4596 - val_acc: 0.8280\n",
            "Epoch 1091/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4025 - acc: 0.8363 - val_loss: 0.4589 - val_acc: 0.8274\n",
            "Epoch 1092/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4009 - acc: 0.8378 - val_loss: 0.4567 - val_acc: 0.8261\n",
            "Epoch 1093/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8390 - val_loss: 0.4543 - val_acc: 0.8300\n",
            "Epoch 1094/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3984 - acc: 0.8404 - val_loss: 0.4540 - val_acc: 0.8297\n",
            "Epoch 1095/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3926 - acc: 0.8381 - val_loss: 0.4562 - val_acc: 0.8277\n",
            "Epoch 1096/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3978 - acc: 0.8402 - val_loss: 0.4617 - val_acc: 0.8247\n",
            "Epoch 1097/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4001 - acc: 0.8404 - val_loss: 0.4638 - val_acc: 0.8264\n",
            "Epoch 1098/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4061 - acc: 0.8363 - val_loss: 0.4610 - val_acc: 0.8264\n",
            "Epoch 1099/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3976 - acc: 0.8408 - val_loss: 0.4606 - val_acc: 0.8261\n",
            "Epoch 1100/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3991 - acc: 0.8387 - val_loss: 0.4587 - val_acc: 0.8284\n",
            "Epoch 1101/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3887 - acc: 0.8386 - val_loss: 0.4587 - val_acc: 0.8271\n",
            "Epoch 1102/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.4006 - acc: 0.8391 - val_loss: 0.4602 - val_acc: 0.8251\n",
            "Epoch 1103/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4006 - acc: 0.8415 - val_loss: 0.4606 - val_acc: 0.8264\n",
            "Epoch 1104/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3947 - acc: 0.8410 - val_loss: 0.4585 - val_acc: 0.8300\n",
            "Epoch 1105/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3941 - acc: 0.8405 - val_loss: 0.4547 - val_acc: 0.8300\n",
            "Epoch 1106/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3956 - acc: 0.8400 - val_loss: 0.4561 - val_acc: 0.8284\n",
            "Epoch 1107/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3968 - acc: 0.8363 - val_loss: 0.4583 - val_acc: 0.8284\n",
            "Epoch 1108/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4015 - acc: 0.8360 - val_loss: 0.4575 - val_acc: 0.8310\n",
            "Epoch 1109/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3983 - acc: 0.8395 - val_loss: 0.4543 - val_acc: 0.8323\n",
            "Epoch 1110/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4012 - acc: 0.8359 - val_loss: 0.4530 - val_acc: 0.8317\n",
            "Epoch 1111/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3941 - acc: 0.8424 - val_loss: 0.4526 - val_acc: 0.8304\n",
            "Epoch 1112/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3952 - acc: 0.8423 - val_loss: 0.4538 - val_acc: 0.8294\n",
            "Epoch 1113/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3967 - acc: 0.8388 - val_loss: 0.4540 - val_acc: 0.8317\n",
            "Epoch 1114/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4030 - acc: 0.8373 - val_loss: 0.4560 - val_acc: 0.8300\n",
            "Epoch 1115/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3995 - acc: 0.8372 - val_loss: 0.4575 - val_acc: 0.8304\n",
            "Epoch 1116/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3969 - acc: 0.8373 - val_loss: 0.4591 - val_acc: 0.8307\n",
            "Epoch 1117/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3894 - acc: 0.8414 - val_loss: 0.4580 - val_acc: 0.8287\n",
            "Epoch 1118/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3971 - acc: 0.8393 - val_loss: 0.4550 - val_acc: 0.8267\n",
            "Epoch 1119/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3963 - acc: 0.8399 - val_loss: 0.4536 - val_acc: 0.8247\n",
            "Epoch 1120/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3911 - acc: 0.8398 - val_loss: 0.4526 - val_acc: 0.8284\n",
            "Epoch 1121/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4002 - acc: 0.8393 - val_loss: 0.4532 - val_acc: 0.8304\n",
            "Epoch 1122/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3908 - acc: 0.8404 - val_loss: 0.4547 - val_acc: 0.8313\n",
            "Epoch 1123/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.4040 - acc: 0.8352 - val_loss: 0.4564 - val_acc: 0.8337\n",
            "Epoch 1124/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3953 - acc: 0.8352 - val_loss: 0.4555 - val_acc: 0.8327\n",
            "Epoch 1125/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.4012 - acc: 0.8354 - val_loss: 0.4546 - val_acc: 0.8304\n",
            "Epoch 1126/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3973 - acc: 0.8371 - val_loss: 0.4523 - val_acc: 0.8300\n",
            "Epoch 1127/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3891 - acc: 0.8409 - val_loss: 0.4522 - val_acc: 0.8277\n",
            "Epoch 1128/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3975 - acc: 0.8380 - val_loss: 0.4517 - val_acc: 0.8297\n",
            "Epoch 1129/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3929 - acc: 0.8395 - val_loss: 0.4523 - val_acc: 0.8307\n",
            "Epoch 1130/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3989 - acc: 0.8389 - val_loss: 0.4536 - val_acc: 0.8313\n",
            "Epoch 1131/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3885 - acc: 0.8455 - val_loss: 0.4554 - val_acc: 0.8347\n",
            "Epoch 1132/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3969 - acc: 0.8371 - val_loss: 0.4590 - val_acc: 0.8353\n",
            "Epoch 1133/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3926 - acc: 0.8400 - val_loss: 0.4600 - val_acc: 0.8347\n",
            "Epoch 1134/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.4029 - acc: 0.8356 - val_loss: 0.4522 - val_acc: 0.8330\n",
            "Epoch 1135/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3945 - acc: 0.8400 - val_loss: 0.4509 - val_acc: 0.8277\n",
            "Epoch 1136/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3932 - acc: 0.8361 - val_loss: 0.4492 - val_acc: 0.8247\n",
            "Epoch 1137/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3942 - acc: 0.8391 - val_loss: 0.4518 - val_acc: 0.8257\n",
            "Epoch 1138/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3921 - acc: 0.8395 - val_loss: 0.4536 - val_acc: 0.8264\n",
            "Epoch 1139/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3957 - acc: 0.8410 - val_loss: 0.4501 - val_acc: 0.8290\n",
            "Epoch 1140/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3900 - acc: 0.8403 - val_loss: 0.4500 - val_acc: 0.8304\n",
            "Epoch 1141/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3912 - acc: 0.8372 - val_loss: 0.4519 - val_acc: 0.8323\n",
            "Epoch 1142/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3905 - acc: 0.8395 - val_loss: 0.4534 - val_acc: 0.8337\n",
            "Epoch 1143/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3944 - acc: 0.8415 - val_loss: 0.4550 - val_acc: 0.8310\n",
            "Epoch 1144/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3905 - acc: 0.8390 - val_loss: 0.4545 - val_acc: 0.8264\n",
            "Epoch 1145/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3850 - acc: 0.8444 - val_loss: 0.4515 - val_acc: 0.8297\n",
            "Epoch 1146/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3965 - acc: 0.8388 - val_loss: 0.4495 - val_acc: 0.8297\n",
            "Epoch 1147/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3942 - acc: 0.8428 - val_loss: 0.4517 - val_acc: 0.8323\n",
            "Epoch 1148/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8399 - val_loss: 0.4548 - val_acc: 0.8327\n",
            "Epoch 1149/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3945 - acc: 0.8405 - val_loss: 0.4563 - val_acc: 0.8307\n",
            "Epoch 1150/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3917 - acc: 0.8419 - val_loss: 0.4529 - val_acc: 0.8304\n",
            "Epoch 1151/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3871 - acc: 0.8428 - val_loss: 0.4477 - val_acc: 0.8350\n",
            "Epoch 1152/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3883 - acc: 0.8422 - val_loss: 0.4461 - val_acc: 0.8353\n",
            "Epoch 1153/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3912 - acc: 0.8414 - val_loss: 0.4469 - val_acc: 0.8323\n",
            "Epoch 1154/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3866 - acc: 0.8416 - val_loss: 0.4481 - val_acc: 0.8330\n",
            "Epoch 1155/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3843 - acc: 0.8454 - val_loss: 0.4541 - val_acc: 0.8304\n",
            "Epoch 1156/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3871 - acc: 0.8434 - val_loss: 0.4556 - val_acc: 0.8313\n",
            "Epoch 1157/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3841 - acc: 0.8401 - val_loss: 0.4556 - val_acc: 0.8304\n",
            "Epoch 1158/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3869 - acc: 0.8422 - val_loss: 0.4535 - val_acc: 0.8294\n",
            "Epoch 1159/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3938 - acc: 0.8407 - val_loss: 0.4534 - val_acc: 0.8297\n",
            "Epoch 1160/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3901 - acc: 0.8390 - val_loss: 0.4529 - val_acc: 0.8287\n",
            "Epoch 1161/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3902 - acc: 0.8439 - val_loss: 0.4506 - val_acc: 0.8284\n",
            "Epoch 1162/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3942 - acc: 0.8395 - val_loss: 0.4513 - val_acc: 0.8274\n",
            "Epoch 1163/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3968 - acc: 0.8393 - val_loss: 0.4544 - val_acc: 0.8284\n",
            "Epoch 1164/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3883 - acc: 0.8421 - val_loss: 0.4556 - val_acc: 0.8320\n",
            "Epoch 1165/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3893 - acc: 0.8393 - val_loss: 0.4577 - val_acc: 0.8294\n",
            "Epoch 1166/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3916 - acc: 0.8395 - val_loss: 0.4541 - val_acc: 0.8304\n",
            "Epoch 1167/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3879 - acc: 0.8400 - val_loss: 0.4494 - val_acc: 0.8294\n",
            "Epoch 1168/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3798 - acc: 0.8466 - val_loss: 0.4498 - val_acc: 0.8274\n",
            "Epoch 1169/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3859 - acc: 0.8460 - val_loss: 0.4536 - val_acc: 0.8294\n",
            "Epoch 1170/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3948 - acc: 0.8438 - val_loss: 0.4564 - val_acc: 0.8307\n",
            "Epoch 1171/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3876 - acc: 0.8436 - val_loss: 0.4581 - val_acc: 0.8313\n",
            "Epoch 1172/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3805 - acc: 0.8444 - val_loss: 0.4630 - val_acc: 0.8297\n",
            "Epoch 1173/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3970 - acc: 0.8395 - val_loss: 0.4614 - val_acc: 0.8290\n",
            "Epoch 1174/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3922 - acc: 0.8408 - val_loss: 0.4588 - val_acc: 0.8274\n",
            "Epoch 1175/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3884 - acc: 0.8423 - val_loss: 0.4553 - val_acc: 0.8280\n",
            "Epoch 1176/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3909 - acc: 0.8439 - val_loss: 0.4521 - val_acc: 0.8287\n",
            "Epoch 1177/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3857 - acc: 0.8439 - val_loss: 0.4543 - val_acc: 0.8300\n",
            "Epoch 1178/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3916 - acc: 0.8425 - val_loss: 0.4592 - val_acc: 0.8300\n",
            "Epoch 1179/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3867 - acc: 0.8474 - val_loss: 0.4607 - val_acc: 0.8313\n",
            "Epoch 1180/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3889 - acc: 0.8423 - val_loss: 0.4577 - val_acc: 0.8337\n",
            "Epoch 1181/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3879 - acc: 0.8447 - val_loss: 0.4503 - val_acc: 0.8320\n",
            "Epoch 1182/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.3923 - acc: 0.8372 - val_loss: 0.4490 - val_acc: 0.8330\n",
            "Epoch 1183/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3901 - acc: 0.8431 - val_loss: 0.4523 - val_acc: 0.8353\n",
            "Epoch 1184/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3916 - acc: 0.8417 - val_loss: 0.4593 - val_acc: 0.8320\n",
            "Epoch 1185/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3861 - acc: 0.8437 - val_loss: 0.4617 - val_acc: 0.8287\n",
            "Epoch 1186/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3870 - acc: 0.8414 - val_loss: 0.4572 - val_acc: 0.8277\n",
            "Epoch 1187/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3875 - acc: 0.8425 - val_loss: 0.4572 - val_acc: 0.8271\n",
            "Epoch 1188/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3863 - acc: 0.8471 - val_loss: 0.4585 - val_acc: 0.8300\n",
            "Epoch 1189/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3881 - acc: 0.8451 - val_loss: 0.4583 - val_acc: 0.8307\n",
            "Epoch 1190/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3818 - acc: 0.8440 - val_loss: 0.4588 - val_acc: 0.8284\n",
            "Epoch 1191/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3828 - acc: 0.8438 - val_loss: 0.4584 - val_acc: 0.8290\n",
            "Epoch 1192/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3882 - acc: 0.8404 - val_loss: 0.4549 - val_acc: 0.8323\n",
            "Epoch 1193/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3922 - acc: 0.8423 - val_loss: 0.4531 - val_acc: 0.8330\n",
            "Epoch 1194/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3842 - acc: 0.8452 - val_loss: 0.4548 - val_acc: 0.8297\n",
            "Epoch 1195/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3861 - acc: 0.8428 - val_loss: 0.4562 - val_acc: 0.8280\n",
            "Epoch 1196/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3883 - acc: 0.8409 - val_loss: 0.4583 - val_acc: 0.8271\n",
            "Epoch 1197/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3878 - acc: 0.8400 - val_loss: 0.4576 - val_acc: 0.8251\n",
            "Epoch 1198/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3866 - acc: 0.8441 - val_loss: 0.4544 - val_acc: 0.8274\n",
            "Epoch 1199/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3943 - acc: 0.8425 - val_loss: 0.4525 - val_acc: 0.8287\n",
            "Epoch 1200/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3871 - acc: 0.8429 - val_loss: 0.4540 - val_acc: 0.8287\n",
            "Epoch 1201/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3852 - acc: 0.8460 - val_loss: 0.4556 - val_acc: 0.8300\n",
            "Epoch 1202/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3874 - acc: 0.8395 - val_loss: 0.4541 - val_acc: 0.8300\n",
            "Epoch 1203/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3895 - acc: 0.8461 - val_loss: 0.4522 - val_acc: 0.8310\n",
            "Epoch 1204/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3861 - acc: 0.8480 - val_loss: 0.4505 - val_acc: 0.8297\n",
            "Epoch 1205/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3856 - acc: 0.8457 - val_loss: 0.4494 - val_acc: 0.8327\n",
            "Epoch 1206/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3766 - acc: 0.8460 - val_loss: 0.4519 - val_acc: 0.8310\n",
            "Epoch 1207/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3781 - acc: 0.8443 - val_loss: 0.4568 - val_acc: 0.8310\n",
            "Epoch 1208/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3880 - acc: 0.8466 - val_loss: 0.4598 - val_acc: 0.8294\n",
            "Epoch 1209/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3854 - acc: 0.8437 - val_loss: 0.4624 - val_acc: 0.8294\n",
            "Epoch 1210/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3869 - acc: 0.8411 - val_loss: 0.4589 - val_acc: 0.8327\n",
            "Epoch 1211/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3805 - acc: 0.8479 - val_loss: 0.4550 - val_acc: 0.8317\n",
            "Epoch 1212/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3872 - acc: 0.8430 - val_loss: 0.4543 - val_acc: 0.8307\n",
            "Epoch 1213/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3836 - acc: 0.8471 - val_loss: 0.4532 - val_acc: 0.8323\n",
            "Epoch 1214/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3840 - acc: 0.8457 - val_loss: 0.4551 - val_acc: 0.8294\n",
            "Epoch 1215/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3841 - acc: 0.8440 - val_loss: 0.4577 - val_acc: 0.8274\n",
            "Epoch 1216/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3858 - acc: 0.8418 - val_loss: 0.4583 - val_acc: 0.8307\n",
            "Epoch 1217/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3804 - acc: 0.8474 - val_loss: 0.4563 - val_acc: 0.8290\n",
            "Epoch 1218/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3763 - acc: 0.8474 - val_loss: 0.4524 - val_acc: 0.8280\n",
            "Epoch 1219/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3839 - acc: 0.8475 - val_loss: 0.4492 - val_acc: 0.8277\n",
            "Epoch 1220/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3874 - acc: 0.8439 - val_loss: 0.4474 - val_acc: 0.8294\n",
            "Epoch 1221/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3815 - acc: 0.8466 - val_loss: 0.4484 - val_acc: 0.8320\n",
            "Epoch 1222/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3820 - acc: 0.8438 - val_loss: 0.4504 - val_acc: 0.8320\n",
            "Epoch 1223/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3810 - acc: 0.8452 - val_loss: 0.4522 - val_acc: 0.8330\n",
            "Epoch 1224/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3859 - acc: 0.8428 - val_loss: 0.4513 - val_acc: 0.8320\n",
            "Epoch 1225/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3851 - acc: 0.8435 - val_loss: 0.4470 - val_acc: 0.8333\n",
            "Epoch 1226/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3806 - acc: 0.8456 - val_loss: 0.4422 - val_acc: 0.8337\n",
            "Epoch 1227/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3836 - acc: 0.8437 - val_loss: 0.4422 - val_acc: 0.8337\n",
            "Epoch 1228/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3879 - acc: 0.8428 - val_loss: 0.4457 - val_acc: 0.8323\n",
            "Epoch 1229/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3839 - acc: 0.8447 - val_loss: 0.4514 - val_acc: 0.8337\n",
            "Epoch 1230/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3810 - acc: 0.8432 - val_loss: 0.4505 - val_acc: 0.8323\n",
            "Epoch 1231/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3781 - acc: 0.8471 - val_loss: 0.4481 - val_acc: 0.8340\n",
            "Epoch 1232/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3763 - acc: 0.8448 - val_loss: 0.4470 - val_acc: 0.8271\n",
            "Epoch 1233/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3802 - acc: 0.8447 - val_loss: 0.4465 - val_acc: 0.8280\n",
            "Epoch 1234/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3798 - acc: 0.8485 - val_loss: 0.4512 - val_acc: 0.8294\n",
            "Epoch 1235/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3809 - acc: 0.8490 - val_loss: 0.4560 - val_acc: 0.8323\n",
            "Epoch 1236/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3779 - acc: 0.8476 - val_loss: 0.4521 - val_acc: 0.8333\n",
            "Epoch 1237/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3896 - acc: 0.8416 - val_loss: 0.4509 - val_acc: 0.8317\n",
            "Epoch 1238/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3782 - acc: 0.8467 - val_loss: 0.4506 - val_acc: 0.8320\n",
            "Epoch 1239/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3842 - acc: 0.8450 - val_loss: 0.4522 - val_acc: 0.8284\n",
            "Epoch 1240/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3816 - acc: 0.8481 - val_loss: 0.4536 - val_acc: 0.8284\n",
            "Epoch 1241/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.3813 - acc: 0.8453 - val_loss: 0.4508 - val_acc: 0.8323\n",
            "Epoch 1242/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3770 - acc: 0.8463 - val_loss: 0.4483 - val_acc: 0.8304\n",
            "Epoch 1243/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3813 - acc: 0.8442 - val_loss: 0.4484 - val_acc: 0.8353\n",
            "Epoch 1244/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3847 - acc: 0.8447 - val_loss: 0.4504 - val_acc: 0.8333\n",
            "Epoch 1245/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3798 - acc: 0.8457 - val_loss: 0.4529 - val_acc: 0.8343\n",
            "Epoch 1246/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3829 - acc: 0.8438 - val_loss: 0.4565 - val_acc: 0.8356\n",
            "Epoch 1247/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3828 - acc: 0.8423 - val_loss: 0.4515 - val_acc: 0.8313\n",
            "Epoch 1248/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3817 - acc: 0.8446 - val_loss: 0.4485 - val_acc: 0.8317\n",
            "Epoch 1249/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3835 - acc: 0.8410 - val_loss: 0.4499 - val_acc: 0.8323\n",
            "Epoch 1250/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3744 - acc: 0.8509 - val_loss: 0.4536 - val_acc: 0.8297\n",
            "Epoch 1251/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3747 - acc: 0.8510 - val_loss: 0.4566 - val_acc: 0.8300\n",
            "Epoch 1252/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3828 - acc: 0.8440 - val_loss: 0.4562 - val_acc: 0.8323\n",
            "Epoch 1253/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3834 - acc: 0.8445 - val_loss: 0.4544 - val_acc: 0.8330\n",
            "Epoch 1254/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3738 - acc: 0.8489 - val_loss: 0.4564 - val_acc: 0.8290\n",
            "Epoch 1255/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3770 - acc: 0.8475 - val_loss: 0.4545 - val_acc: 0.8284\n",
            "Epoch 1256/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3788 - acc: 0.8463 - val_loss: 0.4559 - val_acc: 0.8294\n",
            "Epoch 1257/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3740 - acc: 0.8498 - val_loss: 0.4562 - val_acc: 0.8300\n",
            "Epoch 1258/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3748 - acc: 0.8501 - val_loss: 0.4516 - val_acc: 0.8304\n",
            "Epoch 1259/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3788 - acc: 0.8463 - val_loss: 0.4477 - val_acc: 0.8307\n",
            "Epoch 1260/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3809 - acc: 0.8436 - val_loss: 0.4458 - val_acc: 0.8300\n",
            "Epoch 1261/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3724 - acc: 0.8489 - val_loss: 0.4480 - val_acc: 0.8297\n",
            "Epoch 1262/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3763 - acc: 0.8466 - val_loss: 0.4521 - val_acc: 0.8297\n",
            "Epoch 1263/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3793 - acc: 0.8444 - val_loss: 0.4517 - val_acc: 0.8330\n",
            "Epoch 1264/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3758 - acc: 0.8461 - val_loss: 0.4505 - val_acc: 0.8327\n",
            "Epoch 1265/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3775 - acc: 0.8475 - val_loss: 0.4505 - val_acc: 0.8330\n",
            "Epoch 1266/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3803 - acc: 0.8464 - val_loss: 0.4514 - val_acc: 0.8337\n",
            "Epoch 1267/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3777 - acc: 0.8479 - val_loss: 0.4528 - val_acc: 0.8343\n",
            "Epoch 1268/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3729 - acc: 0.8490 - val_loss: 0.4487 - val_acc: 0.8347\n",
            "Epoch 1269/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3811 - acc: 0.8437 - val_loss: 0.4461 - val_acc: 0.8347\n",
            "Epoch 1270/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3709 - acc: 0.8468 - val_loss: 0.4465 - val_acc: 0.8376\n",
            "Epoch 1271/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3722 - acc: 0.8475 - val_loss: 0.4499 - val_acc: 0.8340\n",
            "Epoch 1272/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3774 - acc: 0.8456 - val_loss: 0.4555 - val_acc: 0.8347\n",
            "Epoch 1273/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3748 - acc: 0.8471 - val_loss: 0.4590 - val_acc: 0.8333\n",
            "Epoch 1274/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3702 - acc: 0.8509 - val_loss: 0.4570 - val_acc: 0.8310\n",
            "Epoch 1275/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8458 - val_loss: 0.4524 - val_acc: 0.8347\n",
            "Epoch 1276/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3772 - acc: 0.8517 - val_loss: 0.4504 - val_acc: 0.8340\n",
            "Epoch 1277/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3755 - acc: 0.8472 - val_loss: 0.4502 - val_acc: 0.8337\n",
            "Epoch 1278/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3714 - acc: 0.8511 - val_loss: 0.4495 - val_acc: 0.8366\n",
            "Epoch 1279/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3752 - acc: 0.8496 - val_loss: 0.4488 - val_acc: 0.8350\n",
            "Epoch 1280/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3739 - acc: 0.8519 - val_loss: 0.4506 - val_acc: 0.8347\n",
            "Epoch 1281/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3742 - acc: 0.8490 - val_loss: 0.4528 - val_acc: 0.8347\n",
            "Epoch 1282/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3750 - acc: 0.8475 - val_loss: 0.4509 - val_acc: 0.8343\n",
            "Epoch 1283/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3746 - acc: 0.8482 - val_loss: 0.4466 - val_acc: 0.8370\n",
            "Epoch 1284/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3768 - acc: 0.8462 - val_loss: 0.4447 - val_acc: 0.8323\n",
            "Epoch 1285/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3781 - acc: 0.8477 - val_loss: 0.4465 - val_acc: 0.8343\n",
            "Epoch 1286/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3690 - acc: 0.8514 - val_loss: 0.4495 - val_acc: 0.8310\n",
            "Epoch 1287/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3765 - acc: 0.8498 - val_loss: 0.4552 - val_acc: 0.8317\n",
            "Epoch 1288/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3807 - acc: 0.8433 - val_loss: 0.4597 - val_acc: 0.8313\n",
            "Epoch 1289/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3768 - acc: 0.8466 - val_loss: 0.4580 - val_acc: 0.8347\n",
            "Epoch 1290/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3770 - acc: 0.8505 - val_loss: 0.4555 - val_acc: 0.8350\n",
            "Epoch 1291/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3753 - acc: 0.8494 - val_loss: 0.4562 - val_acc: 0.8330\n",
            "Epoch 1292/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3748 - acc: 0.8490 - val_loss: 0.4560 - val_acc: 0.8323\n",
            "Epoch 1293/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3728 - acc: 0.8464 - val_loss: 0.4561 - val_acc: 0.8317\n",
            "Epoch 1294/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3828 - acc: 0.8455 - val_loss: 0.4555 - val_acc: 0.8304\n",
            "Epoch 1295/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3737 - acc: 0.8466 - val_loss: 0.4547 - val_acc: 0.8323\n",
            "Epoch 1296/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3774 - acc: 0.8491 - val_loss: 0.4549 - val_acc: 0.8317\n",
            "Epoch 1297/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3745 - acc: 0.8510 - val_loss: 0.4584 - val_acc: 0.8294\n",
            "Epoch 1298/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.3672 - acc: 0.8524 - val_loss: 0.4578 - val_acc: 0.8327\n",
            "Epoch 1299/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3718 - acc: 0.8547 - val_loss: 0.4541 - val_acc: 0.8333\n",
            "Epoch 1300/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3764 - acc: 0.8461 - val_loss: 0.4496 - val_acc: 0.8343\n",
            "Epoch 1301/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3740 - acc: 0.8496 - val_loss: 0.4493 - val_acc: 0.8353\n",
            "Epoch 1302/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3717 - acc: 0.8508 - val_loss: 0.4519 - val_acc: 0.8363\n",
            "Epoch 1303/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3758 - acc: 0.8455 - val_loss: 0.4516 - val_acc: 0.8340\n",
            "Epoch 1304/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3697 - acc: 0.8509 - val_loss: 0.4556 - val_acc: 0.8307\n",
            "Epoch 1305/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3762 - acc: 0.8514 - val_loss: 0.4582 - val_acc: 0.8323\n",
            "Epoch 1306/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3730 - acc: 0.8477 - val_loss: 0.4549 - val_acc: 0.8337\n",
            "Epoch 1307/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3791 - acc: 0.8460 - val_loss: 0.4513 - val_acc: 0.8327\n",
            "Epoch 1308/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3671 - acc: 0.8502 - val_loss: 0.4493 - val_acc: 0.8317\n",
            "Epoch 1309/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3785 - acc: 0.8468 - val_loss: 0.4468 - val_acc: 0.8310\n",
            "Epoch 1310/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3694 - acc: 0.8506 - val_loss: 0.4455 - val_acc: 0.8337\n",
            "Epoch 1311/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3701 - acc: 0.8514 - val_loss: 0.4455 - val_acc: 0.8323\n",
            "Epoch 1312/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3677 - acc: 0.8511 - val_loss: 0.4455 - val_acc: 0.8320\n",
            "Epoch 1313/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3804 - acc: 0.8482 - val_loss: 0.4466 - val_acc: 0.8340\n",
            "Epoch 1314/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3670 - acc: 0.8515 - val_loss: 0.4468 - val_acc: 0.8333\n",
            "Epoch 1315/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3772 - acc: 0.8417 - val_loss: 0.4474 - val_acc: 0.8333\n",
            "Epoch 1316/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3663 - acc: 0.8500 - val_loss: 0.4481 - val_acc: 0.8337\n",
            "Epoch 1317/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3642 - acc: 0.8520 - val_loss: 0.4497 - val_acc: 0.8330\n",
            "Epoch 1318/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3756 - acc: 0.8474 - val_loss: 0.4511 - val_acc: 0.8300\n",
            "Epoch 1319/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3709 - acc: 0.8477 - val_loss: 0.4513 - val_acc: 0.8300\n",
            "Epoch 1320/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3766 - acc: 0.8472 - val_loss: 0.4435 - val_acc: 0.8347\n",
            "Epoch 1321/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3686 - acc: 0.8500 - val_loss: 0.4384 - val_acc: 0.8370\n",
            "Epoch 1322/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3715 - acc: 0.8487 - val_loss: 0.4360 - val_acc: 0.8383\n",
            "Epoch 1323/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3716 - acc: 0.8482 - val_loss: 0.4367 - val_acc: 0.8360\n",
            "Epoch 1324/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3653 - acc: 0.8493 - val_loss: 0.4390 - val_acc: 0.8356\n",
            "Epoch 1325/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3692 - acc: 0.8476 - val_loss: 0.4450 - val_acc: 0.8317\n",
            "Epoch 1326/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3721 - acc: 0.8489 - val_loss: 0.4478 - val_acc: 0.8323\n",
            "Epoch 1327/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3725 - acc: 0.8510 - val_loss: 0.4473 - val_acc: 0.8370\n",
            "Epoch 1328/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3744 - acc: 0.8520 - val_loss: 0.4491 - val_acc: 0.8366\n",
            "Epoch 1329/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3720 - acc: 0.8514 - val_loss: 0.4470 - val_acc: 0.8363\n",
            "Epoch 1330/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3716 - acc: 0.8519 - val_loss: 0.4441 - val_acc: 0.8347\n",
            "Epoch 1331/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3727 - acc: 0.8509 - val_loss: 0.4417 - val_acc: 0.8347\n",
            "Epoch 1332/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3736 - acc: 0.8467 - val_loss: 0.4436 - val_acc: 0.8356\n",
            "Epoch 1333/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3758 - acc: 0.8482 - val_loss: 0.4482 - val_acc: 0.8343\n",
            "Epoch 1334/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3784 - acc: 0.8507 - val_loss: 0.4490 - val_acc: 0.8330\n",
            "Epoch 1335/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3671 - acc: 0.8522 - val_loss: 0.4469 - val_acc: 0.8356\n",
            "Epoch 1336/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3698 - acc: 0.8495 - val_loss: 0.4463 - val_acc: 0.8340\n",
            "Epoch 1337/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3690 - acc: 0.8519 - val_loss: 0.4463 - val_acc: 0.8337\n",
            "Epoch 1338/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3718 - acc: 0.8465 - val_loss: 0.4470 - val_acc: 0.8353\n",
            "Epoch 1339/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3779 - acc: 0.8504 - val_loss: 0.4449 - val_acc: 0.8366\n",
            "Epoch 1340/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3744 - acc: 0.8462 - val_loss: 0.4439 - val_acc: 0.8390\n",
            "Epoch 1341/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3689 - acc: 0.8538 - val_loss: 0.4458 - val_acc: 0.8393\n",
            "Epoch 1342/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3715 - acc: 0.8520 - val_loss: 0.4468 - val_acc: 0.8360\n",
            "Epoch 1343/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3659 - acc: 0.8513 - val_loss: 0.4475 - val_acc: 0.8363\n",
            "Epoch 1344/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3727 - acc: 0.8464 - val_loss: 0.4476 - val_acc: 0.8350\n",
            "Epoch 1345/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3680 - acc: 0.8497 - val_loss: 0.4460 - val_acc: 0.8353\n",
            "Epoch 1346/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3694 - acc: 0.8494 - val_loss: 0.4445 - val_acc: 0.8353\n",
            "Epoch 1347/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3645 - acc: 0.8564 - val_loss: 0.4446 - val_acc: 0.8360\n",
            "Epoch 1348/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3706 - acc: 0.8508 - val_loss: 0.4454 - val_acc: 0.8370\n",
            "Epoch 1349/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3707 - acc: 0.8513 - val_loss: 0.4471 - val_acc: 0.8366\n",
            "Epoch 1350/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3701 - acc: 0.8518 - val_loss: 0.4458 - val_acc: 0.8373\n",
            "Epoch 1351/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3686 - acc: 0.8490 - val_loss: 0.4429 - val_acc: 0.8393\n",
            "Epoch 1352/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3671 - acc: 0.8511 - val_loss: 0.4440 - val_acc: 0.8390\n",
            "Epoch 1353/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3694 - acc: 0.8514 - val_loss: 0.4460 - val_acc: 0.8380\n",
            "Epoch 1354/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3678 - acc: 0.8518 - val_loss: 0.4499 - val_acc: 0.8347\n",
            "Epoch 1355/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3680 - acc: 0.8509 - val_loss: 0.4518 - val_acc: 0.8330\n",
            "Epoch 1356/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3640 - acc: 0.8536 - val_loss: 0.4487 - val_acc: 0.8386\n",
            "Epoch 1357/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3670 - acc: 0.8501 - val_loss: 0.4488 - val_acc: 0.8403\n",
            "Epoch 1358/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3713 - acc: 0.8529 - val_loss: 0.4449 - val_acc: 0.8386\n",
            "Epoch 1359/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3668 - acc: 0.8493 - val_loss: 0.4418 - val_acc: 0.8360\n",
            "Epoch 1360/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3724 - acc: 0.8465 - val_loss: 0.4428 - val_acc: 0.8350\n",
            "Epoch 1361/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3725 - acc: 0.8480 - val_loss: 0.4434 - val_acc: 0.8353\n",
            "Epoch 1362/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3653 - acc: 0.8504 - val_loss: 0.4447 - val_acc: 0.8347\n",
            "Epoch 1363/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3658 - acc: 0.8518 - val_loss: 0.4465 - val_acc: 0.8356\n",
            "Epoch 1364/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3733 - acc: 0.8513 - val_loss: 0.4434 - val_acc: 0.8399\n",
            "Epoch 1365/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3702 - acc: 0.8517 - val_loss: 0.4453 - val_acc: 0.8409\n",
            "Epoch 1366/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3749 - acc: 0.8527 - val_loss: 0.4464 - val_acc: 0.8383\n",
            "Epoch 1367/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3671 - acc: 0.8519 - val_loss: 0.4487 - val_acc: 0.8350\n",
            "Epoch 1368/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3651 - acc: 0.8533 - val_loss: 0.4510 - val_acc: 0.8360\n",
            "Epoch 1369/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3697 - acc: 0.8502 - val_loss: 0.4483 - val_acc: 0.8383\n",
            "Epoch 1370/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3712 - acc: 0.8528 - val_loss: 0.4457 - val_acc: 0.8383\n",
            "Epoch 1371/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3697 - acc: 0.8479 - val_loss: 0.4450 - val_acc: 0.8380\n",
            "Epoch 1372/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3687 - acc: 0.8499 - val_loss: 0.4469 - val_acc: 0.8330\n",
            "Epoch 1373/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3691 - acc: 0.8495 - val_loss: 0.4501 - val_acc: 0.8360\n",
            "Epoch 1374/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3757 - acc: 0.8512 - val_loss: 0.4518 - val_acc: 0.8360\n",
            "Epoch 1375/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3663 - acc: 0.8539 - val_loss: 0.4504 - val_acc: 0.8376\n",
            "Epoch 1376/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3686 - acc: 0.8504 - val_loss: 0.4475 - val_acc: 0.8366\n",
            "Epoch 1377/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3642 - acc: 0.8499 - val_loss: 0.4463 - val_acc: 0.8350\n",
            "Epoch 1378/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3697 - acc: 0.8515 - val_loss: 0.4469 - val_acc: 0.8380\n",
            "Epoch 1379/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3645 - acc: 0.8515 - val_loss: 0.4465 - val_acc: 0.8419\n",
            "Epoch 1380/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3657 - acc: 0.8524 - val_loss: 0.4466 - val_acc: 0.8413\n",
            "Epoch 1381/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3663 - acc: 0.8483 - val_loss: 0.4481 - val_acc: 0.8373\n",
            "Epoch 1382/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3678 - acc: 0.8532 - val_loss: 0.4476 - val_acc: 0.8360\n",
            "Epoch 1383/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3700 - acc: 0.8502 - val_loss: 0.4499 - val_acc: 0.8356\n",
            "Epoch 1384/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3616 - acc: 0.8576 - val_loss: 0.4500 - val_acc: 0.8380\n",
            "Epoch 1385/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3639 - acc: 0.8555 - val_loss: 0.4497 - val_acc: 0.8386\n",
            "Epoch 1386/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3652 - acc: 0.8529 - val_loss: 0.4481 - val_acc: 0.8363\n",
            "Epoch 1387/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3639 - acc: 0.8556 - val_loss: 0.4452 - val_acc: 0.8353\n",
            "Epoch 1388/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3670 - acc: 0.8520 - val_loss: 0.4420 - val_acc: 0.8396\n",
            "Epoch 1389/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3663 - acc: 0.8540 - val_loss: 0.4421 - val_acc: 0.8399\n",
            "Epoch 1390/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3649 - acc: 0.8538 - val_loss: 0.4444 - val_acc: 0.8376\n",
            "Epoch 1391/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3657 - acc: 0.8495 - val_loss: 0.4441 - val_acc: 0.8399\n",
            "Epoch 1392/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3649 - acc: 0.8538 - val_loss: 0.4454 - val_acc: 0.8396\n",
            "Epoch 1393/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3676 - acc: 0.8505 - val_loss: 0.4500 - val_acc: 0.8373\n",
            "Epoch 1394/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3636 - acc: 0.8547 - val_loss: 0.4523 - val_acc: 0.8330\n",
            "Epoch 1395/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3597 - acc: 0.8542 - val_loss: 0.4463 - val_acc: 0.8380\n",
            "Epoch 1396/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3726 - acc: 0.8480 - val_loss: 0.4429 - val_acc: 0.8370\n",
            "Epoch 1397/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3696 - acc: 0.8453 - val_loss: 0.4443 - val_acc: 0.8396\n",
            "Epoch 1398/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3595 - acc: 0.8558 - val_loss: 0.4472 - val_acc: 0.8409\n",
            "Epoch 1399/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3647 - acc: 0.8542 - val_loss: 0.4532 - val_acc: 0.8376\n",
            "Epoch 1400/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3622 - acc: 0.8533 - val_loss: 0.4550 - val_acc: 0.8376\n",
            "Epoch 1401/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3718 - acc: 0.8520 - val_loss: 0.4500 - val_acc: 0.8363\n",
            "Epoch 1402/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3667 - acc: 0.8500 - val_loss: 0.4448 - val_acc: 0.8360\n",
            "Epoch 1403/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3549 - acc: 0.8581 - val_loss: 0.4414 - val_acc: 0.8380\n",
            "Epoch 1404/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3624 - acc: 0.8558 - val_loss: 0.4418 - val_acc: 0.8380\n",
            "Epoch 1405/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3643 - acc: 0.8504 - val_loss: 0.4414 - val_acc: 0.8396\n",
            "Epoch 1406/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3728 - acc: 0.8465 - val_loss: 0.4401 - val_acc: 0.8403\n",
            "Epoch 1407/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3668 - acc: 0.8477 - val_loss: 0.4440 - val_acc: 0.8406\n",
            "Epoch 1408/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3649 - acc: 0.8541 - val_loss: 0.4480 - val_acc: 0.8403\n",
            "Epoch 1409/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3722 - acc: 0.8462 - val_loss: 0.4486 - val_acc: 0.8403\n",
            "Epoch 1410/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3655 - acc: 0.8509 - val_loss: 0.4495 - val_acc: 0.8386\n",
            "Epoch 1411/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8527 - val_loss: 0.4513 - val_acc: 0.8383\n",
            "Epoch 1412/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3675 - acc: 0.8547 - val_loss: 0.4517 - val_acc: 0.8390\n",
            "Epoch 1413/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.3638 - acc: 0.8543 - val_loss: 0.4497 - val_acc: 0.8386\n",
            "Epoch 1414/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3628 - acc: 0.8504 - val_loss: 0.4446 - val_acc: 0.8406\n",
            "Epoch 1415/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3643 - acc: 0.8483 - val_loss: 0.4413 - val_acc: 0.8446\n",
            "Epoch 1416/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3603 - acc: 0.8574 - val_loss: 0.4439 - val_acc: 0.8419\n",
            "Epoch 1417/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3672 - acc: 0.8565 - val_loss: 0.4485 - val_acc: 0.8409\n",
            "Epoch 1418/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3636 - acc: 0.8551 - val_loss: 0.4486 - val_acc: 0.8376\n",
            "Epoch 1419/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3601 - acc: 0.8520 - val_loss: 0.4441 - val_acc: 0.8419\n",
            "Epoch 1420/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3680 - acc: 0.8539 - val_loss: 0.4454 - val_acc: 0.8386\n",
            "Epoch 1421/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3628 - acc: 0.8515 - val_loss: 0.4498 - val_acc: 0.8366\n",
            "Epoch 1422/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3714 - acc: 0.8509 - val_loss: 0.4548 - val_acc: 0.8370\n",
            "Epoch 1423/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3629 - acc: 0.8535 - val_loss: 0.4503 - val_acc: 0.8360\n",
            "Epoch 1424/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3662 - acc: 0.8515 - val_loss: 0.4462 - val_acc: 0.8370\n",
            "Epoch 1425/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3622 - acc: 0.8562 - val_loss: 0.4434 - val_acc: 0.8386\n",
            "Epoch 1426/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3606 - acc: 0.8514 - val_loss: 0.4433 - val_acc: 0.8386\n",
            "Epoch 1427/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3658 - acc: 0.8547 - val_loss: 0.4515 - val_acc: 0.8363\n",
            "Epoch 1428/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3635 - acc: 0.8543 - val_loss: 0.4599 - val_acc: 0.8350\n",
            "Epoch 1429/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3636 - acc: 0.8496 - val_loss: 0.4578 - val_acc: 0.8350\n",
            "Epoch 1430/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3621 - acc: 0.8537 - val_loss: 0.4511 - val_acc: 0.8383\n",
            "Epoch 1431/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3670 - acc: 0.8496 - val_loss: 0.4463 - val_acc: 0.8390\n",
            "Epoch 1432/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8523 - val_loss: 0.4437 - val_acc: 0.8373\n",
            "Epoch 1433/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3629 - acc: 0.8526 - val_loss: 0.4454 - val_acc: 0.8373\n",
            "Epoch 1434/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3634 - acc: 0.8556 - val_loss: 0.4495 - val_acc: 0.8356\n",
            "Epoch 1435/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3671 - acc: 0.8528 - val_loss: 0.4546 - val_acc: 0.8370\n",
            "Epoch 1436/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3580 - acc: 0.8553 - val_loss: 0.4590 - val_acc: 0.8360\n",
            "Epoch 1437/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3617 - acc: 0.8553 - val_loss: 0.4593 - val_acc: 0.8340\n",
            "Epoch 1438/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3611 - acc: 0.8516 - val_loss: 0.4563 - val_acc: 0.8347\n",
            "Epoch 1439/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3638 - acc: 0.8521 - val_loss: 0.4498 - val_acc: 0.8396\n",
            "Epoch 1440/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3593 - acc: 0.8550 - val_loss: 0.4478 - val_acc: 0.8373\n",
            "Epoch 1441/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3707 - acc: 0.8504 - val_loss: 0.4479 - val_acc: 0.8353\n",
            "Epoch 1442/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3599 - acc: 0.8546 - val_loss: 0.4468 - val_acc: 0.8366\n",
            "Epoch 1443/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3584 - acc: 0.8557 - val_loss: 0.4479 - val_acc: 0.8327\n",
            "Epoch 1444/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3663 - acc: 0.8523 - val_loss: 0.4514 - val_acc: 0.8363\n",
            "Epoch 1445/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3612 - acc: 0.8544 - val_loss: 0.4529 - val_acc: 0.8340\n",
            "Epoch 1446/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3552 - acc: 0.8542 - val_loss: 0.4509 - val_acc: 0.8330\n",
            "Epoch 1447/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3631 - acc: 0.8533 - val_loss: 0.4458 - val_acc: 0.8370\n",
            "Epoch 1448/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3657 - acc: 0.8509 - val_loss: 0.4441 - val_acc: 0.8366\n",
            "Epoch 1449/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3639 - acc: 0.8510 - val_loss: 0.4486 - val_acc: 0.8360\n",
            "Epoch 1450/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3695 - acc: 0.8514 - val_loss: 0.4467 - val_acc: 0.8360\n",
            "Epoch 1451/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3541 - acc: 0.8552 - val_loss: 0.4407 - val_acc: 0.8386\n",
            "Epoch 1452/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3639 - acc: 0.8553 - val_loss: 0.4429 - val_acc: 0.8386\n",
            "Epoch 1453/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3568 - acc: 0.8570 - val_loss: 0.4493 - val_acc: 0.8353\n",
            "Epoch 1454/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3639 - acc: 0.8550 - val_loss: 0.4486 - val_acc: 0.8376\n",
            "Epoch 1455/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3562 - acc: 0.8571 - val_loss: 0.4444 - val_acc: 0.8406\n",
            "Epoch 1456/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3598 - acc: 0.8547 - val_loss: 0.4439 - val_acc: 0.8403\n",
            "Epoch 1457/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3632 - acc: 0.8548 - val_loss: 0.4473 - val_acc: 0.8373\n",
            "Epoch 1458/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3560 - acc: 0.8551 - val_loss: 0.4468 - val_acc: 0.8386\n",
            "Epoch 1459/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3611 - acc: 0.8538 - val_loss: 0.4430 - val_acc: 0.8413\n",
            "Epoch 1460/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3576 - acc: 0.8517 - val_loss: 0.4432 - val_acc: 0.8370\n",
            "Epoch 1461/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3607 - acc: 0.8532 - val_loss: 0.4430 - val_acc: 0.8366\n",
            "Epoch 1462/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3576 - acc: 0.8553 - val_loss: 0.4423 - val_acc: 0.8399\n",
            "Epoch 1463/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3615 - acc: 0.8511 - val_loss: 0.4421 - val_acc: 0.8399\n",
            "Epoch 1464/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3548 - acc: 0.8582 - val_loss: 0.4404 - val_acc: 0.8393\n",
            "Epoch 1465/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3591 - acc: 0.8548 - val_loss: 0.4392 - val_acc: 0.8409\n",
            "Epoch 1466/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3598 - acc: 0.8555 - val_loss: 0.4391 - val_acc: 0.8406\n",
            "Epoch 1467/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3572 - acc: 0.8523 - val_loss: 0.4418 - val_acc: 0.8390\n",
            "Epoch 1468/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3543 - acc: 0.8575 - val_loss: 0.4416 - val_acc: 0.8403\n",
            "Epoch 1469/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3657 - acc: 0.8533 - val_loss: 0.4389 - val_acc: 0.8403\n",
            "Epoch 1470/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3537 - acc: 0.8560 - val_loss: 0.4378 - val_acc: 0.8426\n",
            "Epoch 1471/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3601 - acc: 0.8558 - val_loss: 0.4399 - val_acc: 0.8442\n",
            "Epoch 1472/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3527 - acc: 0.8544 - val_loss: 0.4414 - val_acc: 0.8423\n",
            "Epoch 1473/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3630 - acc: 0.8530 - val_loss: 0.4436 - val_acc: 0.8393\n",
            "Epoch 1474/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3568 - acc: 0.8543 - val_loss: 0.4453 - val_acc: 0.8373\n",
            "Epoch 1475/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3556 - acc: 0.8571 - val_loss: 0.4439 - val_acc: 0.8406\n",
            "Epoch 1476/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3636 - acc: 0.8502 - val_loss: 0.4401 - val_acc: 0.8419\n",
            "Epoch 1477/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3562 - acc: 0.8547 - val_loss: 0.4378 - val_acc: 0.8409\n",
            "Epoch 1478/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3535 - acc: 0.8561 - val_loss: 0.4380 - val_acc: 0.8413\n",
            "Epoch 1479/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3619 - acc: 0.8527 - val_loss: 0.4433 - val_acc: 0.8383\n",
            "Epoch 1480/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3605 - acc: 0.8526 - val_loss: 0.4462 - val_acc: 0.8406\n",
            "Epoch 1481/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3555 - acc: 0.8601 - val_loss: 0.4507 - val_acc: 0.8380\n",
            "Epoch 1482/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3527 - acc: 0.8563 - val_loss: 0.4554 - val_acc: 0.8380\n",
            "Epoch 1483/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3651 - acc: 0.8490 - val_loss: 0.4527 - val_acc: 0.8366\n",
            "Epoch 1484/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3623 - acc: 0.8524 - val_loss: 0.4513 - val_acc: 0.8386\n",
            "Epoch 1485/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8548 - val_loss: 0.4459 - val_acc: 0.8390\n",
            "Epoch 1486/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3590 - acc: 0.8544 - val_loss: 0.4425 - val_acc: 0.8419\n",
            "Epoch 1487/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3605 - acc: 0.8572 - val_loss: 0.4459 - val_acc: 0.8409\n",
            "Epoch 1488/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3588 - acc: 0.8552 - val_loss: 0.4491 - val_acc: 0.8396\n",
            "Epoch 1489/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3660 - acc: 0.8541 - val_loss: 0.4457 - val_acc: 0.8393\n",
            "Epoch 1490/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3545 - acc: 0.8604 - val_loss: 0.4437 - val_acc: 0.8403\n",
            "Epoch 1491/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3549 - acc: 0.8547 - val_loss: 0.4417 - val_acc: 0.8416\n",
            "Epoch 1492/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3624 - acc: 0.8554 - val_loss: 0.4417 - val_acc: 0.8399\n",
            "Epoch 1493/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3586 - acc: 0.8571 - val_loss: 0.4462 - val_acc: 0.8413\n",
            "Epoch 1494/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3596 - acc: 0.8564 - val_loss: 0.4520 - val_acc: 0.8380\n",
            "Epoch 1495/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3539 - acc: 0.8543 - val_loss: 0.4549 - val_acc: 0.8383\n",
            "Epoch 1496/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3557 - acc: 0.8562 - val_loss: 0.4557 - val_acc: 0.8363\n",
            "Epoch 1497/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3471 - acc: 0.8625 - val_loss: 0.4555 - val_acc: 0.8333\n",
            "Epoch 1498/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3640 - acc: 0.8530 - val_loss: 0.4515 - val_acc: 0.8383\n",
            "Epoch 1499/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3490 - acc: 0.8554 - val_loss: 0.4515 - val_acc: 0.8406\n",
            "Epoch 1500/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3523 - acc: 0.8552 - val_loss: 0.4555 - val_acc: 0.8403\n",
            "Epoch 1501/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3590 - acc: 0.8509 - val_loss: 0.4604 - val_acc: 0.8393\n",
            "Epoch 1502/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3557 - acc: 0.8559 - val_loss: 0.4569 - val_acc: 0.8363\n",
            "Epoch 1503/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3574 - acc: 0.8563 - val_loss: 0.4543 - val_acc: 0.8350\n",
            "Epoch 1504/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3588 - acc: 0.8539 - val_loss: 0.4533 - val_acc: 0.8363\n",
            "Epoch 1505/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3521 - acc: 0.8573 - val_loss: 0.4515 - val_acc: 0.8380\n",
            "Epoch 1506/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3513 - acc: 0.8570 - val_loss: 0.4544 - val_acc: 0.8386\n",
            "Epoch 1507/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3559 - acc: 0.8560 - val_loss: 0.4549 - val_acc: 0.8380\n",
            "Epoch 1508/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3587 - acc: 0.8559 - val_loss: 0.4518 - val_acc: 0.8376\n",
            "Epoch 1509/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3608 - acc: 0.8526 - val_loss: 0.4449 - val_acc: 0.8399\n",
            "Epoch 1510/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3525 - acc: 0.8578 - val_loss: 0.4455 - val_acc: 0.8393\n",
            "Epoch 1511/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3528 - acc: 0.8579 - val_loss: 0.4471 - val_acc: 0.8399\n",
            "Epoch 1512/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3586 - acc: 0.8562 - val_loss: 0.4494 - val_acc: 0.8406\n",
            "Epoch 1513/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3587 - acc: 0.8550 - val_loss: 0.4523 - val_acc: 0.8370\n",
            "Epoch 1514/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3562 - acc: 0.8532 - val_loss: 0.4546 - val_acc: 0.8353\n",
            "Epoch 1515/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3607 - acc: 0.8543 - val_loss: 0.4531 - val_acc: 0.8380\n",
            "Epoch 1516/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3513 - acc: 0.8573 - val_loss: 0.4480 - val_acc: 0.8399\n",
            "Epoch 1517/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3582 - acc: 0.8555 - val_loss: 0.4450 - val_acc: 0.8419\n",
            "Epoch 1518/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3641 - acc: 0.8522 - val_loss: 0.4437 - val_acc: 0.8403\n",
            "Epoch 1519/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3568 - acc: 0.8538 - val_loss: 0.4417 - val_acc: 0.8399\n",
            "Epoch 1520/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3525 - acc: 0.8578 - val_loss: 0.4408 - val_acc: 0.8409\n",
            "Epoch 1521/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3569 - acc: 0.8542 - val_loss: 0.4407 - val_acc: 0.8390\n",
            "Epoch 1522/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3537 - acc: 0.8569 - val_loss: 0.4405 - val_acc: 0.8373\n",
            "Epoch 1523/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3592 - acc: 0.8589 - val_loss: 0.4407 - val_acc: 0.8416\n",
            "Epoch 1524/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3488 - acc: 0.8589 - val_loss: 0.4412 - val_acc: 0.8413\n",
            "Epoch 1525/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3476 - acc: 0.8616 - val_loss: 0.4429 - val_acc: 0.8413\n",
            "Epoch 1526/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3582 - acc: 0.8516 - val_loss: 0.4439 - val_acc: 0.8416\n",
            "Epoch 1527/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3482 - acc: 0.8590 - val_loss: 0.4455 - val_acc: 0.8419\n",
            "Epoch 1528/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3572 - acc: 0.8567 - val_loss: 0.4408 - val_acc: 0.8416\n",
            "Epoch 1529/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3513 - acc: 0.8613 - val_loss: 0.4372 - val_acc: 0.8439\n",
            "Epoch 1530/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3559 - acc: 0.8545 - val_loss: 0.4338 - val_acc: 0.8442\n",
            "Epoch 1531/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3477 - acc: 0.8621 - val_loss: 0.4323 - val_acc: 0.8423\n",
            "Epoch 1532/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3537 - acc: 0.8562 - val_loss: 0.4339 - val_acc: 0.8442\n",
            "Epoch 1533/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3529 - acc: 0.8579 - val_loss: 0.4384 - val_acc: 0.8426\n",
            "Epoch 1534/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3517 - acc: 0.8601 - val_loss: 0.4429 - val_acc: 0.8390\n",
            "Epoch 1535/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3597 - acc: 0.8541 - val_loss: 0.4394 - val_acc: 0.8433\n",
            "Epoch 1536/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3555 - acc: 0.8574 - val_loss: 0.4342 - val_acc: 0.8476\n",
            "Epoch 1537/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3527 - acc: 0.8580 - val_loss: 0.4321 - val_acc: 0.8416\n",
            "Epoch 1538/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3529 - acc: 0.8564 - val_loss: 0.4311 - val_acc: 0.8416\n",
            "Epoch 1539/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3467 - acc: 0.8585 - val_loss: 0.4333 - val_acc: 0.8426\n",
            "Epoch 1540/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3557 - acc: 0.8579 - val_loss: 0.4381 - val_acc: 0.8442\n",
            "Epoch 1541/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3523 - acc: 0.8581 - val_loss: 0.4422 - val_acc: 0.8433\n",
            "Epoch 1542/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3564 - acc: 0.8562 - val_loss: 0.4429 - val_acc: 0.8426\n",
            "Epoch 1543/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3432 - acc: 0.8643 - val_loss: 0.4416 - val_acc: 0.8446\n",
            "Epoch 1544/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3540 - acc: 0.8528 - val_loss: 0.4415 - val_acc: 0.8416\n",
            "Epoch 1545/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3474 - acc: 0.8606 - val_loss: 0.4430 - val_acc: 0.8409\n",
            "Epoch 1546/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3497 - acc: 0.8601 - val_loss: 0.4466 - val_acc: 0.8409\n",
            "Epoch 1547/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3605 - acc: 0.8538 - val_loss: 0.4467 - val_acc: 0.8396\n",
            "Epoch 1548/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3512 - acc: 0.8582 - val_loss: 0.4444 - val_acc: 0.8396\n",
            "Epoch 1549/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3566 - acc: 0.8554 - val_loss: 0.4431 - val_acc: 0.8433\n",
            "Epoch 1550/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3586 - acc: 0.8576 - val_loss: 0.4420 - val_acc: 0.8419\n",
            "Epoch 1551/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3529 - acc: 0.8582 - val_loss: 0.4409 - val_acc: 0.8423\n",
            "Epoch 1552/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3655 - acc: 0.8544 - val_loss: 0.4393 - val_acc: 0.8433\n",
            "Epoch 1553/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3556 - acc: 0.8580 - val_loss: 0.4394 - val_acc: 0.8436\n",
            "Epoch 1554/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3527 - acc: 0.8566 - val_loss: 0.4403 - val_acc: 0.8429\n",
            "Epoch 1555/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3513 - acc: 0.8573 - val_loss: 0.4432 - val_acc: 0.8396\n",
            "Epoch 1556/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3502 - acc: 0.8568 - val_loss: 0.4474 - val_acc: 0.8350\n",
            "Epoch 1557/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3541 - acc: 0.8537 - val_loss: 0.4471 - val_acc: 0.8356\n",
            "Epoch 1558/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3517 - acc: 0.8562 - val_loss: 0.4422 - val_acc: 0.8390\n",
            "Epoch 1559/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3514 - acc: 0.8588 - val_loss: 0.4379 - val_acc: 0.8423\n",
            "Epoch 1560/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3519 - acc: 0.8574 - val_loss: 0.4369 - val_acc: 0.8429\n",
            "Epoch 1561/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3451 - acc: 0.8577 - val_loss: 0.4372 - val_acc: 0.8406\n",
            "Epoch 1562/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3466 - acc: 0.8627 - val_loss: 0.4349 - val_acc: 0.8426\n",
            "Epoch 1563/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3491 - acc: 0.8566 - val_loss: 0.4353 - val_acc: 0.8429\n",
            "Epoch 1564/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3511 - acc: 0.8585 - val_loss: 0.4395 - val_acc: 0.8409\n",
            "Epoch 1565/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3450 - acc: 0.8604 - val_loss: 0.4457 - val_acc: 0.8356\n",
            "Epoch 1566/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3524 - acc: 0.8592 - val_loss: 0.4489 - val_acc: 0.8366\n",
            "Epoch 1567/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8595 - val_loss: 0.4464 - val_acc: 0.8386\n",
            "Epoch 1568/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3467 - acc: 0.8625 - val_loss: 0.4453 - val_acc: 0.8416\n",
            "Epoch 1569/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3513 - acc: 0.8600 - val_loss: 0.4428 - val_acc: 0.8426\n",
            "Epoch 1570/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3502 - acc: 0.8547 - val_loss: 0.4409 - val_acc: 0.8466\n",
            "Epoch 1571/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3543 - acc: 0.8581 - val_loss: 0.4426 - val_acc: 0.8403\n",
            "Epoch 1572/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3598 - acc: 0.8562 - val_loss: 0.4433 - val_acc: 0.8386\n",
            "Epoch 1573/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3500 - acc: 0.8558 - val_loss: 0.4485 - val_acc: 0.8390\n",
            "Epoch 1574/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3542 - acc: 0.8587 - val_loss: 0.4482 - val_acc: 0.8383\n",
            "Epoch 1575/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3482 - acc: 0.8589 - val_loss: 0.4486 - val_acc: 0.8390\n",
            "Epoch 1576/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3478 - acc: 0.8617 - val_loss: 0.4481 - val_acc: 0.8433\n",
            "Epoch 1577/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3451 - acc: 0.8587 - val_loss: 0.4476 - val_acc: 0.8423\n",
            "Epoch 1578/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3522 - acc: 0.8578 - val_loss: 0.4476 - val_acc: 0.8423\n",
            "Epoch 1579/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3551 - acc: 0.8559 - val_loss: 0.4471 - val_acc: 0.8399\n",
            "Epoch 1580/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3569 - acc: 0.8550 - val_loss: 0.4444 - val_acc: 0.8406\n",
            "Epoch 1581/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3439 - acc: 0.8613 - val_loss: 0.4433 - val_acc: 0.8390\n",
            "Epoch 1582/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3447 - acc: 0.8613 - val_loss: 0.4471 - val_acc: 0.8439\n",
            "Epoch 1583/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8595 - val_loss: 0.4491 - val_acc: 0.8413\n",
            "Epoch 1584/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3473 - acc: 0.8603 - val_loss: 0.4491 - val_acc: 0.8393\n",
            "Epoch 1585/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3504 - acc: 0.8538 - val_loss: 0.4483 - val_acc: 0.8386\n",
            "Epoch 1586/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3568 - acc: 0.8577 - val_loss: 0.4469 - val_acc: 0.8423\n",
            "Epoch 1587/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3489 - acc: 0.8591 - val_loss: 0.4441 - val_acc: 0.8409\n",
            "Epoch 1588/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3595 - acc: 0.8561 - val_loss: 0.4461 - val_acc: 0.8399\n",
            "Epoch 1589/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3493 - acc: 0.8563 - val_loss: 0.4476 - val_acc: 0.8396\n",
            "Epoch 1590/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3526 - acc: 0.8600 - val_loss: 0.4535 - val_acc: 0.8380\n",
            "Epoch 1591/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3532 - acc: 0.8570 - val_loss: 0.4583 - val_acc: 0.8363\n",
            "Epoch 1592/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3565 - acc: 0.8566 - val_loss: 0.4523 - val_acc: 0.8390\n",
            "Epoch 1593/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3517 - acc: 0.8600 - val_loss: 0.4476 - val_acc: 0.8380\n",
            "Epoch 1594/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3511 - acc: 0.8551 - val_loss: 0.4414 - val_acc: 0.8423\n",
            "Epoch 1595/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3546 - acc: 0.8547 - val_loss: 0.4396 - val_acc: 0.8416\n",
            "Epoch 1596/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3568 - acc: 0.8563 - val_loss: 0.4448 - val_acc: 0.8396\n",
            "Epoch 1597/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3549 - acc: 0.8564 - val_loss: 0.4473 - val_acc: 0.8390\n",
            "Epoch 1598/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3478 - acc: 0.8609 - val_loss: 0.4465 - val_acc: 0.8423\n",
            "Epoch 1599/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3503 - acc: 0.8601 - val_loss: 0.4448 - val_acc: 0.8423\n",
            "Epoch 1600/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3476 - acc: 0.8632 - val_loss: 0.4464 - val_acc: 0.8399\n",
            "Epoch 1601/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3485 - acc: 0.8567 - val_loss: 0.4472 - val_acc: 0.8373\n",
            "Epoch 1602/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3509 - acc: 0.8585 - val_loss: 0.4480 - val_acc: 0.8376\n",
            "Epoch 1603/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3472 - acc: 0.8571 - val_loss: 0.4461 - val_acc: 0.8383\n",
            "Epoch 1604/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3463 - acc: 0.8604 - val_loss: 0.4440 - val_acc: 0.8390\n",
            "Epoch 1605/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3424 - acc: 0.8635 - val_loss: 0.4419 - val_acc: 0.8409\n",
            "Epoch 1606/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3450 - acc: 0.8582 - val_loss: 0.4400 - val_acc: 0.8439\n",
            "Epoch 1607/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3511 - acc: 0.8584 - val_loss: 0.4389 - val_acc: 0.8426\n",
            "Epoch 1608/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3469 - acc: 0.8614 - val_loss: 0.4397 - val_acc: 0.8393\n",
            "Epoch 1609/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3453 - acc: 0.8600 - val_loss: 0.4440 - val_acc: 0.8376\n",
            "Epoch 1610/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3465 - acc: 0.8604 - val_loss: 0.4461 - val_acc: 0.8363\n",
            "Epoch 1611/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3494 - acc: 0.8619 - val_loss: 0.4488 - val_acc: 0.8370\n",
            "Epoch 1612/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3500 - acc: 0.8615 - val_loss: 0.4514 - val_acc: 0.8396\n",
            "Epoch 1613/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3530 - acc: 0.8547 - val_loss: 0.4490 - val_acc: 0.8396\n",
            "Epoch 1614/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3511 - acc: 0.8576 - val_loss: 0.4451 - val_acc: 0.8439\n",
            "Epoch 1615/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3492 - acc: 0.8583 - val_loss: 0.4417 - val_acc: 0.8433\n",
            "Epoch 1616/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3462 - acc: 0.8581 - val_loss: 0.4440 - val_acc: 0.8390\n",
            "Epoch 1617/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3485 - acc: 0.8595 - val_loss: 0.4462 - val_acc: 0.8386\n",
            "Epoch 1618/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3475 - acc: 0.8617 - val_loss: 0.4465 - val_acc: 0.8399\n",
            "Epoch 1619/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3519 - acc: 0.8572 - val_loss: 0.4447 - val_acc: 0.8403\n",
            "Epoch 1620/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3494 - acc: 0.8609 - val_loss: 0.4391 - val_acc: 0.8416\n",
            "Epoch 1621/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3533 - acc: 0.8572 - val_loss: 0.4362 - val_acc: 0.8423\n",
            "Epoch 1622/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3538 - acc: 0.8614 - val_loss: 0.4374 - val_acc: 0.8419\n",
            "Epoch 1623/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3507 - acc: 0.8579 - val_loss: 0.4391 - val_acc: 0.8409\n",
            "Epoch 1624/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3508 - acc: 0.8612 - val_loss: 0.4388 - val_acc: 0.8429\n",
            "Epoch 1625/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3489 - acc: 0.8611 - val_loss: 0.4401 - val_acc: 0.8442\n",
            "Epoch 1626/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3491 - acc: 0.8611 - val_loss: 0.4386 - val_acc: 0.8436\n",
            "Epoch 1627/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3509 - acc: 0.8604 - val_loss: 0.4397 - val_acc: 0.8462\n",
            "Epoch 1628/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3550 - acc: 0.8590 - val_loss: 0.4402 - val_acc: 0.8439\n",
            "Epoch 1629/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3566 - acc: 0.8569 - val_loss: 0.4367 - val_acc: 0.8449\n",
            "Epoch 1630/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3439 - acc: 0.8600 - val_loss: 0.4404 - val_acc: 0.8413\n",
            "Epoch 1631/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3526 - acc: 0.8568 - val_loss: 0.4432 - val_acc: 0.8429\n",
            "Epoch 1632/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3519 - acc: 0.8579 - val_loss: 0.4430 - val_acc: 0.8403\n",
            "Epoch 1633/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3544 - acc: 0.8586 - val_loss: 0.4459 - val_acc: 0.8413\n",
            "Epoch 1634/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3457 - acc: 0.8598 - val_loss: 0.4411 - val_acc: 0.8439\n",
            "Epoch 1635/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3527 - acc: 0.8585 - val_loss: 0.4415 - val_acc: 0.8429\n",
            "Epoch 1636/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3490 - acc: 0.8601 - val_loss: 0.4425 - val_acc: 0.8433\n",
            "Epoch 1637/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3440 - acc: 0.8641 - val_loss: 0.4394 - val_acc: 0.8442\n",
            "Epoch 1638/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.8649 - val_loss: 0.4367 - val_acc: 0.8423\n",
            "Epoch 1639/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3440 - acc: 0.8619 - val_loss: 0.4366 - val_acc: 0.8419\n",
            "Epoch 1640/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3417 - acc: 0.8643 - val_loss: 0.4374 - val_acc: 0.8439\n",
            "Epoch 1641/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3409 - acc: 0.8629 - val_loss: 0.4420 - val_acc: 0.8416\n",
            "Epoch 1642/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3432 - acc: 0.8602 - val_loss: 0.4477 - val_acc: 0.8370\n",
            "Epoch 1643/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3513 - acc: 0.8565 - val_loss: 0.4454 - val_acc: 0.8409\n",
            "Epoch 1644/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3481 - acc: 0.8584 - val_loss: 0.4421 - val_acc: 0.8442\n",
            "Epoch 1645/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3395 - acc: 0.8601 - val_loss: 0.4407 - val_acc: 0.8459\n",
            "Epoch 1646/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3414 - acc: 0.8653 - val_loss: 0.4393 - val_acc: 0.8429\n",
            "Epoch 1647/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3459 - acc: 0.8625 - val_loss: 0.4374 - val_acc: 0.8452\n",
            "Epoch 1648/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3471 - acc: 0.8578 - val_loss: 0.4435 - val_acc: 0.8419\n",
            "Epoch 1649/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3430 - acc: 0.8616 - val_loss: 0.4504 - val_acc: 0.8403\n",
            "Epoch 1650/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3560 - acc: 0.8568 - val_loss: 0.4477 - val_acc: 0.8399\n",
            "Epoch 1651/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3555 - acc: 0.8573 - val_loss: 0.4442 - val_acc: 0.8429\n",
            "Epoch 1652/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3498 - acc: 0.8553 - val_loss: 0.4428 - val_acc: 0.8472\n",
            "Epoch 1653/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3437 - acc: 0.8614 - val_loss: 0.4447 - val_acc: 0.8429\n",
            "Epoch 1654/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3417 - acc: 0.8653 - val_loss: 0.4500 - val_acc: 0.8393\n",
            "Epoch 1655/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3507 - acc: 0.8576 - val_loss: 0.4548 - val_acc: 0.8356\n",
            "Epoch 1656/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3432 - acc: 0.8594 - val_loss: 0.4532 - val_acc: 0.8356\n",
            "Epoch 1657/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3491 - acc: 0.8591 - val_loss: 0.4472 - val_acc: 0.8373\n",
            "Epoch 1658/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3500 - acc: 0.8606 - val_loss: 0.4460 - val_acc: 0.8370\n",
            "Epoch 1659/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3457 - acc: 0.8593 - val_loss: 0.4483 - val_acc: 0.8376\n",
            "Epoch 1660/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3401 - acc: 0.8613 - val_loss: 0.4508 - val_acc: 0.8350\n",
            "Epoch 1661/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3503 - acc: 0.8552 - val_loss: 0.4504 - val_acc: 0.8366\n",
            "Epoch 1662/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3470 - acc: 0.8600 - val_loss: 0.4481 - val_acc: 0.8399\n",
            "Epoch 1663/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3484 - acc: 0.8618 - val_loss: 0.4484 - val_acc: 0.8399\n",
            "Epoch 1664/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3373 - acc: 0.8646 - val_loss: 0.4520 - val_acc: 0.8366\n",
            "Epoch 1665/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3408 - acc: 0.8623 - val_loss: 0.4516 - val_acc: 0.8373\n",
            "Epoch 1666/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3488 - acc: 0.8578 - val_loss: 0.4484 - val_acc: 0.8380\n",
            "Epoch 1667/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3518 - acc: 0.8552 - val_loss: 0.4451 - val_acc: 0.8423\n",
            "Epoch 1668/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3516 - acc: 0.8609 - val_loss: 0.4437 - val_acc: 0.8406\n",
            "Epoch 1669/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3498 - acc: 0.8565 - val_loss: 0.4435 - val_acc: 0.8390\n",
            "Epoch 1670/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3407 - acc: 0.8597 - val_loss: 0.4447 - val_acc: 0.8403\n",
            "Epoch 1671/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3505 - acc: 0.8593 - val_loss: 0.4448 - val_acc: 0.8433\n",
            "Epoch 1672/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3473 - acc: 0.8636 - val_loss: 0.4463 - val_acc: 0.8419\n",
            "Epoch 1673/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3434 - acc: 0.8615 - val_loss: 0.4479 - val_acc: 0.8409\n",
            "Epoch 1674/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3455 - acc: 0.8588 - val_loss: 0.4476 - val_acc: 0.8390\n",
            "Epoch 1675/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3436 - acc: 0.8629 - val_loss: 0.4458 - val_acc: 0.8399\n",
            "Epoch 1676/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3390 - acc: 0.8646 - val_loss: 0.4445 - val_acc: 0.8393\n",
            "Epoch 1677/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3420 - acc: 0.8595 - val_loss: 0.4459 - val_acc: 0.8386\n",
            "Epoch 1678/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3422 - acc: 0.8587 - val_loss: 0.4442 - val_acc: 0.8403\n",
            "Epoch 1679/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3422 - acc: 0.8624 - val_loss: 0.4417 - val_acc: 0.8426\n",
            "Epoch 1680/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3480 - acc: 0.8613 - val_loss: 0.4411 - val_acc: 0.8426\n",
            "Epoch 1681/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3396 - acc: 0.8652 - val_loss: 0.4410 - val_acc: 0.8439\n",
            "Epoch 1682/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3410 - acc: 0.8605 - val_loss: 0.4406 - val_acc: 0.8459\n",
            "Epoch 1683/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3438 - acc: 0.8619 - val_loss: 0.4388 - val_acc: 0.8439\n",
            "Epoch 1684/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3465 - acc: 0.8598 - val_loss: 0.4400 - val_acc: 0.8456\n",
            "Epoch 1685/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3451 - acc: 0.8613 - val_loss: 0.4462 - val_acc: 0.8413\n",
            "Epoch 1686/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3424 - acc: 0.8590 - val_loss: 0.4516 - val_acc: 0.8399\n",
            "Epoch 1687/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3437 - acc: 0.8622 - val_loss: 0.4507 - val_acc: 0.8403\n",
            "Epoch 1688/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3394 - acc: 0.8613 - val_loss: 0.4460 - val_acc: 0.8416\n",
            "Epoch 1689/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3446 - acc: 0.8598 - val_loss: 0.4420 - val_acc: 0.8442\n",
            "Epoch 1690/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3384 - acc: 0.8624 - val_loss: 0.4416 - val_acc: 0.8423\n",
            "Epoch 1691/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3416 - acc: 0.8608 - val_loss: 0.4457 - val_acc: 0.8409\n",
            "Epoch 1692/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3482 - acc: 0.8624 - val_loss: 0.4493 - val_acc: 0.8399\n",
            "Epoch 1693/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3473 - acc: 0.8572 - val_loss: 0.4504 - val_acc: 0.8399\n",
            "Epoch 1694/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3487 - acc: 0.8557 - val_loss: 0.4480 - val_acc: 0.8433\n",
            "Epoch 1695/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3445 - acc: 0.8602 - val_loss: 0.4466 - val_acc: 0.8423\n",
            "Epoch 1696/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3426 - acc: 0.8667 - val_loss: 0.4462 - val_acc: 0.8423\n",
            "Epoch 1697/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3463 - acc: 0.8636 - val_loss: 0.4468 - val_acc: 0.8433\n",
            "Epoch 1698/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.3366 - acc: 0.8638 - val_loss: 0.4524 - val_acc: 0.8376\n",
            "Epoch 1699/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3449 - acc: 0.8629 - val_loss: 0.4502 - val_acc: 0.8376\n",
            "Epoch 1700/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3353 - acc: 0.8666 - val_loss: 0.4458 - val_acc: 0.8419\n",
            "Epoch 1701/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3476 - acc: 0.8601 - val_loss: 0.4431 - val_acc: 0.8446\n",
            "Epoch 1702/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3443 - acc: 0.8609 - val_loss: 0.4411 - val_acc: 0.8426\n",
            "Epoch 1703/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3442 - acc: 0.8619 - val_loss: 0.4388 - val_acc: 0.8419\n",
            "Epoch 1704/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3390 - acc: 0.8633 - val_loss: 0.4391 - val_acc: 0.8439\n",
            "Epoch 1705/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3349 - acc: 0.8649 - val_loss: 0.4423 - val_acc: 0.8396\n",
            "Epoch 1706/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3464 - acc: 0.8609 - val_loss: 0.4466 - val_acc: 0.8396\n",
            "Epoch 1707/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3439 - acc: 0.8616 - val_loss: 0.4503 - val_acc: 0.8376\n",
            "Epoch 1708/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3436 - acc: 0.8581 - val_loss: 0.4526 - val_acc: 0.8366\n",
            "Epoch 1709/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3396 - acc: 0.8630 - val_loss: 0.4533 - val_acc: 0.8366\n",
            "Epoch 1710/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3427 - acc: 0.8590 - val_loss: 0.4513 - val_acc: 0.8393\n",
            "Epoch 1711/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3420 - acc: 0.8602 - val_loss: 0.4457 - val_acc: 0.8383\n",
            "Epoch 1712/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3390 - acc: 0.8591 - val_loss: 0.4425 - val_acc: 0.8439\n",
            "Epoch 1713/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3354 - acc: 0.8671 - val_loss: 0.4440 - val_acc: 0.8423\n",
            "Epoch 1714/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3359 - acc: 0.8663 - val_loss: 0.4451 - val_acc: 0.8426\n",
            "Epoch 1715/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3384 - acc: 0.8610 - val_loss: 0.4436 - val_acc: 0.8482\n",
            "Epoch 1716/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3368 - acc: 0.8628 - val_loss: 0.4431 - val_acc: 0.8476\n",
            "Epoch 1717/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3441 - acc: 0.8614 - val_loss: 0.4415 - val_acc: 0.8452\n",
            "Epoch 1718/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3418 - acc: 0.8603 - val_loss: 0.4430 - val_acc: 0.8449\n",
            "Epoch 1719/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3377 - acc: 0.8657 - val_loss: 0.4460 - val_acc: 0.8419\n",
            "Epoch 1720/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.8635 - val_loss: 0.4453 - val_acc: 0.8413\n",
            "Epoch 1721/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3414 - acc: 0.8625 - val_loss: 0.4410 - val_acc: 0.8456\n",
            "Epoch 1722/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3368 - acc: 0.8638 - val_loss: 0.4382 - val_acc: 0.8459\n",
            "Epoch 1723/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3417 - acc: 0.8641 - val_loss: 0.4353 - val_acc: 0.8499\n",
            "Epoch 1724/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3425 - acc: 0.8620 - val_loss: 0.4344 - val_acc: 0.8469\n",
            "Epoch 1725/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3445 - acc: 0.8582 - val_loss: 0.4354 - val_acc: 0.8469\n",
            "Epoch 1726/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3371 - acc: 0.8641 - val_loss: 0.4403 - val_acc: 0.8459\n",
            "Epoch 1727/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3383 - acc: 0.8611 - val_loss: 0.4414 - val_acc: 0.8446\n",
            "Epoch 1728/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3419 - acc: 0.8617 - val_loss: 0.4406 - val_acc: 0.8459\n",
            "Epoch 1729/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3424 - acc: 0.8618 - val_loss: 0.4401 - val_acc: 0.8472\n",
            "Epoch 1730/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.8658 - val_loss: 0.4407 - val_acc: 0.8452\n",
            "Epoch 1731/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3416 - acc: 0.8600 - val_loss: 0.4410 - val_acc: 0.8462\n",
            "Epoch 1732/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3427 - acc: 0.8633 - val_loss: 0.4378 - val_acc: 0.8472\n",
            "Epoch 1733/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.8634 - val_loss: 0.4388 - val_acc: 0.8449\n",
            "Epoch 1734/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3359 - acc: 0.8643 - val_loss: 0.4400 - val_acc: 0.8462\n",
            "Epoch 1735/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3360 - acc: 0.8671 - val_loss: 0.4421 - val_acc: 0.8439\n",
            "Epoch 1736/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3396 - acc: 0.8629 - val_loss: 0.4425 - val_acc: 0.8423\n",
            "Epoch 1737/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3397 - acc: 0.8638 - val_loss: 0.4403 - val_acc: 0.8456\n",
            "Epoch 1738/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3309 - acc: 0.8690 - val_loss: 0.4395 - val_acc: 0.8429\n",
            "Epoch 1739/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3321 - acc: 0.8667 - val_loss: 0.4394 - val_acc: 0.8442\n",
            "Epoch 1740/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3482 - acc: 0.8610 - val_loss: 0.4406 - val_acc: 0.8423\n",
            "Epoch 1741/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3383 - acc: 0.8649 - val_loss: 0.4405 - val_acc: 0.8419\n",
            "Epoch 1742/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3369 - acc: 0.8631 - val_loss: 0.4389 - val_acc: 0.8446\n",
            "Epoch 1743/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3427 - acc: 0.8614 - val_loss: 0.4358 - val_acc: 0.8449\n",
            "Epoch 1744/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3371 - acc: 0.8647 - val_loss: 0.4358 - val_acc: 0.8433\n",
            "Epoch 1745/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3388 - acc: 0.8621 - val_loss: 0.4376 - val_acc: 0.8426\n",
            "Epoch 1746/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3418 - acc: 0.8649 - val_loss: 0.4379 - val_acc: 0.8446\n",
            "Epoch 1747/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3371 - acc: 0.8631 - val_loss: 0.4419 - val_acc: 0.8446\n",
            "Epoch 1748/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3380 - acc: 0.8637 - val_loss: 0.4368 - val_acc: 0.8462\n",
            "Epoch 1749/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3448 - acc: 0.8595 - val_loss: 0.4337 - val_acc: 0.8472\n",
            "Epoch 1750/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3405 - acc: 0.8611 - val_loss: 0.4377 - val_acc: 0.8452\n",
            "Epoch 1751/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3390 - acc: 0.8660 - val_loss: 0.4417 - val_acc: 0.8449\n",
            "Epoch 1752/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3355 - acc: 0.8667 - val_loss: 0.4435 - val_acc: 0.8469\n",
            "Epoch 1753/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3409 - acc: 0.8636 - val_loss: 0.4425 - val_acc: 0.8476\n",
            "Epoch 1754/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3432 - acc: 0.8596 - val_loss: 0.4432 - val_acc: 0.8452\n",
            "Epoch 1755/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3390 - acc: 0.8647 - val_loss: 0.4453 - val_acc: 0.8459\n",
            "Epoch 1756/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3419 - acc: 0.8636 - val_loss: 0.4449 - val_acc: 0.8459\n",
            "Epoch 1757/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3376 - acc: 0.8648 - val_loss: 0.4434 - val_acc: 0.8462\n",
            "Epoch 1758/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3419 - acc: 0.8604 - val_loss: 0.4403 - val_acc: 0.8442\n",
            "Epoch 1759/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3438 - acc: 0.8628 - val_loss: 0.4415 - val_acc: 0.8439\n",
            "Epoch 1760/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3406 - acc: 0.8613 - val_loss: 0.4446 - val_acc: 0.8462\n",
            "Epoch 1761/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3411 - acc: 0.8621 - val_loss: 0.4449 - val_acc: 0.8479\n",
            "Epoch 1762/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3369 - acc: 0.8619 - val_loss: 0.4453 - val_acc: 0.8469\n",
            "Epoch 1763/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3466 - acc: 0.8628 - val_loss: 0.4429 - val_acc: 0.8442\n",
            "Epoch 1764/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3340 - acc: 0.8653 - val_loss: 0.4403 - val_acc: 0.8452\n",
            "Epoch 1765/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3418 - acc: 0.8638 - val_loss: 0.4375 - val_acc: 0.8446\n",
            "Epoch 1766/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3397 - acc: 0.8603 - val_loss: 0.4357 - val_acc: 0.8479\n",
            "Epoch 1767/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3347 - acc: 0.8619 - val_loss: 0.4370 - val_acc: 0.8489\n",
            "Epoch 1768/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3360 - acc: 0.8662 - val_loss: 0.4381 - val_acc: 0.8485\n",
            "Epoch 1769/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3340 - acc: 0.8646 - val_loss: 0.4402 - val_acc: 0.8489\n",
            "Epoch 1770/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3309 - acc: 0.8630 - val_loss: 0.4400 - val_acc: 0.8532\n",
            "Epoch 1771/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3379 - acc: 0.8632 - val_loss: 0.4364 - val_acc: 0.8535\n",
            "Epoch 1772/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3371 - acc: 0.8629 - val_loss: 0.4334 - val_acc: 0.8502\n",
            "Epoch 1773/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3380 - acc: 0.8614 - val_loss: 0.4346 - val_acc: 0.8519\n",
            "Epoch 1774/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3324 - acc: 0.8661 - val_loss: 0.4369 - val_acc: 0.8479\n",
            "Epoch 1775/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3354 - acc: 0.8640 - val_loss: 0.4384 - val_acc: 0.8442\n",
            "Epoch 1776/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3384 - acc: 0.8644 - val_loss: 0.4383 - val_acc: 0.8442\n",
            "Epoch 1777/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3352 - acc: 0.8654 - val_loss: 0.4365 - val_acc: 0.8485\n",
            "Epoch 1778/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3390 - acc: 0.8634 - val_loss: 0.4361 - val_acc: 0.8502\n",
            "Epoch 1779/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3413 - acc: 0.8619 - val_loss: 0.4355 - val_acc: 0.8548\n",
            "Epoch 1780/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3406 - acc: 0.8635 - val_loss: 0.4374 - val_acc: 0.8489\n",
            "Epoch 1781/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3375 - acc: 0.8620 - val_loss: 0.4386 - val_acc: 0.8449\n",
            "Epoch 1782/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3403 - acc: 0.8633 - val_loss: 0.4382 - val_acc: 0.8439\n",
            "Epoch 1783/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3398 - acc: 0.8614 - val_loss: 0.4367 - val_acc: 0.8466\n",
            "Epoch 1784/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3381 - acc: 0.8633 - val_loss: 0.4353 - val_acc: 0.8452\n",
            "Epoch 1785/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3340 - acc: 0.8660 - val_loss: 0.4380 - val_acc: 0.8472\n",
            "Epoch 1786/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3397 - acc: 0.8622 - val_loss: 0.4405 - val_acc: 0.8469\n",
            "Epoch 1787/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3342 - acc: 0.8663 - val_loss: 0.4358 - val_acc: 0.8472\n",
            "Epoch 1788/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3337 - acc: 0.8624 - val_loss: 0.4330 - val_acc: 0.8439\n",
            "Epoch 1789/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3373 - acc: 0.8648 - val_loss: 0.4337 - val_acc: 0.8442\n",
            "Epoch 1790/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3362 - acc: 0.8646 - val_loss: 0.4326 - val_acc: 0.8469\n",
            "Epoch 1791/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3335 - acc: 0.8658 - val_loss: 0.4344 - val_acc: 0.8442\n",
            "Epoch 1792/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3468 - acc: 0.8619 - val_loss: 0.4357 - val_acc: 0.8469\n",
            "Epoch 1793/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3462 - acc: 0.8599 - val_loss: 0.4347 - val_acc: 0.8472\n",
            "Epoch 1794/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3332 - acc: 0.8704 - val_loss: 0.4343 - val_acc: 0.8472\n",
            "Epoch 1795/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3414 - acc: 0.8615 - val_loss: 0.4345 - val_acc: 0.8462\n",
            "Epoch 1796/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3417 - acc: 0.8612 - val_loss: 0.4349 - val_acc: 0.8469\n",
            "Epoch 1797/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3381 - acc: 0.8627 - val_loss: 0.4368 - val_acc: 0.8456\n",
            "Epoch 1798/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3385 - acc: 0.8633 - val_loss: 0.4361 - val_acc: 0.8452\n",
            "Epoch 1799/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3380 - acc: 0.8643 - val_loss: 0.4340 - val_acc: 0.8449\n",
            "Epoch 1800/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8632 - val_loss: 0.4327 - val_acc: 0.8492\n",
            "Epoch 1801/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3296 - acc: 0.8670 - val_loss: 0.4336 - val_acc: 0.8482\n",
            "Epoch 1802/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3420 - acc: 0.8619 - val_loss: 0.4385 - val_acc: 0.8489\n",
            "Epoch 1803/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3381 - acc: 0.8635 - val_loss: 0.4411 - val_acc: 0.8449\n",
            "Epoch 1804/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3359 - acc: 0.8650 - val_loss: 0.4411 - val_acc: 0.8472\n",
            "Epoch 1805/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3407 - acc: 0.8633 - val_loss: 0.4384 - val_acc: 0.8449\n",
            "Epoch 1806/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3333 - acc: 0.8627 - val_loss: 0.4348 - val_acc: 0.8456\n",
            "Epoch 1807/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3263 - acc: 0.8675 - val_loss: 0.4355 - val_acc: 0.8423\n",
            "Epoch 1808/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3349 - acc: 0.8672 - val_loss: 0.4392 - val_acc: 0.8439\n",
            "Epoch 1809/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3324 - acc: 0.8685 - val_loss: 0.4431 - val_acc: 0.8433\n",
            "Epoch 1810/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3407 - acc: 0.8632 - val_loss: 0.4480 - val_acc: 0.8452\n",
            "Epoch 1811/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3296 - acc: 0.8653 - val_loss: 0.4458 - val_acc: 0.8429\n",
            "Epoch 1812/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3299 - acc: 0.8679 - val_loss: 0.4404 - val_acc: 0.8462\n",
            "Epoch 1813/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3392 - acc: 0.8625 - val_loss: 0.4373 - val_acc: 0.8459\n",
            "Epoch 1814/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3351 - acc: 0.8687 - val_loss: 0.4363 - val_acc: 0.8489\n",
            "Epoch 1815/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3372 - acc: 0.8636 - val_loss: 0.4367 - val_acc: 0.8472\n",
            "Epoch 1816/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3291 - acc: 0.8678 - val_loss: 0.4406 - val_acc: 0.8456\n",
            "Epoch 1817/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3349 - acc: 0.8665 - val_loss: 0.4394 - val_acc: 0.8489\n",
            "Epoch 1818/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3366 - acc: 0.8647 - val_loss: 0.4360 - val_acc: 0.8482\n",
            "Epoch 1819/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3336 - acc: 0.8656 - val_loss: 0.4351 - val_acc: 0.8519\n",
            "Epoch 1820/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3301 - acc: 0.8666 - val_loss: 0.4359 - val_acc: 0.8505\n",
            "Epoch 1821/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3347 - acc: 0.8638 - val_loss: 0.4368 - val_acc: 0.8489\n",
            "Epoch 1822/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3310 - acc: 0.8665 - val_loss: 0.4410 - val_acc: 0.8456\n",
            "Epoch 1823/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3322 - acc: 0.8666 - val_loss: 0.4417 - val_acc: 0.8469\n",
            "Epoch 1824/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3398 - acc: 0.8613 - val_loss: 0.4377 - val_acc: 0.8502\n",
            "Epoch 1825/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3361 - acc: 0.8658 - val_loss: 0.4328 - val_acc: 0.8519\n",
            "Epoch 1826/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3350 - acc: 0.8628 - val_loss: 0.4302 - val_acc: 0.8519\n",
            "Epoch 1827/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3273 - acc: 0.8689 - val_loss: 0.4318 - val_acc: 0.8528\n",
            "Epoch 1828/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3293 - acc: 0.8667 - val_loss: 0.4360 - val_acc: 0.8492\n",
            "Epoch 1829/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3287 - acc: 0.8687 - val_loss: 0.4391 - val_acc: 0.8485\n",
            "Epoch 1830/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3291 - acc: 0.8687 - val_loss: 0.4366 - val_acc: 0.8495\n",
            "Epoch 1831/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3225 - acc: 0.8709 - val_loss: 0.4349 - val_acc: 0.8509\n",
            "Epoch 1832/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3340 - acc: 0.8639 - val_loss: 0.4358 - val_acc: 0.8502\n",
            "Epoch 1833/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3314 - acc: 0.8655 - val_loss: 0.4383 - val_acc: 0.8515\n",
            "Epoch 1834/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3290 - acc: 0.8655 - val_loss: 0.4406 - val_acc: 0.8505\n",
            "Epoch 1835/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3299 - acc: 0.8690 - val_loss: 0.4463 - val_acc: 0.8442\n",
            "Epoch 1836/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3425 - acc: 0.8610 - val_loss: 0.4468 - val_acc: 0.8436\n",
            "Epoch 1837/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3350 - acc: 0.8671 - val_loss: 0.4405 - val_acc: 0.8492\n",
            "Epoch 1838/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3376 - acc: 0.8631 - val_loss: 0.4350 - val_acc: 0.8495\n",
            "Epoch 1839/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3309 - acc: 0.8684 - val_loss: 0.4330 - val_acc: 0.8532\n",
            "Epoch 1840/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3337 - acc: 0.8628 - val_loss: 0.4312 - val_acc: 0.8538\n",
            "Epoch 1841/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3383 - acc: 0.8628 - val_loss: 0.4331 - val_acc: 0.8515\n",
            "Epoch 1842/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3327 - acc: 0.8652 - val_loss: 0.4357 - val_acc: 0.8499\n",
            "Epoch 1843/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3228 - acc: 0.8719 - val_loss: 0.4385 - val_acc: 0.8502\n",
            "Epoch 1844/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3354 - acc: 0.8641 - val_loss: 0.4408 - val_acc: 0.8482\n",
            "Epoch 1845/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.8637 - val_loss: 0.4394 - val_acc: 0.8502\n",
            "Epoch 1846/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3385 - acc: 0.8626 - val_loss: 0.4384 - val_acc: 0.8489\n",
            "Epoch 1847/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3394 - acc: 0.8659 - val_loss: 0.4403 - val_acc: 0.8442\n",
            "Epoch 1848/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3378 - acc: 0.8636 - val_loss: 0.4392 - val_acc: 0.8509\n",
            "Epoch 1849/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3349 - acc: 0.8685 - val_loss: 0.4388 - val_acc: 0.8499\n",
            "Epoch 1850/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3342 - acc: 0.8647 - val_loss: 0.4372 - val_acc: 0.8472\n",
            "Epoch 1851/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3318 - acc: 0.8621 - val_loss: 0.4344 - val_acc: 0.8485\n",
            "Epoch 1852/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3289 - acc: 0.8684 - val_loss: 0.4377 - val_acc: 0.8472\n",
            "Epoch 1853/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3342 - acc: 0.8652 - val_loss: 0.4406 - val_acc: 0.8442\n",
            "Epoch 1854/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3351 - acc: 0.8652 - val_loss: 0.4398 - val_acc: 0.8452\n",
            "Epoch 1855/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3410 - acc: 0.8602 - val_loss: 0.4390 - val_acc: 0.8469\n",
            "Epoch 1856/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3344 - acc: 0.8666 - val_loss: 0.4370 - val_acc: 0.8492\n",
            "Epoch 1857/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3324 - acc: 0.8636 - val_loss: 0.4357 - val_acc: 0.8492\n",
            "Epoch 1858/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3247 - acc: 0.8695 - val_loss: 0.4340 - val_acc: 0.8502\n",
            "Epoch 1859/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3337 - acc: 0.8656 - val_loss: 0.4347 - val_acc: 0.8519\n",
            "Epoch 1860/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3373 - acc: 0.8618 - val_loss: 0.4364 - val_acc: 0.8479\n",
            "Epoch 1861/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3335 - acc: 0.8640 - val_loss: 0.4364 - val_acc: 0.8499\n",
            "Epoch 1862/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3367 - acc: 0.8623 - val_loss: 0.4362 - val_acc: 0.8515\n",
            "Epoch 1863/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3362 - acc: 0.8676 - val_loss: 0.4359 - val_acc: 0.8509\n",
            "Epoch 1864/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3333 - acc: 0.8662 - val_loss: 0.4327 - val_acc: 0.8519\n",
            "Epoch 1865/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3320 - acc: 0.8635 - val_loss: 0.4302 - val_acc: 0.8538\n",
            "Epoch 1866/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3268 - acc: 0.8641 - val_loss: 0.4323 - val_acc: 0.8545\n",
            "Epoch 1867/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3273 - acc: 0.8652 - val_loss: 0.4374 - val_acc: 0.8509\n",
            "Epoch 1868/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3398 - acc: 0.8609 - val_loss: 0.4394 - val_acc: 0.8499\n",
            "Epoch 1869/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3234 - acc: 0.8675 - val_loss: 0.4355 - val_acc: 0.8495\n",
            "Epoch 1870/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3342 - acc: 0.8634 - val_loss: 0.4320 - val_acc: 0.8515\n",
            "Epoch 1871/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3322 - acc: 0.8646 - val_loss: 0.4329 - val_acc: 0.8502\n",
            "Epoch 1872/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3319 - acc: 0.8644 - val_loss: 0.4357 - val_acc: 0.8522\n",
            "Epoch 1873/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3369 - acc: 0.8630 - val_loss: 0.4389 - val_acc: 0.8499\n",
            "Epoch 1874/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3332 - acc: 0.8636 - val_loss: 0.4411 - val_acc: 0.8469\n",
            "Epoch 1875/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3339 - acc: 0.8657 - val_loss: 0.4437 - val_acc: 0.8456\n",
            "Epoch 1876/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3328 - acc: 0.8641 - val_loss: 0.4400 - val_acc: 0.8459\n",
            "Epoch 1877/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3279 - acc: 0.8676 - val_loss: 0.4360 - val_acc: 0.8462\n",
            "Epoch 1878/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3369 - acc: 0.8620 - val_loss: 0.4347 - val_acc: 0.8485\n",
            "Epoch 1879/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3363 - acc: 0.8642 - val_loss: 0.4359 - val_acc: 0.8489\n",
            "Epoch 1880/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3345 - acc: 0.8669 - val_loss: 0.4381 - val_acc: 0.8515\n",
            "Epoch 1881/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3249 - acc: 0.8696 - val_loss: 0.4411 - val_acc: 0.8479\n",
            "Epoch 1882/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3264 - acc: 0.8668 - val_loss: 0.4421 - val_acc: 0.8462\n",
            "Epoch 1883/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3336 - acc: 0.8634 - val_loss: 0.4407 - val_acc: 0.8472\n",
            "Epoch 1884/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3384 - acc: 0.8635 - val_loss: 0.4401 - val_acc: 0.8485\n",
            "Epoch 1885/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3322 - acc: 0.8671 - val_loss: 0.4408 - val_acc: 0.8472\n",
            "Epoch 1886/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3393 - acc: 0.8609 - val_loss: 0.4405 - val_acc: 0.8459\n",
            "Epoch 1887/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3310 - acc: 0.8690 - val_loss: 0.4401 - val_acc: 0.8469\n",
            "Epoch 1888/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3323 - acc: 0.8679 - val_loss: 0.4383 - val_acc: 0.8512\n",
            "Epoch 1889/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3262 - acc: 0.8702 - val_loss: 0.4372 - val_acc: 0.8479\n",
            "Epoch 1890/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3331 - acc: 0.8630 - val_loss: 0.4313 - val_acc: 0.8489\n",
            "Epoch 1891/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3374 - acc: 0.8624 - val_loss: 0.4265 - val_acc: 0.8502\n",
            "Epoch 1892/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3301 - acc: 0.8709 - val_loss: 0.4296 - val_acc: 0.8479\n",
            "Epoch 1893/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3259 - acc: 0.8679 - val_loss: 0.4358 - val_acc: 0.8482\n",
            "Epoch 1894/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3371 - acc: 0.8653 - val_loss: 0.4401 - val_acc: 0.8479\n",
            "Epoch 1895/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3367 - acc: 0.8627 - val_loss: 0.4402 - val_acc: 0.8485\n",
            "Epoch 1896/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3282 - acc: 0.8668 - val_loss: 0.4393 - val_acc: 0.8492\n",
            "Epoch 1897/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3343 - acc: 0.8676 - val_loss: 0.4379 - val_acc: 0.8512\n",
            "Epoch 1898/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3345 - acc: 0.8660 - val_loss: 0.4378 - val_acc: 0.8532\n",
            "Epoch 1899/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3272 - acc: 0.8703 - val_loss: 0.4432 - val_acc: 0.8466\n",
            "Epoch 1900/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3272 - acc: 0.8687 - val_loss: 0.4496 - val_acc: 0.8419\n",
            "Epoch 1901/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3344 - acc: 0.8662 - val_loss: 0.4467 - val_acc: 0.8439\n",
            "Epoch 1902/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3309 - acc: 0.8682 - val_loss: 0.4414 - val_acc: 0.8462\n",
            "Epoch 1903/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3281 - acc: 0.8679 - val_loss: 0.4379 - val_acc: 0.8472\n",
            "Epoch 1904/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3301 - acc: 0.8662 - val_loss: 0.4384 - val_acc: 0.8479\n",
            "Epoch 1905/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3295 - acc: 0.8660 - val_loss: 0.4410 - val_acc: 0.8472\n",
            "Epoch 1906/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3282 - acc: 0.8655 - val_loss: 0.4471 - val_acc: 0.8472\n",
            "Epoch 1907/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3256 - acc: 0.8682 - val_loss: 0.4454 - val_acc: 0.8492\n",
            "Epoch 1908/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3329 - acc: 0.8638 - val_loss: 0.4403 - val_acc: 0.8512\n",
            "Epoch 1909/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3317 - acc: 0.8630 - val_loss: 0.4382 - val_acc: 0.8522\n",
            "Epoch 1910/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3367 - acc: 0.8631 - val_loss: 0.4351 - val_acc: 0.8548\n",
            "Epoch 1911/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3263 - acc: 0.8707 - val_loss: 0.4371 - val_acc: 0.8515\n",
            "Epoch 1912/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3251 - acc: 0.8685 - val_loss: 0.4387 - val_acc: 0.8502\n",
            "Epoch 1913/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3301 - acc: 0.8667 - val_loss: 0.4394 - val_acc: 0.8495\n",
            "Epoch 1914/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3303 - acc: 0.8657 - val_loss: 0.4403 - val_acc: 0.8485\n",
            "Epoch 1915/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3217 - acc: 0.8709 - val_loss: 0.4424 - val_acc: 0.8495\n",
            "Epoch 1916/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3276 - acc: 0.8675 - val_loss: 0.4431 - val_acc: 0.8479\n",
            "Epoch 1917/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3329 - acc: 0.8648 - val_loss: 0.4415 - val_acc: 0.8502\n",
            "Epoch 1918/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3365 - acc: 0.8631 - val_loss: 0.4411 - val_acc: 0.8492\n",
            "Epoch 1919/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3256 - acc: 0.8680 - val_loss: 0.4399 - val_acc: 0.8505\n",
            "Epoch 1920/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3276 - acc: 0.8666 - val_loss: 0.4419 - val_acc: 0.8515\n",
            "Epoch 1921/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3228 - acc: 0.8690 - val_loss: 0.4421 - val_acc: 0.8538\n",
            "Epoch 1922/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3287 - acc: 0.8709 - val_loss: 0.4386 - val_acc: 0.8532\n",
            "Epoch 1923/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3330 - acc: 0.8697 - val_loss: 0.4358 - val_acc: 0.8515\n",
            "Epoch 1924/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3250 - acc: 0.8650 - val_loss: 0.4319 - val_acc: 0.8525\n",
            "Epoch 1925/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3296 - acc: 0.8685 - val_loss: 0.4321 - val_acc: 0.8519\n",
            "Epoch 1926/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3301 - acc: 0.8664 - val_loss: 0.4370 - val_acc: 0.8482\n",
            "Epoch 1927/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3293 - acc: 0.8662 - val_loss: 0.4384 - val_acc: 0.8479\n",
            "Epoch 1928/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3288 - acc: 0.8648 - val_loss: 0.4391 - val_acc: 0.8472\n",
            "Epoch 1929/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3304 - acc: 0.8698 - val_loss: 0.4377 - val_acc: 0.8495\n",
            "Epoch 1930/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3243 - acc: 0.8718 - val_loss: 0.4382 - val_acc: 0.8515\n",
            "Epoch 1931/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3335 - acc: 0.8641 - val_loss: 0.4384 - val_acc: 0.8525\n",
            "Epoch 1932/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3193 - acc: 0.8711 - val_loss: 0.4381 - val_acc: 0.8525\n",
            "Epoch 1933/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3235 - acc: 0.8704 - val_loss: 0.4362 - val_acc: 0.8505\n",
            "Epoch 1934/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3298 - acc: 0.8694 - val_loss: 0.4355 - val_acc: 0.8502\n",
            "Epoch 1935/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3353 - acc: 0.8643 - val_loss: 0.4352 - val_acc: 0.8509\n",
            "Epoch 1936/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3258 - acc: 0.8679 - val_loss: 0.4355 - val_acc: 0.8492\n",
            "Epoch 1937/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3247 - acc: 0.8691 - val_loss: 0.4367 - val_acc: 0.8505\n",
            "Epoch 1938/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3260 - acc: 0.8686 - val_loss: 0.4374 - val_acc: 0.8499\n",
            "Epoch 1939/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3219 - acc: 0.8693 - val_loss: 0.4363 - val_acc: 0.8515\n",
            "Epoch 1940/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3230 - acc: 0.8706 - val_loss: 0.4374 - val_acc: 0.8505\n",
            "Epoch 1941/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3301 - acc: 0.8671 - val_loss: 0.4410 - val_acc: 0.8466\n",
            "Epoch 1942/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3254 - acc: 0.8693 - val_loss: 0.4409 - val_acc: 0.8446\n",
            "Epoch 1943/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3339 - acc: 0.8643 - val_loss: 0.4405 - val_acc: 0.8479\n",
            "Epoch 1944/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8665 - val_loss: 0.4386 - val_acc: 0.8485\n",
            "Epoch 1945/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3330 - acc: 0.8679 - val_loss: 0.4378 - val_acc: 0.8479\n",
            "Epoch 1946/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3255 - acc: 0.8655 - val_loss: 0.4380 - val_acc: 0.8499\n",
            "Epoch 1947/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8686 - val_loss: 0.4385 - val_acc: 0.8495\n",
            "Epoch 1948/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3251 - acc: 0.8667 - val_loss: 0.4413 - val_acc: 0.8469\n",
            "Epoch 1949/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3324 - acc: 0.8669 - val_loss: 0.4426 - val_acc: 0.8485\n",
            "Epoch 1950/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3259 - acc: 0.8643 - val_loss: 0.4379 - val_acc: 0.8482\n",
            "Epoch 1951/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3176 - acc: 0.8721 - val_loss: 0.4337 - val_acc: 0.8528\n",
            "Epoch 1952/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3308 - acc: 0.8660 - val_loss: 0.4310 - val_acc: 0.8505\n",
            "Epoch 1953/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3254 - acc: 0.8667 - val_loss: 0.4330 - val_acc: 0.8482\n",
            "Epoch 1954/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3292 - acc: 0.8665 - val_loss: 0.4341 - val_acc: 0.8479\n",
            "Epoch 1955/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3253 - acc: 0.8658 - val_loss: 0.4386 - val_acc: 0.8489\n",
            "Epoch 1956/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3281 - acc: 0.8663 - val_loss: 0.4416 - val_acc: 0.8489\n",
            "Epoch 1957/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3287 - acc: 0.8647 - val_loss: 0.4422 - val_acc: 0.8505\n",
            "Epoch 1958/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3289 - acc: 0.8675 - val_loss: 0.4415 - val_acc: 0.8466\n",
            "Epoch 1959/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3310 - acc: 0.8675 - val_loss: 0.4386 - val_acc: 0.8466\n",
            "Epoch 1960/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3242 - acc: 0.8689 - val_loss: 0.4381 - val_acc: 0.8456\n",
            "Epoch 1961/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3262 - acc: 0.8714 - val_loss: 0.4383 - val_acc: 0.8462\n",
            "Epoch 1962/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3232 - acc: 0.8697 - val_loss: 0.4383 - val_acc: 0.8452\n",
            "Epoch 1963/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3246 - acc: 0.8675 - val_loss: 0.4378 - val_acc: 0.8446\n",
            "Epoch 1964/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3311 - acc: 0.8671 - val_loss: 0.4371 - val_acc: 0.8472\n",
            "Epoch 1965/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3227 - acc: 0.8712 - val_loss: 0.4365 - val_acc: 0.8476\n",
            "Epoch 1966/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8675 - val_loss: 0.4378 - val_acc: 0.8466\n",
            "Epoch 1967/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3271 - acc: 0.8661 - val_loss: 0.4405 - val_acc: 0.8479\n",
            "Epoch 1968/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3346 - acc: 0.8638 - val_loss: 0.4431 - val_acc: 0.8492\n",
            "Epoch 1969/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3207 - acc: 0.8714 - val_loss: 0.4433 - val_acc: 0.8479\n",
            "Epoch 1970/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3351 - acc: 0.8663 - val_loss: 0.4429 - val_acc: 0.8446\n",
            "Epoch 1971/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3301 - acc: 0.8669 - val_loss: 0.4396 - val_acc: 0.8452\n",
            "Epoch 1972/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3274 - acc: 0.8670 - val_loss: 0.4335 - val_acc: 0.8479\n",
            "Epoch 1973/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3300 - acc: 0.8685 - val_loss: 0.4335 - val_acc: 0.8502\n",
            "Epoch 1974/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3175 - acc: 0.8709 - val_loss: 0.4384 - val_acc: 0.8512\n",
            "Epoch 1975/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3336 - acc: 0.8643 - val_loss: 0.4436 - val_acc: 0.8505\n",
            "Epoch 1976/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3199 - acc: 0.8724 - val_loss: 0.4461 - val_acc: 0.8476\n",
            "Epoch 1977/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3269 - acc: 0.8652 - val_loss: 0.4433 - val_acc: 0.8499\n",
            "Epoch 1978/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3273 - acc: 0.8657 - val_loss: 0.4407 - val_acc: 0.8495\n",
            "Epoch 1979/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3221 - acc: 0.8694 - val_loss: 0.4370 - val_acc: 0.8489\n",
            "Epoch 1980/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3199 - acc: 0.8731 - val_loss: 0.4354 - val_acc: 0.8522\n",
            "Epoch 1981/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3209 - acc: 0.8668 - val_loss: 0.4362 - val_acc: 0.8489\n",
            "Epoch 1982/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3259 - acc: 0.8695 - val_loss: 0.4360 - val_acc: 0.8446\n",
            "Epoch 1983/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3265 - acc: 0.8676 - val_loss: 0.4371 - val_acc: 0.8482\n",
            "Epoch 1984/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3206 - acc: 0.8758 - val_loss: 0.4405 - val_acc: 0.8489\n",
            "Epoch 1985/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3264 - acc: 0.8691 - val_loss: 0.4382 - val_acc: 0.8482\n",
            "Epoch 1986/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3246 - acc: 0.8684 - val_loss: 0.4358 - val_acc: 0.8509\n",
            "Epoch 1987/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3159 - acc: 0.8741 - val_loss: 0.4358 - val_acc: 0.8505\n",
            "Epoch 1988/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3229 - acc: 0.8696 - val_loss: 0.4350 - val_acc: 0.8476\n",
            "Epoch 1989/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3324 - acc: 0.8646 - val_loss: 0.4343 - val_acc: 0.8482\n",
            "Epoch 1990/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3276 - acc: 0.8705 - val_loss: 0.4341 - val_acc: 0.8466\n",
            "Epoch 1991/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3239 - acc: 0.8690 - val_loss: 0.4336 - val_acc: 0.8485\n",
            "Epoch 1992/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3231 - acc: 0.8700 - val_loss: 0.4325 - val_acc: 0.8519\n",
            "Epoch 1993/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3287 - acc: 0.8716 - val_loss: 0.4329 - val_acc: 0.8495\n",
            "Epoch 1994/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3220 - acc: 0.8705 - val_loss: 0.4327 - val_acc: 0.8502\n",
            "Epoch 1995/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3235 - acc: 0.8726 - val_loss: 0.4333 - val_acc: 0.8495\n",
            "Epoch 1996/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3234 - acc: 0.8726 - val_loss: 0.4364 - val_acc: 0.8502\n",
            "Epoch 1997/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3189 - acc: 0.8697 - val_loss: 0.4343 - val_acc: 0.8519\n",
            "Epoch 1998/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3336 - acc: 0.8652 - val_loss: 0.4282 - val_acc: 0.8548\n",
            "Epoch 1999/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3316 - acc: 0.8662 - val_loss: 0.4252 - val_acc: 0.8532\n",
            "Epoch 2000/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3195 - acc: 0.8731 - val_loss: 0.4237 - val_acc: 0.8538\n",
            "Epoch 2001/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3266 - acc: 0.8644 - val_loss: 0.4242 - val_acc: 0.8555\n",
            "Epoch 2002/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3230 - acc: 0.8708 - val_loss: 0.4295 - val_acc: 0.8512\n",
            "Epoch 2003/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3247 - acc: 0.8680 - val_loss: 0.4331 - val_acc: 0.8499\n",
            "Epoch 2004/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3283 - acc: 0.8676 - val_loss: 0.4318 - val_acc: 0.8519\n",
            "Epoch 2005/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3309 - acc: 0.8662 - val_loss: 0.4306 - val_acc: 0.8515\n",
            "Epoch 2006/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3265 - acc: 0.8682 - val_loss: 0.4295 - val_acc: 0.8476\n",
            "Epoch 2007/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3259 - acc: 0.8695 - val_loss: 0.4314 - val_acc: 0.8519\n",
            "Epoch 2008/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3264 - acc: 0.8672 - val_loss: 0.4353 - val_acc: 0.8485\n",
            "Epoch 2009/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3216 - acc: 0.8688 - val_loss: 0.4373 - val_acc: 0.8485\n",
            "Epoch 2010/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8649 - val_loss: 0.4369 - val_acc: 0.8485\n",
            "Epoch 2011/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3181 - acc: 0.8712 - val_loss: 0.4381 - val_acc: 0.8482\n",
            "Epoch 2012/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.8702 - val_loss: 0.4404 - val_acc: 0.8476\n",
            "Epoch 2013/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3270 - acc: 0.8670 - val_loss: 0.4414 - val_acc: 0.8472\n",
            "Epoch 2014/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3260 - acc: 0.8667 - val_loss: 0.4414 - val_acc: 0.8459\n",
            "Epoch 2015/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3233 - acc: 0.8676 - val_loss: 0.4403 - val_acc: 0.8462\n",
            "Epoch 2016/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3226 - acc: 0.8684 - val_loss: 0.4338 - val_acc: 0.8479\n",
            "Epoch 2017/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3277 - acc: 0.8705 - val_loss: 0.4275 - val_acc: 0.8515\n",
            "Epoch 2018/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3236 - acc: 0.8654 - val_loss: 0.4266 - val_acc: 0.8535\n",
            "Epoch 2019/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3270 - acc: 0.8676 - val_loss: 0.4280 - val_acc: 0.8528\n",
            "Epoch 2020/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3217 - acc: 0.8692 - val_loss: 0.4313 - val_acc: 0.8502\n",
            "Epoch 2021/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3242 - acc: 0.8702 - val_loss: 0.4377 - val_acc: 0.8479\n",
            "Epoch 2022/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3289 - acc: 0.8674 - val_loss: 0.4367 - val_acc: 0.8476\n",
            "Epoch 2023/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3278 - acc: 0.8677 - val_loss: 0.4288 - val_acc: 0.8492\n",
            "Epoch 2024/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3269 - acc: 0.8705 - val_loss: 0.4242 - val_acc: 0.8482\n",
            "Epoch 2025/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3285 - acc: 0.8691 - val_loss: 0.4251 - val_acc: 0.8502\n",
            "Epoch 2026/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3245 - acc: 0.8723 - val_loss: 0.4284 - val_acc: 0.8522\n",
            "Epoch 2027/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3222 - acc: 0.8725 - val_loss: 0.4332 - val_acc: 0.8489\n",
            "Epoch 2028/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3212 - acc: 0.8672 - val_loss: 0.4339 - val_acc: 0.8492\n",
            "Epoch 2029/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3255 - acc: 0.8680 - val_loss: 0.4316 - val_acc: 0.8509\n",
            "Epoch 2030/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3225 - acc: 0.8703 - val_loss: 0.4264 - val_acc: 0.8512\n",
            "Epoch 2031/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3252 - acc: 0.8691 - val_loss: 0.4228 - val_acc: 0.8555\n",
            "Epoch 2032/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3204 - acc: 0.8727 - val_loss: 0.4277 - val_acc: 0.8532\n",
            "Epoch 2033/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3291 - acc: 0.8681 - val_loss: 0.4356 - val_acc: 0.8502\n",
            "Epoch 2034/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3285 - acc: 0.8699 - val_loss: 0.4401 - val_acc: 0.8485\n",
            "Epoch 2035/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3175 - acc: 0.8694 - val_loss: 0.4417 - val_acc: 0.8466\n",
            "Epoch 2036/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3240 - acc: 0.8718 - val_loss: 0.4404 - val_acc: 0.8476\n",
            "Epoch 2037/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3235 - acc: 0.8685 - val_loss: 0.4361 - val_acc: 0.8472\n",
            "Epoch 2038/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3202 - acc: 0.8699 - val_loss: 0.4329 - val_acc: 0.8495\n",
            "Epoch 2039/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3243 - acc: 0.8699 - val_loss: 0.4329 - val_acc: 0.8505\n",
            "Epoch 2040/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3208 - acc: 0.8729 - val_loss: 0.4316 - val_acc: 0.8509\n",
            "Epoch 2041/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3210 - acc: 0.8677 - val_loss: 0.4326 - val_acc: 0.8532\n",
            "Epoch 2042/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3298 - acc: 0.8692 - val_loss: 0.4340 - val_acc: 0.8535\n",
            "Epoch 2043/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3181 - acc: 0.8703 - val_loss: 0.4358 - val_acc: 0.8525\n",
            "Epoch 2044/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3178 - acc: 0.8713 - val_loss: 0.4369 - val_acc: 0.8505\n",
            "Epoch 2045/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3180 - acc: 0.8709 - val_loss: 0.4362 - val_acc: 0.8499\n",
            "Epoch 2046/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3306 - acc: 0.8652 - val_loss: 0.4326 - val_acc: 0.8542\n",
            "Epoch 2047/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3193 - acc: 0.8681 - val_loss: 0.4328 - val_acc: 0.8558\n",
            "Epoch 2048/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3240 - acc: 0.8704 - val_loss: 0.4345 - val_acc: 0.8509\n",
            "Epoch 2049/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3143 - acc: 0.8756 - val_loss: 0.4377 - val_acc: 0.8489\n",
            "Epoch 2050/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.8655 - val_loss: 0.4384 - val_acc: 0.8482\n",
            "Epoch 2051/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3246 - acc: 0.8700 - val_loss: 0.4356 - val_acc: 0.8476\n",
            "Epoch 2052/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3200 - acc: 0.8712 - val_loss: 0.4340 - val_acc: 0.8502\n",
            "Epoch 2053/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3222 - acc: 0.8699 - val_loss: 0.4369 - val_acc: 0.8519\n",
            "Epoch 2054/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3177 - acc: 0.8715 - val_loss: 0.4394 - val_acc: 0.8525\n",
            "Epoch 2055/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3234 - acc: 0.8691 - val_loss: 0.4416 - val_acc: 0.8532\n",
            "Epoch 2056/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3154 - acc: 0.8735 - val_loss: 0.4432 - val_acc: 0.8519\n",
            "Epoch 2057/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3218 - acc: 0.8692 - val_loss: 0.4406 - val_acc: 0.8522\n",
            "Epoch 2058/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3192 - acc: 0.8699 - val_loss: 0.4347 - val_acc: 0.8555\n",
            "Epoch 2059/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.8708 - val_loss: 0.4310 - val_acc: 0.8552\n",
            "Epoch 2060/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3267 - acc: 0.8690 - val_loss: 0.4304 - val_acc: 0.8528\n",
            "Epoch 2061/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3185 - acc: 0.8730 - val_loss: 0.4320 - val_acc: 0.8522\n",
            "Epoch 2062/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3218 - acc: 0.8708 - val_loss: 0.4362 - val_acc: 0.8515\n",
            "Epoch 2063/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3165 - acc: 0.8737 - val_loss: 0.4405 - val_acc: 0.8505\n",
            "Epoch 2064/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3244 - acc: 0.8686 - val_loss: 0.4388 - val_acc: 0.8495\n",
            "Epoch 2065/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3187 - acc: 0.8699 - val_loss: 0.4333 - val_acc: 0.8538\n",
            "Epoch 2066/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3230 - acc: 0.8711 - val_loss: 0.4302 - val_acc: 0.8558\n",
            "Epoch 2067/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3241 - acc: 0.8718 - val_loss: 0.4295 - val_acc: 0.8542\n",
            "Epoch 2068/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3268 - acc: 0.8655 - val_loss: 0.4274 - val_acc: 0.8519\n",
            "Epoch 2069/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3185 - acc: 0.8733 - val_loss: 0.4272 - val_acc: 0.8528\n",
            "Epoch 2070/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3192 - acc: 0.8726 - val_loss: 0.4290 - val_acc: 0.8535\n",
            "Epoch 2071/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3224 - acc: 0.8696 - val_loss: 0.4282 - val_acc: 0.8535\n",
            "Epoch 2072/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3278 - acc: 0.8695 - val_loss: 0.4295 - val_acc: 0.8542\n",
            "Epoch 2073/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3165 - acc: 0.8726 - val_loss: 0.4302 - val_acc: 0.8548\n",
            "Epoch 2074/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.8730 - val_loss: 0.4300 - val_acc: 0.8538\n",
            "Epoch 2075/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3187 - acc: 0.8722 - val_loss: 0.4308 - val_acc: 0.8515\n",
            "Epoch 2076/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3186 - acc: 0.8681 - val_loss: 0.4316 - val_acc: 0.8519\n",
            "Epoch 2077/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3192 - acc: 0.8749 - val_loss: 0.4323 - val_acc: 0.8528\n",
            "Epoch 2078/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3247 - acc: 0.8681 - val_loss: 0.4330 - val_acc: 0.8532\n",
            "Epoch 2079/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3216 - acc: 0.8673 - val_loss: 0.4345 - val_acc: 0.8535\n",
            "Epoch 2080/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3147 - acc: 0.8724 - val_loss: 0.4365 - val_acc: 0.8538\n",
            "Epoch 2081/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3147 - acc: 0.8719 - val_loss: 0.4370 - val_acc: 0.8532\n",
            "Epoch 2082/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3147 - acc: 0.8724 - val_loss: 0.4353 - val_acc: 0.8505\n",
            "Epoch 2083/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3134 - acc: 0.8748 - val_loss: 0.4327 - val_acc: 0.8495\n",
            "Epoch 2084/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3191 - acc: 0.8697 - val_loss: 0.4334 - val_acc: 0.8499\n",
            "Epoch 2085/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3084 - acc: 0.8750 - val_loss: 0.4344 - val_acc: 0.8476\n",
            "Epoch 2086/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3208 - acc: 0.8695 - val_loss: 0.4361 - val_acc: 0.8492\n",
            "Epoch 2087/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3215 - acc: 0.8707 - val_loss: 0.4386 - val_acc: 0.8482\n",
            "Epoch 2088/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3224 - acc: 0.8695 - val_loss: 0.4390 - val_acc: 0.8482\n",
            "Epoch 2089/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3185 - acc: 0.8720 - val_loss: 0.4370 - val_acc: 0.8499\n",
            "Epoch 2090/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3226 - acc: 0.8669 - val_loss: 0.4366 - val_acc: 0.8505\n",
            "Epoch 2091/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3188 - acc: 0.8729 - val_loss: 0.4399 - val_acc: 0.8466\n",
            "Epoch 2092/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3134 - acc: 0.8731 - val_loss: 0.4407 - val_acc: 0.8489\n",
            "Epoch 2093/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3194 - acc: 0.8707 - val_loss: 0.4363 - val_acc: 0.8509\n",
            "Epoch 2094/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3209 - acc: 0.8721 - val_loss: 0.4337 - val_acc: 0.8509\n",
            "Epoch 2095/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3157 - acc: 0.8747 - val_loss: 0.4323 - val_acc: 0.8509\n",
            "Epoch 2096/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3142 - acc: 0.8748 - val_loss: 0.4322 - val_acc: 0.8492\n",
            "Epoch 2097/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3197 - acc: 0.8707 - val_loss: 0.4304 - val_acc: 0.8509\n",
            "Epoch 2098/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3108 - acc: 0.8723 - val_loss: 0.4297 - val_acc: 0.8515\n",
            "Epoch 2099/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3157 - acc: 0.8695 - val_loss: 0.4315 - val_acc: 0.8512\n",
            "Epoch 2100/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3172 - acc: 0.8683 - val_loss: 0.4357 - val_acc: 0.8492\n",
            "Epoch 2101/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3111 - acc: 0.8705 - val_loss: 0.4400 - val_acc: 0.8479\n",
            "Epoch 2102/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3230 - acc: 0.8701 - val_loss: 0.4398 - val_acc: 0.8479\n",
            "Epoch 2103/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3178 - acc: 0.8718 - val_loss: 0.4368 - val_acc: 0.8499\n",
            "Epoch 2104/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3176 - acc: 0.8689 - val_loss: 0.4365 - val_acc: 0.8502\n",
            "Epoch 2105/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3169 - acc: 0.8714 - val_loss: 0.4373 - val_acc: 0.8519\n",
            "Epoch 2106/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.8725 - val_loss: 0.4393 - val_acc: 0.8489\n",
            "Epoch 2107/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3199 - acc: 0.8732 - val_loss: 0.4401 - val_acc: 0.8466\n",
            "Epoch 2108/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3237 - acc: 0.8676 - val_loss: 0.4377 - val_acc: 0.8472\n",
            "Epoch 2109/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8731 - val_loss: 0.4358 - val_acc: 0.8489\n",
            "Epoch 2110/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3163 - acc: 0.8708 - val_loss: 0.4376 - val_acc: 0.8479\n",
            "Epoch 2111/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3169 - acc: 0.8748 - val_loss: 0.4415 - val_acc: 0.8459\n",
            "Epoch 2112/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3230 - acc: 0.8730 - val_loss: 0.4462 - val_acc: 0.8449\n",
            "Epoch 2113/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3202 - acc: 0.8719 - val_loss: 0.4449 - val_acc: 0.8466\n",
            "Epoch 2114/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3172 - acc: 0.8743 - val_loss: 0.4386 - val_acc: 0.8509\n",
            "Epoch 2115/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3099 - acc: 0.8765 - val_loss: 0.4341 - val_acc: 0.8492\n",
            "Epoch 2116/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3196 - acc: 0.8667 - val_loss: 0.4343 - val_acc: 0.8512\n",
            "Epoch 2117/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3263 - acc: 0.8659 - val_loss: 0.4337 - val_acc: 0.8485\n",
            "Epoch 2118/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3222 - acc: 0.8675 - val_loss: 0.4360 - val_acc: 0.8505\n",
            "Epoch 2119/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3199 - acc: 0.8699 - val_loss: 0.4386 - val_acc: 0.8499\n",
            "Epoch 2120/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3148 - acc: 0.8715 - val_loss: 0.4376 - val_acc: 0.8502\n",
            "Epoch 2121/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3222 - acc: 0.8708 - val_loss: 0.4337 - val_acc: 0.8515\n",
            "Epoch 2122/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3195 - acc: 0.8712 - val_loss: 0.4313 - val_acc: 0.8545\n",
            "Epoch 2123/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3224 - acc: 0.8714 - val_loss: 0.4319 - val_acc: 0.8542\n",
            "Epoch 2124/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3238 - acc: 0.8719 - val_loss: 0.4372 - val_acc: 0.8505\n",
            "Epoch 2125/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3197 - acc: 0.8708 - val_loss: 0.4434 - val_acc: 0.8479\n",
            "Epoch 2126/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3179 - acc: 0.8720 - val_loss: 0.4428 - val_acc: 0.8492\n",
            "Epoch 2127/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3179 - acc: 0.8734 - val_loss: 0.4401 - val_acc: 0.8512\n",
            "Epoch 2128/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3143 - acc: 0.8743 - val_loss: 0.4415 - val_acc: 0.8492\n",
            "Epoch 2129/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3240 - acc: 0.8661 - val_loss: 0.4451 - val_acc: 0.8466\n",
            "Epoch 2130/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3187 - acc: 0.8696 - val_loss: 0.4491 - val_acc: 0.8452\n",
            "Epoch 2131/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3112 - acc: 0.8757 - val_loss: 0.4473 - val_acc: 0.8489\n",
            "Epoch 2132/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3143 - acc: 0.8726 - val_loss: 0.4415 - val_acc: 0.8515\n",
            "Epoch 2133/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3202 - acc: 0.8715 - val_loss: 0.4366 - val_acc: 0.8515\n",
            "Epoch 2134/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3175 - acc: 0.8758 - val_loss: 0.4395 - val_acc: 0.8512\n",
            "Epoch 2135/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.8705 - val_loss: 0.4443 - val_acc: 0.8489\n",
            "Epoch 2136/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3202 - acc: 0.8723 - val_loss: 0.4428 - val_acc: 0.8522\n",
            "Epoch 2137/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3222 - acc: 0.8681 - val_loss: 0.4365 - val_acc: 0.8509\n",
            "Epoch 2138/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3209 - acc: 0.8708 - val_loss: 0.4300 - val_acc: 0.8509\n",
            "Epoch 2139/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3174 - acc: 0.8679 - val_loss: 0.4282 - val_acc: 0.8535\n",
            "Epoch 2140/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3178 - acc: 0.8737 - val_loss: 0.4318 - val_acc: 0.8525\n",
            "Epoch 2141/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3201 - acc: 0.8716 - val_loss: 0.4410 - val_acc: 0.8485\n",
            "Epoch 2142/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3196 - acc: 0.8695 - val_loss: 0.4489 - val_acc: 0.8459\n",
            "Epoch 2143/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3163 - acc: 0.8746 - val_loss: 0.4519 - val_acc: 0.8476\n",
            "Epoch 2144/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3156 - acc: 0.8711 - val_loss: 0.4452 - val_acc: 0.8505\n",
            "Epoch 2145/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3156 - acc: 0.8700 - val_loss: 0.4386 - val_acc: 0.8519\n",
            "Epoch 2146/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3210 - acc: 0.8694 - val_loss: 0.4339 - val_acc: 0.8522\n",
            "Epoch 2147/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3194 - acc: 0.8702 - val_loss: 0.4338 - val_acc: 0.8502\n",
            "Epoch 2148/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3136 - acc: 0.8709 - val_loss: 0.4377 - val_acc: 0.8452\n",
            "Epoch 2149/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3230 - acc: 0.8719 - val_loss: 0.4447 - val_acc: 0.8452\n",
            "Epoch 2150/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3203 - acc: 0.8703 - val_loss: 0.4476 - val_acc: 0.8436\n",
            "Epoch 2151/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3103 - acc: 0.8740 - val_loss: 0.4446 - val_acc: 0.8469\n",
            "Epoch 2152/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3093 - acc: 0.8738 - val_loss: 0.4404 - val_acc: 0.8492\n",
            "Epoch 2153/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3232 - acc: 0.8687 - val_loss: 0.4401 - val_acc: 0.8528\n",
            "Epoch 2154/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.8748 - val_loss: 0.4427 - val_acc: 0.8499\n",
            "Epoch 2155/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3155 - acc: 0.8724 - val_loss: 0.4510 - val_acc: 0.8452\n",
            "Epoch 2156/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3139 - acc: 0.8724 - val_loss: 0.4505 - val_acc: 0.8433\n",
            "Epoch 2157/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3209 - acc: 0.8701 - val_loss: 0.4420 - val_acc: 0.8469\n",
            "Epoch 2158/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3210 - acc: 0.8687 - val_loss: 0.4373 - val_acc: 0.8472\n",
            "Epoch 2159/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3188 - acc: 0.8729 - val_loss: 0.4387 - val_acc: 0.8449\n",
            "Epoch 2160/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3204 - acc: 0.8691 - val_loss: 0.4436 - val_acc: 0.8429\n",
            "Epoch 2161/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3167 - acc: 0.8722 - val_loss: 0.4485 - val_acc: 0.8426\n",
            "Epoch 2162/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3085 - acc: 0.8786 - val_loss: 0.4526 - val_acc: 0.8426\n",
            "Epoch 2163/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3119 - acc: 0.8746 - val_loss: 0.4498 - val_acc: 0.8413\n",
            "Epoch 2164/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3153 - acc: 0.8713 - val_loss: 0.4430 - val_acc: 0.8452\n",
            "Epoch 2165/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3216 - acc: 0.8724 - val_loss: 0.4378 - val_acc: 0.8489\n",
            "Epoch 2166/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3168 - acc: 0.8723 - val_loss: 0.4390 - val_acc: 0.8495\n",
            "Epoch 2167/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3120 - acc: 0.8766 - val_loss: 0.4416 - val_acc: 0.8476\n",
            "Epoch 2168/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3176 - acc: 0.8708 - val_loss: 0.4462 - val_acc: 0.8456\n",
            "Epoch 2169/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3140 - acc: 0.8733 - val_loss: 0.4488 - val_acc: 0.8426\n",
            "Epoch 2170/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3119 - acc: 0.8731 - val_loss: 0.4478 - val_acc: 0.8423\n",
            "Epoch 2171/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3256 - acc: 0.8647 - val_loss: 0.4406 - val_acc: 0.8446\n",
            "Epoch 2172/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3127 - acc: 0.8712 - val_loss: 0.4348 - val_acc: 0.8476\n",
            "Epoch 2173/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3201 - acc: 0.8676 - val_loss: 0.4339 - val_acc: 0.8502\n",
            "Epoch 2174/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3176 - acc: 0.8712 - val_loss: 0.4369 - val_acc: 0.8466\n",
            "Epoch 2175/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3140 - acc: 0.8720 - val_loss: 0.4437 - val_acc: 0.8426\n",
            "Epoch 2176/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3127 - acc: 0.8759 - val_loss: 0.4482 - val_acc: 0.8423\n",
            "Epoch 2177/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3174 - acc: 0.8697 - val_loss: 0.4474 - val_acc: 0.8459\n",
            "Epoch 2178/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3193 - acc: 0.8715 - val_loss: 0.4393 - val_acc: 0.8489\n",
            "Epoch 2179/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3143 - acc: 0.8736 - val_loss: 0.4319 - val_acc: 0.8456\n",
            "Epoch 2180/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3158 - acc: 0.8707 - val_loss: 0.4329 - val_acc: 0.8452\n",
            "Epoch 2181/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.8715 - val_loss: 0.4356 - val_acc: 0.8423\n",
            "Epoch 2182/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3226 - acc: 0.8695 - val_loss: 0.4391 - val_acc: 0.8442\n",
            "Epoch 2183/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3218 - acc: 0.8709 - val_loss: 0.4453 - val_acc: 0.8413\n",
            "Epoch 2184/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3156 - acc: 0.8722 - val_loss: 0.4447 - val_acc: 0.8442\n",
            "Epoch 2185/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3176 - acc: 0.8710 - val_loss: 0.4400 - val_acc: 0.8482\n",
            "Epoch 2186/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3160 - acc: 0.8733 - val_loss: 0.4361 - val_acc: 0.8469\n",
            "Epoch 2187/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3204 - acc: 0.8710 - val_loss: 0.4346 - val_acc: 0.8476\n",
            "Epoch 2188/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3196 - acc: 0.8729 - val_loss: 0.4382 - val_acc: 0.8462\n",
            "Epoch 2189/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.8786 - val_loss: 0.4410 - val_acc: 0.8452\n",
            "Epoch 2190/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3160 - acc: 0.8743 - val_loss: 0.4406 - val_acc: 0.8452\n",
            "Epoch 2191/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3174 - acc: 0.8703 - val_loss: 0.4380 - val_acc: 0.8459\n",
            "Epoch 2192/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3217 - acc: 0.8685 - val_loss: 0.4367 - val_acc: 0.8426\n",
            "Epoch 2193/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3225 - acc: 0.8684 - val_loss: 0.4376 - val_acc: 0.8466\n",
            "Epoch 2194/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.8719 - val_loss: 0.4373 - val_acc: 0.8466\n",
            "Epoch 2195/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3139 - acc: 0.8752 - val_loss: 0.4352 - val_acc: 0.8482\n",
            "Epoch 2196/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3158 - acc: 0.8725 - val_loss: 0.4337 - val_acc: 0.8462\n",
            "Epoch 2197/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3138 - acc: 0.8726 - val_loss: 0.4349 - val_acc: 0.8449\n",
            "Epoch 2198/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3163 - acc: 0.8709 - val_loss: 0.4350 - val_acc: 0.8472\n",
            "Epoch 2199/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3093 - acc: 0.8761 - val_loss: 0.4357 - val_acc: 0.8485\n",
            "Epoch 2200/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3166 - acc: 0.8752 - val_loss: 0.4333 - val_acc: 0.8492\n",
            "Epoch 2201/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3173 - acc: 0.8714 - val_loss: 0.4303 - val_acc: 0.8499\n",
            "Epoch 2202/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3125 - acc: 0.8733 - val_loss: 0.4309 - val_acc: 0.8492\n",
            "Epoch 2203/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3109 - acc: 0.8760 - val_loss: 0.4344 - val_acc: 0.8476\n",
            "Epoch 2204/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3108 - acc: 0.8752 - val_loss: 0.4367 - val_acc: 0.8482\n",
            "Epoch 2205/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3164 - acc: 0.8709 - val_loss: 0.4349 - val_acc: 0.8505\n",
            "Epoch 2206/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3187 - acc: 0.8731 - val_loss: 0.4325 - val_acc: 0.8532\n",
            "Epoch 2207/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3129 - acc: 0.8736 - val_loss: 0.4331 - val_acc: 0.8522\n",
            "Epoch 2208/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3216 - acc: 0.8665 - val_loss: 0.4360 - val_acc: 0.8502\n",
            "Epoch 2209/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3106 - acc: 0.8745 - val_loss: 0.4396 - val_acc: 0.8489\n",
            "Epoch 2210/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3103 - acc: 0.8732 - val_loss: 0.4384 - val_acc: 0.8495\n",
            "Epoch 2211/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3122 - acc: 0.8744 - val_loss: 0.4377 - val_acc: 0.8489\n",
            "Epoch 2212/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3136 - acc: 0.8757 - val_loss: 0.4373 - val_acc: 0.8495\n",
            "Epoch 2213/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3232 - acc: 0.8719 - val_loss: 0.4363 - val_acc: 0.8492\n",
            "Epoch 2214/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3176 - acc: 0.8728 - val_loss: 0.4338 - val_acc: 0.8502\n",
            "Epoch 2215/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3128 - acc: 0.8733 - val_loss: 0.4340 - val_acc: 0.8492\n",
            "Epoch 2216/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8709 - val_loss: 0.4340 - val_acc: 0.8492\n",
            "Epoch 2217/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3164 - acc: 0.8733 - val_loss: 0.4355 - val_acc: 0.8495\n",
            "Epoch 2218/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3120 - acc: 0.8745 - val_loss: 0.4373 - val_acc: 0.8499\n",
            "Epoch 2219/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8735 - val_loss: 0.4375 - val_acc: 0.8462\n",
            "Epoch 2220/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3139 - acc: 0.8737 - val_loss: 0.4368 - val_acc: 0.8459\n",
            "Epoch 2221/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3064 - acc: 0.8752 - val_loss: 0.4342 - val_acc: 0.8485\n",
            "Epoch 2222/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8751 - val_loss: 0.4340 - val_acc: 0.8492\n",
            "Epoch 2223/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3089 - acc: 0.8729 - val_loss: 0.4352 - val_acc: 0.8499\n",
            "Epoch 2224/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3132 - acc: 0.8759 - val_loss: 0.4377 - val_acc: 0.8485\n",
            "Epoch 2225/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3135 - acc: 0.8724 - val_loss: 0.4375 - val_acc: 0.8505\n",
            "Epoch 2226/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3175 - acc: 0.8755 - val_loss: 0.4359 - val_acc: 0.8528\n",
            "Epoch 2227/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3203 - acc: 0.8725 - val_loss: 0.4350 - val_acc: 0.8505\n",
            "Epoch 2228/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3105 - acc: 0.8734 - val_loss: 0.4349 - val_acc: 0.8495\n",
            "Epoch 2229/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3206 - acc: 0.8733 - val_loss: 0.4358 - val_acc: 0.8512\n",
            "Epoch 2230/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3135 - acc: 0.8724 - val_loss: 0.4344 - val_acc: 0.8505\n",
            "Epoch 2231/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3102 - acc: 0.8738 - val_loss: 0.4312 - val_acc: 0.8522\n",
            "Epoch 2232/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3149 - acc: 0.8727 - val_loss: 0.4303 - val_acc: 0.8505\n",
            "Epoch 2233/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3083 - acc: 0.8758 - val_loss: 0.4337 - val_acc: 0.8499\n",
            "Epoch 2234/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3117 - acc: 0.8738 - val_loss: 0.4374 - val_acc: 0.8505\n",
            "Epoch 2235/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3178 - acc: 0.8711 - val_loss: 0.4426 - val_acc: 0.8509\n",
            "Epoch 2236/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3195 - acc: 0.8725 - val_loss: 0.4429 - val_acc: 0.8512\n",
            "Epoch 2237/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3178 - acc: 0.8752 - val_loss: 0.4362 - val_acc: 0.8512\n",
            "Epoch 2238/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3121 - acc: 0.8746 - val_loss: 0.4318 - val_acc: 0.8528\n",
            "Epoch 2239/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3063 - acc: 0.8794 - val_loss: 0.4315 - val_acc: 0.8522\n",
            "Epoch 2240/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3177 - acc: 0.8742 - val_loss: 0.4328 - val_acc: 0.8565\n",
            "Epoch 2241/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3160 - acc: 0.8740 - val_loss: 0.4369 - val_acc: 0.8528\n",
            "Epoch 2242/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3132 - acc: 0.8754 - val_loss: 0.4421 - val_acc: 0.8525\n",
            "Epoch 2243/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3154 - acc: 0.8718 - val_loss: 0.4398 - val_acc: 0.8515\n",
            "Epoch 2244/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3154 - acc: 0.8724 - val_loss: 0.4355 - val_acc: 0.8528\n",
            "Epoch 2245/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3131 - acc: 0.8707 - val_loss: 0.4339 - val_acc: 0.8532\n",
            "Epoch 2246/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3166 - acc: 0.8761 - val_loss: 0.4365 - val_acc: 0.8519\n",
            "Epoch 2247/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3092 - acc: 0.8739 - val_loss: 0.4405 - val_acc: 0.8495\n",
            "Epoch 2248/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3159 - acc: 0.8700 - val_loss: 0.4372 - val_acc: 0.8502\n",
            "Epoch 2249/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3130 - acc: 0.8748 - val_loss: 0.4330 - val_acc: 0.8502\n",
            "Epoch 2250/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3157 - acc: 0.8752 - val_loss: 0.4332 - val_acc: 0.8462\n",
            "Epoch 2251/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3130 - acc: 0.8728 - val_loss: 0.4332 - val_acc: 0.8479\n",
            "Epoch 2252/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3057 - acc: 0.8750 - val_loss: 0.4331 - val_acc: 0.8519\n",
            "Epoch 2253/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3115 - acc: 0.8749 - val_loss: 0.4327 - val_acc: 0.8525\n",
            "Epoch 2254/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3139 - acc: 0.8738 - val_loss: 0.4314 - val_acc: 0.8512\n",
            "Epoch 2255/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8742 - val_loss: 0.4320 - val_acc: 0.8502\n",
            "Epoch 2256/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3096 - acc: 0.8767 - val_loss: 0.4329 - val_acc: 0.8505\n",
            "Epoch 2257/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3096 - acc: 0.8715 - val_loss: 0.4345 - val_acc: 0.8495\n",
            "Epoch 2258/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3221 - acc: 0.8667 - val_loss: 0.4373 - val_acc: 0.8499\n",
            "Epoch 2259/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3150 - acc: 0.8728 - val_loss: 0.4377 - val_acc: 0.8485\n",
            "Epoch 2260/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3110 - acc: 0.8759 - val_loss: 0.4394 - val_acc: 0.8482\n",
            "Epoch 2261/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8773 - val_loss: 0.4373 - val_acc: 0.8479\n",
            "Epoch 2262/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3066 - acc: 0.8784 - val_loss: 0.4369 - val_acc: 0.8525\n",
            "Epoch 2263/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3172 - acc: 0.8715 - val_loss: 0.4379 - val_acc: 0.8528\n",
            "Epoch 2264/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3126 - acc: 0.8758 - val_loss: 0.4383 - val_acc: 0.8505\n",
            "Epoch 2265/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3129 - acc: 0.8746 - val_loss: 0.4373 - val_acc: 0.8489\n",
            "Epoch 2266/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3145 - acc: 0.8724 - val_loss: 0.4399 - val_acc: 0.8515\n",
            "Epoch 2267/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3077 - acc: 0.8739 - val_loss: 0.4404 - val_acc: 0.8512\n",
            "Epoch 2268/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3092 - acc: 0.8714 - val_loss: 0.4368 - val_acc: 0.8512\n",
            "Epoch 2269/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3058 - acc: 0.8757 - val_loss: 0.4355 - val_acc: 0.8502\n",
            "Epoch 2270/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3141 - acc: 0.8724 - val_loss: 0.4356 - val_acc: 0.8512\n",
            "Epoch 2271/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3125 - acc: 0.8737 - val_loss: 0.4381 - val_acc: 0.8499\n",
            "Epoch 2272/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3111 - acc: 0.8742 - val_loss: 0.4356 - val_acc: 0.8509\n",
            "Epoch 2273/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3115 - acc: 0.8737 - val_loss: 0.4357 - val_acc: 0.8528\n",
            "Epoch 2274/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3049 - acc: 0.8758 - val_loss: 0.4378 - val_acc: 0.8505\n",
            "Epoch 2275/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3058 - acc: 0.8762 - val_loss: 0.4383 - val_acc: 0.8502\n",
            "Epoch 2276/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3085 - acc: 0.8750 - val_loss: 0.4402 - val_acc: 0.8519\n",
            "Epoch 2277/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3158 - acc: 0.8700 - val_loss: 0.4419 - val_acc: 0.8502\n",
            "Epoch 2278/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3181 - acc: 0.8724 - val_loss: 0.4381 - val_acc: 0.8505\n",
            "Epoch 2279/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3079 - acc: 0.8788 - val_loss: 0.4365 - val_acc: 0.8512\n",
            "Epoch 2280/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.8776 - val_loss: 0.4372 - val_acc: 0.8499\n",
            "Epoch 2281/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8755 - val_loss: 0.4381 - val_acc: 0.8479\n",
            "Epoch 2282/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3148 - acc: 0.8708 - val_loss: 0.4389 - val_acc: 0.8482\n",
            "Epoch 2283/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3080 - acc: 0.8769 - val_loss: 0.4384 - val_acc: 0.8505\n",
            "Epoch 2284/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3087 - acc: 0.8735 - val_loss: 0.4344 - val_acc: 0.8535\n",
            "Epoch 2285/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3089 - acc: 0.8714 - val_loss: 0.4334 - val_acc: 0.8545\n",
            "Epoch 2286/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3084 - acc: 0.8730 - val_loss: 0.4336 - val_acc: 0.8535\n",
            "Epoch 2287/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8804 - val_loss: 0.4351 - val_acc: 0.8552\n",
            "Epoch 2288/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3105 - acc: 0.8743 - val_loss: 0.4358 - val_acc: 0.8538\n",
            "Epoch 2289/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.8786 - val_loss: 0.4349 - val_acc: 0.8575\n",
            "Epoch 2290/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3064 - acc: 0.8738 - val_loss: 0.4343 - val_acc: 0.8538\n",
            "Epoch 2291/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3134 - acc: 0.8743 - val_loss: 0.4333 - val_acc: 0.8542\n",
            "Epoch 2292/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8755 - val_loss: 0.4345 - val_acc: 0.8545\n",
            "Epoch 2293/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8714 - val_loss: 0.4369 - val_acc: 0.8502\n",
            "Epoch 2294/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3094 - acc: 0.8774 - val_loss: 0.4399 - val_acc: 0.8502\n",
            "Epoch 2295/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3154 - acc: 0.8757 - val_loss: 0.4405 - val_acc: 0.8522\n",
            "Epoch 2296/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3046 - acc: 0.8774 - val_loss: 0.4381 - val_acc: 0.8502\n",
            "Epoch 2297/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3133 - acc: 0.8733 - val_loss: 0.4362 - val_acc: 0.8512\n",
            "Epoch 2298/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3155 - acc: 0.8714 - val_loss: 0.4354 - val_acc: 0.8519\n",
            "Epoch 2299/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3049 - acc: 0.8771 - val_loss: 0.4373 - val_acc: 0.8535\n",
            "Epoch 2300/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3114 - acc: 0.8753 - val_loss: 0.4403 - val_acc: 0.8532\n",
            "Epoch 2301/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3087 - acc: 0.8754 - val_loss: 0.4413 - val_acc: 0.8505\n",
            "Epoch 2302/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3071 - acc: 0.8757 - val_loss: 0.4405 - val_acc: 0.8502\n",
            "Epoch 2303/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3077 - acc: 0.8730 - val_loss: 0.4383 - val_acc: 0.8502\n",
            "Epoch 2304/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3092 - acc: 0.8755 - val_loss: 0.4334 - val_acc: 0.8532\n",
            "Epoch 2305/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3167 - acc: 0.8690 - val_loss: 0.4375 - val_acc: 0.8519\n",
            "Epoch 2306/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3104 - acc: 0.8778 - val_loss: 0.4413 - val_acc: 0.8502\n",
            "Epoch 2307/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3108 - acc: 0.8732 - val_loss: 0.4416 - val_acc: 0.8528\n",
            "Epoch 2308/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3016 - acc: 0.8762 - val_loss: 0.4387 - val_acc: 0.8552\n",
            "Epoch 2309/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.8748 - val_loss: 0.4380 - val_acc: 0.8532\n",
            "Epoch 2310/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3119 - acc: 0.8760 - val_loss: 0.4397 - val_acc: 0.8515\n",
            "Epoch 2311/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3019 - acc: 0.8814 - val_loss: 0.4418 - val_acc: 0.8502\n",
            "Epoch 2312/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3088 - acc: 0.8721 - val_loss: 0.4378 - val_acc: 0.8515\n",
            "Epoch 2313/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3150 - acc: 0.8745 - val_loss: 0.4340 - val_acc: 0.8528\n",
            "Epoch 2314/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3140 - acc: 0.8746 - val_loss: 0.4310 - val_acc: 0.8502\n",
            "Epoch 2315/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3194 - acc: 0.8726 - val_loss: 0.4285 - val_acc: 0.8525\n",
            "Epoch 2316/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.8737 - val_loss: 0.4289 - val_acc: 0.8555\n",
            "Epoch 2317/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3077 - acc: 0.8747 - val_loss: 0.4337 - val_acc: 0.8525\n",
            "Epoch 2318/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3115 - acc: 0.8774 - val_loss: 0.4346 - val_acc: 0.8562\n",
            "Epoch 2319/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3144 - acc: 0.8728 - val_loss: 0.4333 - val_acc: 0.8558\n",
            "Epoch 2320/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.8758 - val_loss: 0.4327 - val_acc: 0.8515\n",
            "Epoch 2321/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3105 - acc: 0.8752 - val_loss: 0.4311 - val_acc: 0.8505\n",
            "Epoch 2322/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3085 - acc: 0.8786 - val_loss: 0.4292 - val_acc: 0.8525\n",
            "Epoch 2323/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3131 - acc: 0.8734 - val_loss: 0.4315 - val_acc: 0.8542\n",
            "Epoch 2324/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3072 - acc: 0.8733 - val_loss: 0.4339 - val_acc: 0.8512\n",
            "Epoch 2325/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3109 - acc: 0.8746 - val_loss: 0.4365 - val_acc: 0.8499\n",
            "Epoch 2326/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3131 - acc: 0.8765 - val_loss: 0.4385 - val_acc: 0.8535\n",
            "Epoch 2327/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3068 - acc: 0.8748 - val_loss: 0.4376 - val_acc: 0.8542\n",
            "Epoch 2328/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3127 - acc: 0.8773 - val_loss: 0.4363 - val_acc: 0.8522\n",
            "Epoch 2329/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.8738 - val_loss: 0.4365 - val_acc: 0.8492\n",
            "Epoch 2330/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3063 - acc: 0.8767 - val_loss: 0.4380 - val_acc: 0.8495\n",
            "Epoch 2331/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3083 - acc: 0.8742 - val_loss: 0.4369 - val_acc: 0.8502\n",
            "Epoch 2332/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3204 - acc: 0.8715 - val_loss: 0.4329 - val_acc: 0.8505\n",
            "Epoch 2333/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3108 - acc: 0.8732 - val_loss: 0.4308 - val_acc: 0.8505\n",
            "Epoch 2334/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3007 - acc: 0.8776 - val_loss: 0.4330 - val_acc: 0.8519\n",
            "Epoch 2335/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8767 - val_loss: 0.4334 - val_acc: 0.8522\n",
            "Epoch 2336/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3112 - acc: 0.8725 - val_loss: 0.4326 - val_acc: 0.8545\n",
            "Epoch 2337/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3134 - acc: 0.8743 - val_loss: 0.4310 - val_acc: 0.8558\n",
            "Epoch 2338/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3081 - acc: 0.8774 - val_loss: 0.4281 - val_acc: 0.8578\n",
            "Epoch 2339/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3113 - acc: 0.8713 - val_loss: 0.4289 - val_acc: 0.8562\n",
            "Epoch 2340/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3106 - acc: 0.8723 - val_loss: 0.4315 - val_acc: 0.8552\n",
            "Epoch 2341/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.8752 - val_loss: 0.4360 - val_acc: 0.8528\n",
            "Epoch 2342/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3127 - acc: 0.8711 - val_loss: 0.4402 - val_acc: 0.8505\n",
            "Epoch 2343/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2991 - acc: 0.8794 - val_loss: 0.4411 - val_acc: 0.8495\n",
            "Epoch 2344/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3091 - acc: 0.8768 - val_loss: 0.4390 - val_acc: 0.8502\n",
            "Epoch 2345/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3111 - acc: 0.8765 - val_loss: 0.4368 - val_acc: 0.8522\n",
            "Epoch 2346/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3108 - acc: 0.8738 - val_loss: 0.4352 - val_acc: 0.8532\n",
            "Epoch 2347/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3114 - acc: 0.8746 - val_loss: 0.4349 - val_acc: 0.8535\n",
            "Epoch 2348/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8751 - val_loss: 0.4367 - val_acc: 0.8535\n",
            "Epoch 2349/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3100 - acc: 0.8768 - val_loss: 0.4367 - val_acc: 0.8542\n",
            "Epoch 2350/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3087 - acc: 0.8718 - val_loss: 0.4331 - val_acc: 0.8525\n",
            "Epoch 2351/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3083 - acc: 0.8743 - val_loss: 0.4318 - val_acc: 0.8519\n",
            "Epoch 2352/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3159 - acc: 0.8729 - val_loss: 0.4321 - val_acc: 0.8555\n",
            "Epoch 2353/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.8791 - val_loss: 0.4343 - val_acc: 0.8528\n",
            "Epoch 2354/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8743 - val_loss: 0.4352 - val_acc: 0.8528\n",
            "Epoch 2355/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3161 - acc: 0.8739 - val_loss: 0.4341 - val_acc: 0.8528\n",
            "Epoch 2356/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3155 - acc: 0.8689 - val_loss: 0.4341 - val_acc: 0.8535\n",
            "Epoch 2357/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8743 - val_loss: 0.4351 - val_acc: 0.8538\n",
            "Epoch 2358/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.8771 - val_loss: 0.4334 - val_acc: 0.8545\n",
            "Epoch 2359/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3182 - acc: 0.8689 - val_loss: 0.4318 - val_acc: 0.8519\n",
            "Epoch 2360/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3088 - acc: 0.8753 - val_loss: 0.4311 - val_acc: 0.8545\n",
            "Epoch 2361/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.8792 - val_loss: 0.4310 - val_acc: 0.8542\n",
            "Epoch 2362/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8772 - val_loss: 0.4353 - val_acc: 0.8505\n",
            "Epoch 2363/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.8743 - val_loss: 0.4372 - val_acc: 0.8492\n",
            "Epoch 2364/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3058 - acc: 0.8766 - val_loss: 0.4358 - val_acc: 0.8502\n",
            "Epoch 2365/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3102 - acc: 0.8721 - val_loss: 0.4307 - val_acc: 0.8525\n",
            "Epoch 2366/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.8757 - val_loss: 0.4290 - val_acc: 0.8522\n",
            "Epoch 2367/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8770 - val_loss: 0.4321 - val_acc: 0.8528\n",
            "Epoch 2368/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3126 - acc: 0.8719 - val_loss: 0.4371 - val_acc: 0.8525\n",
            "Epoch 2369/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3011 - acc: 0.8790 - val_loss: 0.4385 - val_acc: 0.8515\n",
            "Epoch 2370/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3117 - acc: 0.8743 - val_loss: 0.4392 - val_acc: 0.8519\n",
            "Epoch 2371/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3031 - acc: 0.8768 - val_loss: 0.4375 - val_acc: 0.8515\n",
            "Epoch 2372/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3111 - acc: 0.8771 - val_loss: 0.4360 - val_acc: 0.8466\n",
            "Epoch 2373/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3056 - acc: 0.8799 - val_loss: 0.4335 - val_acc: 0.8502\n",
            "Epoch 2374/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3210 - acc: 0.8713 - val_loss: 0.4334 - val_acc: 0.8528\n",
            "Epoch 2375/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3046 - acc: 0.8762 - val_loss: 0.4375 - val_acc: 0.8538\n",
            "Epoch 2376/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3059 - acc: 0.8780 - val_loss: 0.4400 - val_acc: 0.8505\n",
            "Epoch 2377/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.8746 - val_loss: 0.4394 - val_acc: 0.8502\n",
            "Epoch 2378/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3056 - acc: 0.8771 - val_loss: 0.4356 - val_acc: 0.8489\n",
            "Epoch 2379/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3110 - acc: 0.8767 - val_loss: 0.4306 - val_acc: 0.8505\n",
            "Epoch 2380/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.8775 - val_loss: 0.4305 - val_acc: 0.8502\n",
            "Epoch 2381/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3081 - acc: 0.8756 - val_loss: 0.4343 - val_acc: 0.8499\n",
            "Epoch 2382/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3011 - acc: 0.8782 - val_loss: 0.4339 - val_acc: 0.8489\n",
            "Epoch 2383/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3048 - acc: 0.8806 - val_loss: 0.4337 - val_acc: 0.8495\n",
            "Epoch 2384/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3090 - acc: 0.8752 - val_loss: 0.4325 - val_acc: 0.8515\n",
            "Epoch 2385/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3050 - acc: 0.8809 - val_loss: 0.4326 - val_acc: 0.8538\n",
            "Epoch 2386/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8731 - val_loss: 0.4348 - val_acc: 0.8515\n",
            "Epoch 2387/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3151 - acc: 0.8697 - val_loss: 0.4363 - val_acc: 0.8492\n",
            "Epoch 2388/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3074 - acc: 0.8796 - val_loss: 0.4358 - val_acc: 0.8476\n",
            "Epoch 2389/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3064 - acc: 0.8709 - val_loss: 0.4338 - val_acc: 0.8502\n",
            "Epoch 2390/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8771 - val_loss: 0.4305 - val_acc: 0.8499\n",
            "Epoch 2391/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3076 - acc: 0.8771 - val_loss: 0.4275 - val_acc: 0.8515\n",
            "Epoch 2392/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3081 - acc: 0.8776 - val_loss: 0.4290 - val_acc: 0.8509\n",
            "Epoch 2393/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2995 - acc: 0.8774 - val_loss: 0.4309 - val_acc: 0.8538\n",
            "Epoch 2394/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3063 - acc: 0.8757 - val_loss: 0.4351 - val_acc: 0.8535\n",
            "Epoch 2395/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3028 - acc: 0.8781 - val_loss: 0.4357 - val_acc: 0.8532\n",
            "Epoch 2396/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3088 - acc: 0.8771 - val_loss: 0.4336 - val_acc: 0.8519\n",
            "Epoch 2397/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3027 - acc: 0.8787 - val_loss: 0.4330 - val_acc: 0.8542\n",
            "Epoch 2398/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3088 - acc: 0.8738 - val_loss: 0.4337 - val_acc: 0.8545\n",
            "Epoch 2399/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3024 - acc: 0.8791 - val_loss: 0.4352 - val_acc: 0.8528\n",
            "Epoch 2400/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3048 - acc: 0.8788 - val_loss: 0.4348 - val_acc: 0.8519\n",
            "Epoch 2401/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3057 - acc: 0.8779 - val_loss: 0.4346 - val_acc: 0.8532\n",
            "Epoch 2402/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.8773 - val_loss: 0.4333 - val_acc: 0.8545\n",
            "Epoch 2403/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.8790 - val_loss: 0.4337 - val_acc: 0.8548\n",
            "Epoch 2404/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3003 - acc: 0.8780 - val_loss: 0.4338 - val_acc: 0.8538\n",
            "Epoch 2405/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.8765 - val_loss: 0.4305 - val_acc: 0.8512\n",
            "Epoch 2406/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3105 - acc: 0.8752 - val_loss: 0.4266 - val_acc: 0.8542\n",
            "Epoch 2407/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.8795 - val_loss: 0.4287 - val_acc: 0.8538\n",
            "Epoch 2408/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3081 - acc: 0.8753 - val_loss: 0.4333 - val_acc: 0.8522\n",
            "Epoch 2409/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3044 - acc: 0.8766 - val_loss: 0.4360 - val_acc: 0.8545\n",
            "Epoch 2410/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3033 - acc: 0.8766 - val_loss: 0.4329 - val_acc: 0.8578\n",
            "Epoch 2411/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2993 - acc: 0.8783 - val_loss: 0.4306 - val_acc: 0.8578\n",
            "Epoch 2412/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3040 - acc: 0.8762 - val_loss: 0.4299 - val_acc: 0.8545\n",
            "Epoch 2413/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3087 - acc: 0.8780 - val_loss: 0.4299 - val_acc: 0.8535\n",
            "Epoch 2414/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3063 - acc: 0.8797 - val_loss: 0.4312 - val_acc: 0.8525\n",
            "Epoch 2415/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3076 - acc: 0.8762 - val_loss: 0.4287 - val_acc: 0.8538\n",
            "Epoch 2416/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8758 - val_loss: 0.4304 - val_acc: 0.8538\n",
            "Epoch 2417/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3030 - acc: 0.8776 - val_loss: 0.4334 - val_acc: 0.8538\n",
            "Epoch 2418/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3142 - acc: 0.8752 - val_loss: 0.4332 - val_acc: 0.8575\n",
            "Epoch 2419/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3079 - acc: 0.8764 - val_loss: 0.4316 - val_acc: 0.8568\n",
            "Epoch 2420/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3065 - acc: 0.8758 - val_loss: 0.4319 - val_acc: 0.8535\n",
            "Epoch 2421/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.8787 - val_loss: 0.4334 - val_acc: 0.8512\n",
            "Epoch 2422/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3135 - acc: 0.8737 - val_loss: 0.4331 - val_acc: 0.8502\n",
            "Epoch 2423/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3126 - acc: 0.8750 - val_loss: 0.4336 - val_acc: 0.8519\n",
            "Epoch 2424/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3048 - acc: 0.8775 - val_loss: 0.4324 - val_acc: 0.8532\n",
            "Epoch 2425/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3044 - acc: 0.8775 - val_loss: 0.4300 - val_acc: 0.8552\n",
            "Epoch 2426/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2996 - acc: 0.8807 - val_loss: 0.4315 - val_acc: 0.8565\n",
            "Epoch 2427/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8782 - val_loss: 0.4312 - val_acc: 0.8568\n",
            "Epoch 2428/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3093 - acc: 0.8752 - val_loss: 0.4290 - val_acc: 0.8575\n",
            "Epoch 2429/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3092 - acc: 0.8736 - val_loss: 0.4278 - val_acc: 0.8552\n",
            "Epoch 2430/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.8770 - val_loss: 0.4299 - val_acc: 0.8535\n",
            "Epoch 2431/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.8757 - val_loss: 0.4315 - val_acc: 0.8538\n",
            "Epoch 2432/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3089 - acc: 0.8729 - val_loss: 0.4328 - val_acc: 0.8548\n",
            "Epoch 2433/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3079 - acc: 0.8790 - val_loss: 0.4328 - val_acc: 0.8522\n",
            "Epoch 2434/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3040 - acc: 0.8782 - val_loss: 0.4274 - val_acc: 0.8548\n",
            "Epoch 2435/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3058 - acc: 0.8795 - val_loss: 0.4252 - val_acc: 0.8595\n",
            "Epoch 2436/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3102 - acc: 0.8700 - val_loss: 0.4257 - val_acc: 0.8581\n",
            "Epoch 2437/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3026 - acc: 0.8797 - val_loss: 0.4246 - val_acc: 0.8565\n",
            "Epoch 2438/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3068 - acc: 0.8763 - val_loss: 0.4235 - val_acc: 0.8562\n",
            "Epoch 2439/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2956 - acc: 0.8792 - val_loss: 0.4240 - val_acc: 0.8558\n",
            "Epoch 2440/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3048 - acc: 0.8786 - val_loss: 0.4281 - val_acc: 0.8542\n",
            "Epoch 2441/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3044 - acc: 0.8764 - val_loss: 0.4296 - val_acc: 0.8595\n",
            "Epoch 2442/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3096 - acc: 0.8737 - val_loss: 0.4310 - val_acc: 0.8571\n",
            "Epoch 2443/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2995 - acc: 0.8787 - val_loss: 0.4306 - val_acc: 0.8575\n",
            "Epoch 2444/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.8772 - val_loss: 0.4291 - val_acc: 0.8565\n",
            "Epoch 2445/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.8748 - val_loss: 0.4300 - val_acc: 0.8538\n",
            "Epoch 2446/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3030 - acc: 0.8774 - val_loss: 0.4320 - val_acc: 0.8512\n",
            "Epoch 2447/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3086 - acc: 0.8795 - val_loss: 0.4346 - val_acc: 0.8515\n",
            "Epoch 2448/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3056 - acc: 0.8770 - val_loss: 0.4354 - val_acc: 0.8562\n",
            "Epoch 2449/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.8772 - val_loss: 0.4340 - val_acc: 0.8562\n",
            "Epoch 2450/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2985 - acc: 0.8813 - val_loss: 0.4343 - val_acc: 0.8525\n",
            "Epoch 2451/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8781 - val_loss: 0.4346 - val_acc: 0.8552\n",
            "Epoch 2452/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8761 - val_loss: 0.4352 - val_acc: 0.8552\n",
            "Epoch 2453/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3074 - acc: 0.8743 - val_loss: 0.4352 - val_acc: 0.8565\n",
            "Epoch 2454/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3036 - acc: 0.8781 - val_loss: 0.4384 - val_acc: 0.8535\n",
            "Epoch 2455/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3066 - acc: 0.8749 - val_loss: 0.4406 - val_acc: 0.8548\n",
            "Epoch 2456/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3018 - acc: 0.8812 - val_loss: 0.4393 - val_acc: 0.8552\n",
            "Epoch 2457/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3031 - acc: 0.8795 - val_loss: 0.4363 - val_acc: 0.8538\n",
            "Epoch 2458/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3062 - acc: 0.8748 - val_loss: 0.4324 - val_acc: 0.8555\n",
            "Epoch 2459/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3014 - acc: 0.8782 - val_loss: 0.4292 - val_acc: 0.8562\n",
            "Epoch 2460/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3048 - acc: 0.8775 - val_loss: 0.4286 - val_acc: 0.8542\n",
            "Epoch 2461/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3077 - acc: 0.8786 - val_loss: 0.4308 - val_acc: 0.8555\n",
            "Epoch 2462/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3016 - acc: 0.8754 - val_loss: 0.4326 - val_acc: 0.8528\n",
            "Epoch 2463/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8764 - val_loss: 0.4322 - val_acc: 0.8552\n",
            "Epoch 2464/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3108 - acc: 0.8725 - val_loss: 0.4300 - val_acc: 0.8598\n",
            "Epoch 2465/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3027 - acc: 0.8768 - val_loss: 0.4300 - val_acc: 0.8585\n",
            "Epoch 2466/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2988 - acc: 0.8774 - val_loss: 0.4331 - val_acc: 0.8552\n",
            "Epoch 2467/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8763 - val_loss: 0.4371 - val_acc: 0.8562\n",
            "Epoch 2468/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3045 - acc: 0.8781 - val_loss: 0.4389 - val_acc: 0.8565\n",
            "Epoch 2469/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3071 - acc: 0.8796 - val_loss: 0.4371 - val_acc: 0.8601\n",
            "Epoch 2470/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3028 - acc: 0.8797 - val_loss: 0.4341 - val_acc: 0.8571\n",
            "Epoch 2471/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3027 - acc: 0.8762 - val_loss: 0.4290 - val_acc: 0.8562\n",
            "Epoch 2472/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2978 - acc: 0.8805 - val_loss: 0.4276 - val_acc: 0.8575\n",
            "Epoch 2473/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3110 - acc: 0.8746 - val_loss: 0.4277 - val_acc: 0.8578\n",
            "Epoch 2474/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3031 - acc: 0.8792 - val_loss: 0.4300 - val_acc: 0.8548\n",
            "Epoch 2475/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.8787 - val_loss: 0.4307 - val_acc: 0.8545\n",
            "Epoch 2476/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2999 - acc: 0.8786 - val_loss: 0.4295 - val_acc: 0.8571\n",
            "Epoch 2477/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3001 - acc: 0.8793 - val_loss: 0.4304 - val_acc: 0.8562\n",
            "Epoch 2478/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8794 - val_loss: 0.4300 - val_acc: 0.8558\n",
            "Epoch 2479/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3067 - acc: 0.8753 - val_loss: 0.4267 - val_acc: 0.8568\n",
            "Epoch 2480/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3049 - acc: 0.8800 - val_loss: 0.4242 - val_acc: 0.8558\n",
            "Epoch 2481/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3028 - acc: 0.8810 - val_loss: 0.4260 - val_acc: 0.8581\n",
            "Epoch 2482/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3069 - acc: 0.8775 - val_loss: 0.4265 - val_acc: 0.8575\n",
            "Epoch 2483/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2920 - acc: 0.8792 - val_loss: 0.4297 - val_acc: 0.8565\n",
            "Epoch 2484/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3104 - acc: 0.8755 - val_loss: 0.4335 - val_acc: 0.8558\n",
            "Epoch 2485/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3011 - acc: 0.8784 - val_loss: 0.4362 - val_acc: 0.8578\n",
            "Epoch 2486/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2988 - acc: 0.8774 - val_loss: 0.4372 - val_acc: 0.8548\n",
            "Epoch 2487/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.8790 - val_loss: 0.4354 - val_acc: 0.8512\n",
            "Epoch 2488/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3102 - acc: 0.8741 - val_loss: 0.4361 - val_acc: 0.8519\n",
            "Epoch 2489/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3079 - acc: 0.8727 - val_loss: 0.4384 - val_acc: 0.8499\n",
            "Epoch 2490/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3034 - acc: 0.8807 - val_loss: 0.4390 - val_acc: 0.8512\n",
            "Epoch 2491/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8790 - val_loss: 0.4387 - val_acc: 0.8499\n",
            "Epoch 2492/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3067 - acc: 0.8758 - val_loss: 0.4354 - val_acc: 0.8519\n",
            "Epoch 2493/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3023 - acc: 0.8796 - val_loss: 0.4335 - val_acc: 0.8528\n",
            "Epoch 2494/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3015 - acc: 0.8803 - val_loss: 0.4328 - val_acc: 0.8515\n",
            "Epoch 2495/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3099 - acc: 0.8758 - val_loss: 0.4319 - val_acc: 0.8522\n",
            "Epoch 2496/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3009 - acc: 0.8792 - val_loss: 0.4313 - val_acc: 0.8509\n",
            "Epoch 2497/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8779 - val_loss: 0.4344 - val_acc: 0.8535\n",
            "Epoch 2498/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3056 - acc: 0.8740 - val_loss: 0.4377 - val_acc: 0.8512\n",
            "Epoch 2499/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2951 - acc: 0.8836 - val_loss: 0.4393 - val_acc: 0.8502\n",
            "Epoch 2500/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.8759 - val_loss: 0.4393 - val_acc: 0.8492\n",
            "Epoch 2501/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.8784 - val_loss: 0.4386 - val_acc: 0.8505\n",
            "Epoch 2502/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3110 - acc: 0.8728 - val_loss: 0.4381 - val_acc: 0.8528\n",
            "Epoch 2503/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8756 - val_loss: 0.4360 - val_acc: 0.8542\n",
            "Epoch 2504/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3000 - acc: 0.8768 - val_loss: 0.4320 - val_acc: 0.8535\n",
            "Epoch 2505/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3030 - acc: 0.8796 - val_loss: 0.4299 - val_acc: 0.8545\n",
            "Epoch 2506/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3029 - acc: 0.8806 - val_loss: 0.4296 - val_acc: 0.8542\n",
            "Epoch 2507/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2992 - acc: 0.8780 - val_loss: 0.4322 - val_acc: 0.8548\n",
            "Epoch 2508/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3027 - acc: 0.8819 - val_loss: 0.4370 - val_acc: 0.8558\n",
            "Epoch 2509/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3068 - acc: 0.8767 - val_loss: 0.4383 - val_acc: 0.8525\n",
            "Epoch 2510/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3049 - acc: 0.8749 - val_loss: 0.4395 - val_acc: 0.8495\n",
            "Epoch 2511/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3003 - acc: 0.8786 - val_loss: 0.4366 - val_acc: 0.8482\n",
            "Epoch 2512/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3063 - acc: 0.8794 - val_loss: 0.4320 - val_acc: 0.8509\n",
            "Epoch 2513/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8779 - val_loss: 0.4299 - val_acc: 0.8532\n",
            "Epoch 2514/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3034 - acc: 0.8762 - val_loss: 0.4317 - val_acc: 0.8512\n",
            "Epoch 2515/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2986 - acc: 0.8796 - val_loss: 0.4360 - val_acc: 0.8519\n",
            "Epoch 2516/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3011 - acc: 0.8789 - val_loss: 0.4383 - val_acc: 0.8535\n",
            "Epoch 2517/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2981 - acc: 0.8805 - val_loss: 0.4365 - val_acc: 0.8532\n",
            "Epoch 2518/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3010 - acc: 0.8804 - val_loss: 0.4338 - val_acc: 0.8542\n",
            "Epoch 2519/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2960 - acc: 0.8825 - val_loss: 0.4338 - val_acc: 0.8525\n",
            "Epoch 2520/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3044 - acc: 0.8770 - val_loss: 0.4348 - val_acc: 0.8535\n",
            "Epoch 2521/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3006 - acc: 0.8785 - val_loss: 0.4345 - val_acc: 0.8532\n",
            "Epoch 2522/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3023 - acc: 0.8826 - val_loss: 0.4323 - val_acc: 0.8565\n",
            "Epoch 2523/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3009 - acc: 0.8795 - val_loss: 0.4310 - val_acc: 0.8598\n",
            "Epoch 2524/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2982 - acc: 0.8816 - val_loss: 0.4314 - val_acc: 0.8601\n",
            "Epoch 2525/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3042 - acc: 0.8770 - val_loss: 0.4332 - val_acc: 0.8591\n",
            "Epoch 2526/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8752 - val_loss: 0.4352 - val_acc: 0.8552\n",
            "Epoch 2527/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3098 - acc: 0.8736 - val_loss: 0.4329 - val_acc: 0.8562\n",
            "Epoch 2528/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8793 - val_loss: 0.4304 - val_acc: 0.8565\n",
            "Epoch 2529/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3060 - acc: 0.8772 - val_loss: 0.4334 - val_acc: 0.8555\n",
            "Epoch 2530/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3004 - acc: 0.8787 - val_loss: 0.4348 - val_acc: 0.8571\n",
            "Epoch 2531/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3067 - acc: 0.8728 - val_loss: 0.4341 - val_acc: 0.8552\n",
            "Epoch 2532/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.8823 - val_loss: 0.4329 - val_acc: 0.8558\n",
            "Epoch 2533/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.8768 - val_loss: 0.4312 - val_acc: 0.8562\n",
            "Epoch 2534/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3004 - acc: 0.8800 - val_loss: 0.4298 - val_acc: 0.8542\n",
            "Epoch 2535/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8767 - val_loss: 0.4292 - val_acc: 0.8515\n",
            "Epoch 2536/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3043 - acc: 0.8759 - val_loss: 0.4304 - val_acc: 0.8528\n",
            "Epoch 2537/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3046 - acc: 0.8757 - val_loss: 0.4306 - val_acc: 0.8545\n",
            "Epoch 2538/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2977 - acc: 0.8790 - val_loss: 0.4331 - val_acc: 0.8548\n",
            "Epoch 2539/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2995 - acc: 0.8806 - val_loss: 0.4364 - val_acc: 0.8548\n",
            "Epoch 2540/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8779 - val_loss: 0.4359 - val_acc: 0.8545\n",
            "Epoch 2541/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8792 - val_loss: 0.4318 - val_acc: 0.8555\n",
            "Epoch 2542/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2987 - acc: 0.8814 - val_loss: 0.4297 - val_acc: 0.8548\n",
            "Epoch 2543/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2999 - acc: 0.8777 - val_loss: 0.4309 - val_acc: 0.8558\n",
            "Epoch 2544/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2990 - acc: 0.8777 - val_loss: 0.4319 - val_acc: 0.8578\n",
            "Epoch 2545/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3059 - acc: 0.8750 - val_loss: 0.4319 - val_acc: 0.8562\n",
            "Epoch 2546/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2952 - acc: 0.8840 - val_loss: 0.4289 - val_acc: 0.8562\n",
            "Epoch 2547/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2991 - acc: 0.8795 - val_loss: 0.4291 - val_acc: 0.8552\n",
            "Epoch 2548/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2927 - acc: 0.8805 - val_loss: 0.4293 - val_acc: 0.8538\n",
            "Epoch 2549/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3000 - acc: 0.8790 - val_loss: 0.4309 - val_acc: 0.8535\n",
            "Epoch 2550/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8786 - val_loss: 0.4338 - val_acc: 0.8528\n",
            "Epoch 2551/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2911 - acc: 0.8811 - val_loss: 0.4327 - val_acc: 0.8548\n",
            "Epoch 2552/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3045 - acc: 0.8776 - val_loss: 0.4299 - val_acc: 0.8552\n",
            "Epoch 2553/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3019 - acc: 0.8790 - val_loss: 0.4321 - val_acc: 0.8535\n",
            "Epoch 2554/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.8762 - val_loss: 0.4328 - val_acc: 0.8522\n",
            "Epoch 2555/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3034 - acc: 0.8760 - val_loss: 0.4278 - val_acc: 0.8571\n",
            "Epoch 2556/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3000 - acc: 0.8771 - val_loss: 0.4259 - val_acc: 0.8565\n",
            "Epoch 2557/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3095 - acc: 0.8740 - val_loss: 0.4286 - val_acc: 0.8565\n",
            "Epoch 2558/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.8747 - val_loss: 0.4311 - val_acc: 0.8558\n",
            "Epoch 2559/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3042 - acc: 0.8788 - val_loss: 0.4335 - val_acc: 0.8555\n",
            "Epoch 2560/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8767 - val_loss: 0.4366 - val_acc: 0.8538\n",
            "Epoch 2561/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2974 - acc: 0.8793 - val_loss: 0.4399 - val_acc: 0.8545\n",
            "Epoch 2562/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2982 - acc: 0.8781 - val_loss: 0.4405 - val_acc: 0.8555\n",
            "Epoch 2563/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3128 - acc: 0.8760 - val_loss: 0.4366 - val_acc: 0.8528\n",
            "Epoch 2564/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2963 - acc: 0.8816 - val_loss: 0.4338 - val_acc: 0.8552\n",
            "Epoch 2565/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3016 - acc: 0.8767 - val_loss: 0.4334 - val_acc: 0.8575\n",
            "Epoch 2566/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3059 - acc: 0.8778 - val_loss: 0.4329 - val_acc: 0.8545\n",
            "Epoch 2567/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8779 - val_loss: 0.4351 - val_acc: 0.8538\n",
            "Epoch 2568/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3003 - acc: 0.8795 - val_loss: 0.4405 - val_acc: 0.8542\n",
            "Epoch 2569/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2974 - acc: 0.8785 - val_loss: 0.4424 - val_acc: 0.8558\n",
            "Epoch 2570/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8770 - val_loss: 0.4413 - val_acc: 0.8565\n",
            "Epoch 2571/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.8784 - val_loss: 0.4365 - val_acc: 0.8568\n",
            "Epoch 2572/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3066 - acc: 0.8763 - val_loss: 0.4345 - val_acc: 0.8581\n",
            "Epoch 2573/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3009 - acc: 0.8798 - val_loss: 0.4348 - val_acc: 0.8535\n",
            "Epoch 2574/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8776 - val_loss: 0.4310 - val_acc: 0.8568\n",
            "Epoch 2575/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3067 - acc: 0.8800 - val_loss: 0.4287 - val_acc: 0.8555\n",
            "Epoch 2576/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8781 - val_loss: 0.4287 - val_acc: 0.8552\n",
            "Epoch 2577/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3084 - acc: 0.8743 - val_loss: 0.4295 - val_acc: 0.8542\n",
            "Epoch 2578/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3010 - acc: 0.8806 - val_loss: 0.4311 - val_acc: 0.8585\n",
            "Epoch 2579/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2977 - acc: 0.8791 - val_loss: 0.4307 - val_acc: 0.8555\n",
            "Epoch 2580/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8786 - val_loss: 0.4314 - val_acc: 0.8548\n",
            "Epoch 2581/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3054 - acc: 0.8774 - val_loss: 0.4330 - val_acc: 0.8558\n",
            "Epoch 2582/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.8776 - val_loss: 0.4358 - val_acc: 0.8548\n",
            "Epoch 2583/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3011 - acc: 0.8772 - val_loss: 0.4331 - val_acc: 0.8542\n",
            "Epoch 2584/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.8757 - val_loss: 0.4282 - val_acc: 0.8555\n",
            "Epoch 2585/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8787 - val_loss: 0.4257 - val_acc: 0.8575\n",
            "Epoch 2586/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3051 - acc: 0.8736 - val_loss: 0.4275 - val_acc: 0.8581\n",
            "Epoch 2587/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3050 - acc: 0.8773 - val_loss: 0.4314 - val_acc: 0.8565\n",
            "Epoch 2588/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3003 - acc: 0.8779 - val_loss: 0.4357 - val_acc: 0.8552\n",
            "Epoch 2589/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2912 - acc: 0.8809 - val_loss: 0.4355 - val_acc: 0.8562\n",
            "Epoch 2590/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3020 - acc: 0.8791 - val_loss: 0.4304 - val_acc: 0.8545\n",
            "Epoch 2591/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3026 - acc: 0.8766 - val_loss: 0.4273 - val_acc: 0.8555\n",
            "Epoch 2592/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2990 - acc: 0.8767 - val_loss: 0.4277 - val_acc: 0.8552\n",
            "Epoch 2593/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2998 - acc: 0.8769 - val_loss: 0.4304 - val_acc: 0.8555\n",
            "Epoch 2594/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.3005 - acc: 0.8778 - val_loss: 0.4344 - val_acc: 0.8581\n",
            "Epoch 2595/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3009 - acc: 0.8814 - val_loss: 0.4368 - val_acc: 0.8581\n",
            "Epoch 2596/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3082 - acc: 0.8763 - val_loss: 0.4356 - val_acc: 0.8555\n",
            "Epoch 2597/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2963 - acc: 0.8814 - val_loss: 0.4320 - val_acc: 0.8542\n",
            "Epoch 2598/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2998 - acc: 0.8802 - val_loss: 0.4277 - val_acc: 0.8578\n",
            "Epoch 2599/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3013 - acc: 0.8819 - val_loss: 0.4267 - val_acc: 0.8565\n",
            "Epoch 2600/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2992 - acc: 0.8807 - val_loss: 0.4271 - val_acc: 0.8568\n",
            "Epoch 2601/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2980 - acc: 0.8786 - val_loss: 0.4320 - val_acc: 0.8571\n",
            "Epoch 2602/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3013 - acc: 0.8769 - val_loss: 0.4410 - val_acc: 0.8542\n",
            "Epoch 2603/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3042 - acc: 0.8803 - val_loss: 0.4441 - val_acc: 0.8535\n",
            "Epoch 2604/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2991 - acc: 0.8782 - val_loss: 0.4436 - val_acc: 0.8548\n",
            "Epoch 2605/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.8799 - val_loss: 0.4381 - val_acc: 0.8532\n",
            "Epoch 2606/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3020 - acc: 0.8808 - val_loss: 0.4356 - val_acc: 0.8519\n",
            "Epoch 2607/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3000 - acc: 0.8800 - val_loss: 0.4326 - val_acc: 0.8495\n",
            "Epoch 2608/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2973 - acc: 0.8785 - val_loss: 0.4315 - val_acc: 0.8555\n",
            "Epoch 2609/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2922 - acc: 0.8826 - val_loss: 0.4358 - val_acc: 0.8568\n",
            "Epoch 2610/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.8765 - val_loss: 0.4406 - val_acc: 0.8542\n",
            "Epoch 2611/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3031 - acc: 0.8786 - val_loss: 0.4397 - val_acc: 0.8538\n",
            "Epoch 2612/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3011 - acc: 0.8786 - val_loss: 0.4368 - val_acc: 0.8545\n",
            "Epoch 2613/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.8783 - val_loss: 0.4346 - val_acc: 0.8552\n",
            "Epoch 2614/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2967 - acc: 0.8837 - val_loss: 0.4360 - val_acc: 0.8532\n",
            "Epoch 2615/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.8776 - val_loss: 0.4374 - val_acc: 0.8528\n",
            "Epoch 2616/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2973 - acc: 0.8792 - val_loss: 0.4391 - val_acc: 0.8528\n",
            "Epoch 2617/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3060 - acc: 0.8767 - val_loss: 0.4394 - val_acc: 0.8522\n",
            "Epoch 2618/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2981 - acc: 0.8793 - val_loss: 0.4370 - val_acc: 0.8555\n",
            "Epoch 2619/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2998 - acc: 0.8800 - val_loss: 0.4362 - val_acc: 0.8552\n",
            "Epoch 2620/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3061 - acc: 0.8768 - val_loss: 0.4365 - val_acc: 0.8548\n",
            "Epoch 2621/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.8773 - val_loss: 0.4378 - val_acc: 0.8555\n",
            "Epoch 2622/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2929 - acc: 0.8804 - val_loss: 0.4385 - val_acc: 0.8552\n",
            "Epoch 2623/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2929 - acc: 0.8808 - val_loss: 0.4385 - val_acc: 0.8542\n",
            "Epoch 2624/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3001 - acc: 0.8807 - val_loss: 0.4355 - val_acc: 0.8571\n",
            "Epoch 2625/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2934 - acc: 0.8819 - val_loss: 0.4347 - val_acc: 0.8562\n",
            "Epoch 2626/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2987 - acc: 0.8804 - val_loss: 0.4362 - val_acc: 0.8545\n",
            "Epoch 2627/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.8783 - val_loss: 0.4369 - val_acc: 0.8535\n",
            "Epoch 2628/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2967 - acc: 0.8806 - val_loss: 0.4366 - val_acc: 0.8502\n",
            "Epoch 2629/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2976 - acc: 0.8817 - val_loss: 0.4354 - val_acc: 0.8505\n",
            "Epoch 2630/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2959 - acc: 0.8805 - val_loss: 0.4352 - val_acc: 0.8542\n",
            "Epoch 2631/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2992 - acc: 0.8839 - val_loss: 0.4341 - val_acc: 0.8581\n",
            "Epoch 2632/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.3018 - acc: 0.8762 - val_loss: 0.4362 - val_acc: 0.8585\n",
            "Epoch 2633/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2996 - acc: 0.8788 - val_loss: 0.4317 - val_acc: 0.8568\n",
            "Epoch 2634/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2936 - acc: 0.8810 - val_loss: 0.4296 - val_acc: 0.8588\n",
            "Epoch 2635/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8768 - val_loss: 0.4305 - val_acc: 0.8555\n",
            "Epoch 2636/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3004 - acc: 0.8771 - val_loss: 0.4333 - val_acc: 0.8552\n",
            "Epoch 2637/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3010 - acc: 0.8772 - val_loss: 0.4396 - val_acc: 0.8552\n",
            "Epoch 2638/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2967 - acc: 0.8790 - val_loss: 0.4438 - val_acc: 0.8558\n",
            "Epoch 2639/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3023 - acc: 0.8791 - val_loss: 0.4409 - val_acc: 0.8562\n",
            "Epoch 2640/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2991 - acc: 0.8810 - val_loss: 0.4357 - val_acc: 0.8565\n",
            "Epoch 2641/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2990 - acc: 0.8781 - val_loss: 0.4337 - val_acc: 0.8568\n",
            "Epoch 2642/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.8767 - val_loss: 0.4343 - val_acc: 0.8571\n",
            "Epoch 2643/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2991 - acc: 0.8779 - val_loss: 0.4385 - val_acc: 0.8552\n",
            "Epoch 2644/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2951 - acc: 0.8800 - val_loss: 0.4414 - val_acc: 0.8532\n",
            "Epoch 2645/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3103 - acc: 0.8761 - val_loss: 0.4385 - val_acc: 0.8558\n",
            "Epoch 2646/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2988 - acc: 0.8771 - val_loss: 0.4388 - val_acc: 0.8581\n",
            "Epoch 2647/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2913 - acc: 0.8803 - val_loss: 0.4384 - val_acc: 0.8588\n",
            "Epoch 2648/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2933 - acc: 0.8846 - val_loss: 0.4371 - val_acc: 0.8575\n",
            "Epoch 2649/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2991 - acc: 0.8810 - val_loss: 0.4345 - val_acc: 0.8552\n",
            "Epoch 2650/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2985 - acc: 0.8829 - val_loss: 0.4356 - val_acc: 0.8542\n",
            "Epoch 2651/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.8813 - val_loss: 0.4337 - val_acc: 0.8555\n",
            "Epoch 2652/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3085 - acc: 0.8780 - val_loss: 0.4330 - val_acc: 0.8591\n",
            "Epoch 2653/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.8791 - val_loss: 0.4368 - val_acc: 0.8591\n",
            "Epoch 2654/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2992 - acc: 0.8812 - val_loss: 0.4409 - val_acc: 0.8581\n",
            "Epoch 2655/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2997 - acc: 0.8752 - val_loss: 0.4401 - val_acc: 0.8562\n",
            "Epoch 2656/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2958 - acc: 0.8820 - val_loss: 0.4360 - val_acc: 0.8585\n",
            "Epoch 2657/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2992 - acc: 0.8786 - val_loss: 0.4360 - val_acc: 0.8578\n",
            "Epoch 2658/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2969 - acc: 0.8813 - val_loss: 0.4376 - val_acc: 0.8595\n",
            "Epoch 2659/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8780 - val_loss: 0.4395 - val_acc: 0.8571\n",
            "Epoch 2660/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2911 - acc: 0.8791 - val_loss: 0.4429 - val_acc: 0.8548\n",
            "Epoch 2661/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3034 - acc: 0.8795 - val_loss: 0.4447 - val_acc: 0.8548\n",
            "Epoch 2662/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.8773 - val_loss: 0.4399 - val_acc: 0.8568\n",
            "Epoch 2663/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3018 - acc: 0.8760 - val_loss: 0.4374 - val_acc: 0.8591\n",
            "Epoch 2664/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2976 - acc: 0.8786 - val_loss: 0.4392 - val_acc: 0.8578\n",
            "Epoch 2665/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2965 - acc: 0.8794 - val_loss: 0.4412 - val_acc: 0.8588\n",
            "Epoch 2666/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.8803 - val_loss: 0.4389 - val_acc: 0.8585\n",
            "Epoch 2667/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2974 - acc: 0.8806 - val_loss: 0.4351 - val_acc: 0.8581\n",
            "Epoch 2668/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2910 - acc: 0.8821 - val_loss: 0.4324 - val_acc: 0.8565\n",
            "Epoch 2669/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3013 - acc: 0.8791 - val_loss: 0.4340 - val_acc: 0.8568\n",
            "Epoch 2670/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2971 - acc: 0.8827 - val_loss: 0.4382 - val_acc: 0.8555\n",
            "Epoch 2671/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.8754 - val_loss: 0.4439 - val_acc: 0.8528\n",
            "Epoch 2672/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2973 - acc: 0.8781 - val_loss: 0.4466 - val_acc: 0.8522\n",
            "Epoch 2673/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2936 - acc: 0.8814 - val_loss: 0.4446 - val_acc: 0.8552\n",
            "Epoch 2674/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.8811 - val_loss: 0.4401 - val_acc: 0.8585\n",
            "Epoch 2675/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2978 - acc: 0.8800 - val_loss: 0.4350 - val_acc: 0.8588\n",
            "Epoch 2676/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8776 - val_loss: 0.4354 - val_acc: 0.8565\n",
            "Epoch 2677/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2966 - acc: 0.8783 - val_loss: 0.4375 - val_acc: 0.8581\n",
            "Epoch 2678/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3043 - acc: 0.8776 - val_loss: 0.4395 - val_acc: 0.8568\n",
            "Epoch 2679/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2987 - acc: 0.8831 - val_loss: 0.4401 - val_acc: 0.8578\n",
            "Epoch 2680/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2979 - acc: 0.8786 - val_loss: 0.4374 - val_acc: 0.8571\n",
            "Epoch 2681/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2984 - acc: 0.8789 - val_loss: 0.4323 - val_acc: 0.8538\n",
            "Epoch 2682/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.8757 - val_loss: 0.4310 - val_acc: 0.8548\n",
            "Epoch 2683/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.8767 - val_loss: 0.4337 - val_acc: 0.8555\n",
            "Epoch 2684/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2999 - acc: 0.8807 - val_loss: 0.4401 - val_acc: 0.8552\n",
            "Epoch 2685/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2977 - acc: 0.8786 - val_loss: 0.4444 - val_acc: 0.8552\n",
            "Epoch 2686/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3065 - acc: 0.8778 - val_loss: 0.4435 - val_acc: 0.8571\n",
            "Epoch 2687/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.8802 - val_loss: 0.4409 - val_acc: 0.8558\n",
            "Epoch 2688/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2984 - acc: 0.8810 - val_loss: 0.4365 - val_acc: 0.8565\n",
            "Epoch 2689/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2908 - acc: 0.8793 - val_loss: 0.4323 - val_acc: 0.8591\n",
            "Epoch 2690/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2961 - acc: 0.8830 - val_loss: 0.4329 - val_acc: 0.8578\n",
            "Epoch 2691/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2982 - acc: 0.8805 - val_loss: 0.4350 - val_acc: 0.8591\n",
            "Epoch 2692/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2988 - acc: 0.8779 - val_loss: 0.4379 - val_acc: 0.8578\n",
            "Epoch 2693/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2927 - acc: 0.8832 - val_loss: 0.4392 - val_acc: 0.8568\n",
            "Epoch 2694/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2954 - acc: 0.8788 - val_loss: 0.4402 - val_acc: 0.8532\n",
            "Epoch 2695/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2936 - acc: 0.8826 - val_loss: 0.4394 - val_acc: 0.8538\n",
            "Epoch 2696/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2966 - acc: 0.8824 - val_loss: 0.4437 - val_acc: 0.8558\n",
            "Epoch 2697/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3038 - acc: 0.8772 - val_loss: 0.4460 - val_acc: 0.8555\n",
            "Epoch 2698/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2953 - acc: 0.8822 - val_loss: 0.4453 - val_acc: 0.8548\n",
            "Epoch 2699/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2959 - acc: 0.8782 - val_loss: 0.4411 - val_acc: 0.8565\n",
            "Epoch 2700/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2989 - acc: 0.8820 - val_loss: 0.4375 - val_acc: 0.8565\n",
            "Epoch 2701/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2940 - acc: 0.8823 - val_loss: 0.4346 - val_acc: 0.8562\n",
            "Epoch 2702/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2956 - acc: 0.8810 - val_loss: 0.4346 - val_acc: 0.8565\n",
            "Epoch 2703/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2964 - acc: 0.8815 - val_loss: 0.4376 - val_acc: 0.8552\n",
            "Epoch 2704/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2898 - acc: 0.8803 - val_loss: 0.4414 - val_acc: 0.8532\n",
            "Epoch 2705/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2967 - acc: 0.8842 - val_loss: 0.4419 - val_acc: 0.8528\n",
            "Epoch 2706/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2929 - acc: 0.8795 - val_loss: 0.4422 - val_acc: 0.8542\n",
            "Epoch 2707/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2964 - acc: 0.8819 - val_loss: 0.4448 - val_acc: 0.8542\n",
            "Epoch 2708/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2927 - acc: 0.8798 - val_loss: 0.4447 - val_acc: 0.8548\n",
            "Epoch 2709/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.8807 - val_loss: 0.4410 - val_acc: 0.8575\n",
            "Epoch 2710/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2943 - acc: 0.8764 - val_loss: 0.4391 - val_acc: 0.8591\n",
            "Epoch 2711/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2964 - acc: 0.8797 - val_loss: 0.4371 - val_acc: 0.8588\n",
            "Epoch 2712/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2984 - acc: 0.8783 - val_loss: 0.4354 - val_acc: 0.8568\n",
            "Epoch 2713/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2982 - acc: 0.8818 - val_loss: 0.4368 - val_acc: 0.8555\n",
            "Epoch 2714/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2971 - acc: 0.8805 - val_loss: 0.4386 - val_acc: 0.8555\n",
            "Epoch 2715/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2892 - acc: 0.8857 - val_loss: 0.4378 - val_acc: 0.8538\n",
            "Epoch 2716/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.8759 - val_loss: 0.4359 - val_acc: 0.8552\n",
            "Epoch 2717/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2996 - acc: 0.8784 - val_loss: 0.4336 - val_acc: 0.8565\n",
            "Epoch 2718/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2927 - acc: 0.8808 - val_loss: 0.4336 - val_acc: 0.8552\n",
            "Epoch 2719/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2886 - acc: 0.8837 - val_loss: 0.4351 - val_acc: 0.8548\n",
            "Epoch 2720/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2972 - acc: 0.8794 - val_loss: 0.4363 - val_acc: 0.8565\n",
            "Epoch 2721/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3028 - acc: 0.8781 - val_loss: 0.4367 - val_acc: 0.8568\n",
            "Epoch 2722/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2999 - acc: 0.8794 - val_loss: 0.4374 - val_acc: 0.8552\n",
            "Epoch 2723/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.8811 - val_loss: 0.4389 - val_acc: 0.8535\n",
            "Epoch 2724/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2979 - acc: 0.8789 - val_loss: 0.4401 - val_acc: 0.8555\n",
            "Epoch 2725/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3013 - acc: 0.8783 - val_loss: 0.4411 - val_acc: 0.8575\n",
            "Epoch 2726/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2960 - acc: 0.8771 - val_loss: 0.4415 - val_acc: 0.8595\n",
            "Epoch 2727/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2976 - acc: 0.8814 - val_loss: 0.4391 - val_acc: 0.8608\n",
            "Epoch 2728/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2941 - acc: 0.8790 - val_loss: 0.4373 - val_acc: 0.8571\n",
            "Epoch 2729/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2973 - acc: 0.8806 - val_loss: 0.4354 - val_acc: 0.8581\n",
            "Epoch 2730/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3108 - acc: 0.8749 - val_loss: 0.4353 - val_acc: 0.8535\n",
            "Epoch 2731/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3015 - acc: 0.8779 - val_loss: 0.4324 - val_acc: 0.8558\n",
            "Epoch 2732/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.2996 - acc: 0.8795 - val_loss: 0.4324 - val_acc: 0.8568\n",
            "Epoch 2733/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3015 - acc: 0.8791 - val_loss: 0.4345 - val_acc: 0.8571\n",
            "Epoch 2734/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.8831 - val_loss: 0.4363 - val_acc: 0.8555\n",
            "Epoch 2735/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2946 - acc: 0.8796 - val_loss: 0.4349 - val_acc: 0.8552\n",
            "Epoch 2736/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2946 - acc: 0.8800 - val_loss: 0.4369 - val_acc: 0.8555\n",
            "Epoch 2737/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2938 - acc: 0.8822 - val_loss: 0.4364 - val_acc: 0.8552\n",
            "Epoch 2738/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2961 - acc: 0.8800 - val_loss: 0.4344 - val_acc: 0.8538\n",
            "Epoch 2739/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2910 - acc: 0.8845 - val_loss: 0.4334 - val_acc: 0.8528\n",
            "Epoch 2740/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3017 - acc: 0.8768 - val_loss: 0.4335 - val_acc: 0.8528\n",
            "Epoch 2741/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2914 - acc: 0.8810 - val_loss: 0.4366 - val_acc: 0.8535\n",
            "Epoch 2742/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.8777 - val_loss: 0.4418 - val_acc: 0.8542\n",
            "Epoch 2743/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8829 - val_loss: 0.4441 - val_acc: 0.8535\n",
            "Epoch 2744/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2920 - acc: 0.8798 - val_loss: 0.4424 - val_acc: 0.8552\n",
            "Epoch 2745/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2947 - acc: 0.8804 - val_loss: 0.4379 - val_acc: 0.8525\n",
            "Epoch 2746/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2982 - acc: 0.8794 - val_loss: 0.4350 - val_acc: 0.8519\n",
            "Epoch 2747/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2974 - acc: 0.8848 - val_loss: 0.4357 - val_acc: 0.8532\n",
            "Epoch 2748/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8781 - val_loss: 0.4363 - val_acc: 0.8538\n",
            "Epoch 2749/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2908 - acc: 0.8839 - val_loss: 0.4358 - val_acc: 0.8568\n",
            "Epoch 2750/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2993 - acc: 0.8812 - val_loss: 0.4358 - val_acc: 0.8545\n",
            "Epoch 2751/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2995 - acc: 0.8807 - val_loss: 0.4348 - val_acc: 0.8538\n",
            "Epoch 2752/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3003 - acc: 0.8783 - val_loss: 0.4311 - val_acc: 0.8552\n",
            "Epoch 2753/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2918 - acc: 0.8825 - val_loss: 0.4304 - val_acc: 0.8535\n",
            "Epoch 2754/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2959 - acc: 0.8771 - val_loss: 0.4342 - val_acc: 0.8535\n",
            "Epoch 2755/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3013 - acc: 0.8791 - val_loss: 0.4406 - val_acc: 0.8502\n",
            "Epoch 2756/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2936 - acc: 0.8811 - val_loss: 0.4448 - val_acc: 0.8509\n",
            "Epoch 2757/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3019 - acc: 0.8803 - val_loss: 0.4457 - val_acc: 0.8542\n",
            "Epoch 2758/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.8814 - val_loss: 0.4430 - val_acc: 0.8555\n",
            "Epoch 2759/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2984 - acc: 0.8825 - val_loss: 0.4415 - val_acc: 0.8578\n",
            "Epoch 2760/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8806 - val_loss: 0.4395 - val_acc: 0.8562\n",
            "Epoch 2761/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2933 - acc: 0.8817 - val_loss: 0.4373 - val_acc: 0.8562\n",
            "Epoch 2762/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2988 - acc: 0.8799 - val_loss: 0.4389 - val_acc: 0.8552\n",
            "Epoch 2763/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2986 - acc: 0.8769 - val_loss: 0.4409 - val_acc: 0.8545\n",
            "Epoch 2764/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2884 - acc: 0.8838 - val_loss: 0.4401 - val_acc: 0.8552\n",
            "Epoch 2765/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2973 - acc: 0.8838 - val_loss: 0.4370 - val_acc: 0.8601\n",
            "Epoch 2766/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2965 - acc: 0.8829 - val_loss: 0.4368 - val_acc: 0.8588\n",
            "Epoch 2767/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.8848 - val_loss: 0.4388 - val_acc: 0.8591\n",
            "Epoch 2768/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.8820 - val_loss: 0.4401 - val_acc: 0.8601\n",
            "Epoch 2769/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2926 - acc: 0.8846 - val_loss: 0.4407 - val_acc: 0.8575\n",
            "Epoch 2770/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2967 - acc: 0.8791 - val_loss: 0.4407 - val_acc: 0.8562\n",
            "Epoch 2771/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2867 - acc: 0.8824 - val_loss: 0.4383 - val_acc: 0.8562\n",
            "Epoch 2772/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2941 - acc: 0.8781 - val_loss: 0.4380 - val_acc: 0.8571\n",
            "Epoch 2773/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2940 - acc: 0.8827 - val_loss: 0.4392 - val_acc: 0.8581\n",
            "Epoch 2774/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.8800 - val_loss: 0.4395 - val_acc: 0.8565\n",
            "Epoch 2775/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2903 - acc: 0.8825 - val_loss: 0.4389 - val_acc: 0.8555\n",
            "Epoch 2776/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2934 - acc: 0.8808 - val_loss: 0.4368 - val_acc: 0.8542\n",
            "Epoch 2777/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2924 - acc: 0.8804 - val_loss: 0.4343 - val_acc: 0.8565\n",
            "Epoch 2778/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2960 - acc: 0.8818 - val_loss: 0.4341 - val_acc: 0.8581\n",
            "Epoch 2779/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2971 - acc: 0.8823 - val_loss: 0.4349 - val_acc: 0.8558\n",
            "Epoch 2780/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2956 - acc: 0.8827 - val_loss: 0.4404 - val_acc: 0.8545\n",
            "Epoch 2781/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2893 - acc: 0.8838 - val_loss: 0.4445 - val_acc: 0.8548\n",
            "Epoch 2782/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2947 - acc: 0.8821 - val_loss: 0.4457 - val_acc: 0.8555\n",
            "Epoch 2783/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.8856 - val_loss: 0.4463 - val_acc: 0.8548\n",
            "Epoch 2784/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8822 - val_loss: 0.4437 - val_acc: 0.8565\n",
            "Epoch 2785/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2967 - acc: 0.8805 - val_loss: 0.4392 - val_acc: 0.8538\n",
            "Epoch 2786/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2938 - acc: 0.8794 - val_loss: 0.4337 - val_acc: 0.8571\n",
            "Epoch 2787/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2901 - acc: 0.8805 - val_loss: 0.4304 - val_acc: 0.8552\n",
            "Epoch 2788/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.2861 - acc: 0.8815 - val_loss: 0.4312 - val_acc: 0.8528\n",
            "Epoch 2789/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2986 - acc: 0.8804 - val_loss: 0.4314 - val_acc: 0.8568\n",
            "Epoch 2790/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2964 - acc: 0.8794 - val_loss: 0.4321 - val_acc: 0.8578\n",
            "Epoch 2791/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2972 - acc: 0.8799 - val_loss: 0.4324 - val_acc: 0.8565\n",
            "Epoch 2792/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2965 - acc: 0.8797 - val_loss: 0.4340 - val_acc: 0.8538\n",
            "Epoch 2793/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.8826 - val_loss: 0.4382 - val_acc: 0.8519\n",
            "Epoch 2794/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2948 - acc: 0.8841 - val_loss: 0.4408 - val_acc: 0.8512\n",
            "Epoch 2795/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2927 - acc: 0.8792 - val_loss: 0.4382 - val_acc: 0.8509\n",
            "Epoch 2796/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2912 - acc: 0.8826 - val_loss: 0.4352 - val_acc: 0.8548\n",
            "Epoch 2797/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2895 - acc: 0.8804 - val_loss: 0.4361 - val_acc: 0.8512\n",
            "Epoch 2798/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2993 - acc: 0.8805 - val_loss: 0.4328 - val_acc: 0.8528\n",
            "Epoch 2799/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2975 - acc: 0.8822 - val_loss: 0.4318 - val_acc: 0.8538\n",
            "Epoch 2800/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2961 - acc: 0.8804 - val_loss: 0.4367 - val_acc: 0.8571\n",
            "Epoch 2801/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2952 - acc: 0.8819 - val_loss: 0.4429 - val_acc: 0.8568\n",
            "Epoch 2802/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.8781 - val_loss: 0.4454 - val_acc: 0.8535\n",
            "Epoch 2803/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.3002 - acc: 0.8791 - val_loss: 0.4402 - val_acc: 0.8552\n",
            "Epoch 2804/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2961 - acc: 0.8806 - val_loss: 0.4343 - val_acc: 0.8535\n",
            "Epoch 2805/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2935 - acc: 0.8802 - val_loss: 0.4339 - val_acc: 0.8522\n",
            "Epoch 2806/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2940 - acc: 0.8824 - val_loss: 0.4350 - val_acc: 0.8532\n",
            "Epoch 2807/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2927 - acc: 0.8844 - val_loss: 0.4352 - val_acc: 0.8528\n",
            "Epoch 2808/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2878 - acc: 0.8850 - val_loss: 0.4372 - val_acc: 0.8548\n",
            "Epoch 2809/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2972 - acc: 0.8851 - val_loss: 0.4363 - val_acc: 0.8545\n",
            "Epoch 2810/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.8791 - val_loss: 0.4334 - val_acc: 0.8552\n",
            "Epoch 2811/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.8805 - val_loss: 0.4360 - val_acc: 0.8565\n",
            "Epoch 2812/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2832 - acc: 0.8874 - val_loss: 0.4387 - val_acc: 0.8598\n",
            "Epoch 2813/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3029 - acc: 0.8772 - val_loss: 0.4391 - val_acc: 0.8552\n",
            "Epoch 2814/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2933 - acc: 0.8784 - val_loss: 0.4403 - val_acc: 0.8545\n",
            "Epoch 2815/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8793 - val_loss: 0.4430 - val_acc: 0.8548\n",
            "Epoch 2816/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.8807 - val_loss: 0.4460 - val_acc: 0.8538\n",
            "Epoch 2817/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2995 - acc: 0.8838 - val_loss: 0.4458 - val_acc: 0.8552\n",
            "Epoch 2818/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2874 - acc: 0.8837 - val_loss: 0.4432 - val_acc: 0.8558\n",
            "Epoch 2819/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2885 - acc: 0.8845 - val_loss: 0.4424 - val_acc: 0.8568\n",
            "Epoch 2820/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2996 - acc: 0.8831 - val_loss: 0.4397 - val_acc: 0.8575\n",
            "Epoch 2821/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.8780 - val_loss: 0.4375 - val_acc: 0.8581\n",
            "Epoch 2822/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2971 - acc: 0.8791 - val_loss: 0.4342 - val_acc: 0.8581\n",
            "Epoch 2823/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2975 - acc: 0.8810 - val_loss: 0.4348 - val_acc: 0.8575\n",
            "Epoch 2824/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2993 - acc: 0.8800 - val_loss: 0.4368 - val_acc: 0.8562\n",
            "Epoch 2825/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2981 - acc: 0.8790 - val_loss: 0.4379 - val_acc: 0.8562\n",
            "Epoch 2826/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2951 - acc: 0.8822 - val_loss: 0.4357 - val_acc: 0.8581\n",
            "Epoch 2827/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2935 - acc: 0.8837 - val_loss: 0.4333 - val_acc: 0.8565\n",
            "Epoch 2828/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.8809 - val_loss: 0.4293 - val_acc: 0.8565\n",
            "Epoch 2829/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3001 - acc: 0.8761 - val_loss: 0.4288 - val_acc: 0.8604\n",
            "Epoch 2830/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2878 - acc: 0.8844 - val_loss: 0.4314 - val_acc: 0.8604\n",
            "Epoch 2831/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2946 - acc: 0.8824 - val_loss: 0.4353 - val_acc: 0.8618\n",
            "Epoch 2832/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2923 - acc: 0.8837 - val_loss: 0.4385 - val_acc: 0.8575\n",
            "Epoch 2833/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2868 - acc: 0.8833 - val_loss: 0.4397 - val_acc: 0.8571\n",
            "Epoch 2834/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2870 - acc: 0.8848 - val_loss: 0.4386 - val_acc: 0.8581\n",
            "Epoch 2835/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2909 - acc: 0.8819 - val_loss: 0.4382 - val_acc: 0.8552\n",
            "Epoch 2836/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.8846 - val_loss: 0.4374 - val_acc: 0.8545\n",
            "Epoch 2837/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2922 - acc: 0.8810 - val_loss: 0.4358 - val_acc: 0.8535\n",
            "Epoch 2838/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2896 - acc: 0.8808 - val_loss: 0.4351 - val_acc: 0.8548\n",
            "Epoch 2839/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2906 - acc: 0.8808 - val_loss: 0.4358 - val_acc: 0.8555\n",
            "Epoch 2840/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2896 - acc: 0.8832 - val_loss: 0.4397 - val_acc: 0.8558\n",
            "Epoch 2841/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3005 - acc: 0.8787 - val_loss: 0.4427 - val_acc: 0.8538\n",
            "Epoch 2842/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2942 - acc: 0.8798 - val_loss: 0.4448 - val_acc: 0.8535\n",
            "Epoch 2843/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2902 - acc: 0.8835 - val_loss: 0.4448 - val_acc: 0.8525\n",
            "Epoch 2844/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2958 - acc: 0.8819 - val_loss: 0.4421 - val_acc: 0.8538\n",
            "Epoch 2845/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2875 - acc: 0.8830 - val_loss: 0.4389 - val_acc: 0.8565\n",
            "Epoch 2846/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2870 - acc: 0.8840 - val_loss: 0.4366 - val_acc: 0.8552\n",
            "Epoch 2847/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2873 - acc: 0.8830 - val_loss: 0.4381 - val_acc: 0.8568\n",
            "Epoch 2848/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2875 - acc: 0.8848 - val_loss: 0.4420 - val_acc: 0.8571\n",
            "Epoch 2849/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2909 - acc: 0.8815 - val_loss: 0.4447 - val_acc: 0.8585\n",
            "Epoch 2850/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2932 - acc: 0.8847 - val_loss: 0.4418 - val_acc: 0.8565\n",
            "Epoch 2851/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.8812 - val_loss: 0.4373 - val_acc: 0.8568\n",
            "Epoch 2852/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2889 - acc: 0.8854 - val_loss: 0.4362 - val_acc: 0.8532\n",
            "Epoch 2853/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2963 - acc: 0.8824 - val_loss: 0.4388 - val_acc: 0.8532\n",
            "Epoch 2854/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2864 - acc: 0.8872 - val_loss: 0.4430 - val_acc: 0.8542\n",
            "Epoch 2855/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2981 - acc: 0.8803 - val_loss: 0.4441 - val_acc: 0.8562\n",
            "Epoch 2856/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2918 - acc: 0.8824 - val_loss: 0.4424 - val_acc: 0.8571\n",
            "Epoch 2857/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2898 - acc: 0.8846 - val_loss: 0.4371 - val_acc: 0.8591\n",
            "Epoch 2858/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2969 - acc: 0.8814 - val_loss: 0.4333 - val_acc: 0.8555\n",
            "Epoch 2859/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2918 - acc: 0.8836 - val_loss: 0.4349 - val_acc: 0.8575\n",
            "Epoch 2860/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2938 - acc: 0.8810 - val_loss: 0.4389 - val_acc: 0.8562\n",
            "Epoch 2861/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.8757 - val_loss: 0.4439 - val_acc: 0.8555\n",
            "Epoch 2862/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2953 - acc: 0.8794 - val_loss: 0.4492 - val_acc: 0.8528\n",
            "Epoch 2863/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2942 - acc: 0.8800 - val_loss: 0.4489 - val_acc: 0.8535\n",
            "Epoch 2864/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2880 - acc: 0.8837 - val_loss: 0.4438 - val_acc: 0.8545\n",
            "Epoch 2865/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2939 - acc: 0.8833 - val_loss: 0.4410 - val_acc: 0.8565\n",
            "Epoch 2866/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2867 - acc: 0.8845 - val_loss: 0.4404 - val_acc: 0.8575\n",
            "Epoch 2867/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2880 - acc: 0.8819 - val_loss: 0.4376 - val_acc: 0.8558\n",
            "Epoch 2868/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2953 - acc: 0.8819 - val_loss: 0.4387 - val_acc: 0.8545\n",
            "Epoch 2869/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2882 - acc: 0.8813 - val_loss: 0.4418 - val_acc: 0.8499\n",
            "Epoch 2870/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2969 - acc: 0.8796 - val_loss: 0.4447 - val_acc: 0.8502\n",
            "Epoch 2871/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2913 - acc: 0.8825 - val_loss: 0.4432 - val_acc: 0.8542\n",
            "Epoch 2872/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2915 - acc: 0.8805 - val_loss: 0.4409 - val_acc: 0.8565\n",
            "Epoch 2873/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2951 - acc: 0.8827 - val_loss: 0.4410 - val_acc: 0.8552\n",
            "Epoch 2874/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2905 - acc: 0.8865 - val_loss: 0.4405 - val_acc: 0.8525\n",
            "Epoch 2875/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2930 - acc: 0.8795 - val_loss: 0.4390 - val_acc: 0.8552\n",
            "Epoch 2876/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.8773 - val_loss: 0.4376 - val_acc: 0.8555\n",
            "Epoch 2877/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2928 - acc: 0.8829 - val_loss: 0.4366 - val_acc: 0.8562\n",
            "Epoch 2878/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2853 - acc: 0.8847 - val_loss: 0.4388 - val_acc: 0.8558\n",
            "Epoch 2879/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8804 - val_loss: 0.4409 - val_acc: 0.8568\n",
            "Epoch 2880/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2976 - acc: 0.8803 - val_loss: 0.4396 - val_acc: 0.8581\n",
            "Epoch 2881/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.4378 - val_acc: 0.8558\n",
            "Epoch 2882/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2819 - acc: 0.8885 - val_loss: 0.4377 - val_acc: 0.8562\n",
            "Epoch 2883/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2965 - acc: 0.8805 - val_loss: 0.4351 - val_acc: 0.8568\n",
            "Epoch 2884/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2928 - acc: 0.8811 - val_loss: 0.4350 - val_acc: 0.8585\n",
            "Epoch 2885/7500\n",
            "12096/12096 [==============================] - 0s 13us/step - loss: 0.2905 - acc: 0.8815 - val_loss: 0.4360 - val_acc: 0.8555\n",
            "Epoch 2886/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2928 - acc: 0.8826 - val_loss: 0.4365 - val_acc: 0.8571\n",
            "Epoch 2887/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.8781 - val_loss: 0.4385 - val_acc: 0.8588\n",
            "Epoch 2888/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2806 - acc: 0.8868 - val_loss: 0.4413 - val_acc: 0.8575\n",
            "Epoch 2889/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2978 - acc: 0.8805 - val_loss: 0.4409 - val_acc: 0.8552\n",
            "Epoch 2890/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2872 - acc: 0.8868 - val_loss: 0.4383 - val_acc: 0.8565\n",
            "Epoch 2891/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2938 - acc: 0.8823 - val_loss: 0.4378 - val_acc: 0.8581\n",
            "Epoch 2892/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2915 - acc: 0.8813 - val_loss: 0.4379 - val_acc: 0.8588\n",
            "Epoch 2893/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2925 - acc: 0.8831 - val_loss: 0.4389 - val_acc: 0.8588\n",
            "Epoch 2894/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2944 - acc: 0.8810 - val_loss: 0.4413 - val_acc: 0.8585\n",
            "Epoch 2895/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2910 - acc: 0.8843 - val_loss: 0.4440 - val_acc: 0.8542\n",
            "Epoch 2896/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2893 - acc: 0.8830 - val_loss: 0.4444 - val_acc: 0.8548\n",
            "Epoch 2897/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2946 - acc: 0.8799 - val_loss: 0.4448 - val_acc: 0.8555\n",
            "Epoch 2898/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2987 - acc: 0.8814 - val_loss: 0.4428 - val_acc: 0.8562\n",
            "Epoch 2899/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2945 - acc: 0.8788 - val_loss: 0.4401 - val_acc: 0.8571\n",
            "Epoch 2900/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2870 - acc: 0.8832 - val_loss: 0.4393 - val_acc: 0.8558\n",
            "Epoch 2901/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2904 - acc: 0.8820 - val_loss: 0.4390 - val_acc: 0.8505\n",
            "Epoch 2902/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2899 - acc: 0.8833 - val_loss: 0.4378 - val_acc: 0.8528\n",
            "Epoch 2903/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.2884 - acc: 0.8844 - val_loss: 0.4377 - val_acc: 0.8525\n",
            "Epoch 2904/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.8802 - val_loss: 0.4371 - val_acc: 0.8522\n",
            "Epoch 2905/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2907 - acc: 0.8796 - val_loss: 0.4369 - val_acc: 0.8542\n",
            "Epoch 2906/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2894 - acc: 0.8819 - val_loss: 0.4398 - val_acc: 0.8562\n",
            "Epoch 2907/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.8815 - val_loss: 0.4409 - val_acc: 0.8568\n",
            "Epoch 2908/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2923 - acc: 0.8821 - val_loss: 0.4403 - val_acc: 0.8552\n",
            "Epoch 2909/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2918 - acc: 0.8828 - val_loss: 0.4396 - val_acc: 0.8542\n",
            "Epoch 2910/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2961 - acc: 0.8804 - val_loss: 0.4377 - val_acc: 0.8525\n",
            "Epoch 2911/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2905 - acc: 0.8861 - val_loss: 0.4332 - val_acc: 0.8548\n",
            "Epoch 2912/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2842 - acc: 0.8863 - val_loss: 0.4328 - val_acc: 0.8555\n",
            "Epoch 2913/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2986 - acc: 0.8812 - val_loss: 0.4365 - val_acc: 0.8535\n",
            "Epoch 2914/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2892 - acc: 0.8819 - val_loss: 0.4403 - val_acc: 0.8522\n",
            "Epoch 2915/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2992 - acc: 0.8805 - val_loss: 0.4402 - val_acc: 0.8519\n",
            "Epoch 2916/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2906 - acc: 0.8839 - val_loss: 0.4429 - val_acc: 0.8512\n",
            "Epoch 2917/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2927 - acc: 0.8830 - val_loss: 0.4416 - val_acc: 0.8505\n",
            "Epoch 2918/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2930 - acc: 0.8791 - val_loss: 0.4360 - val_acc: 0.8535\n",
            "Epoch 2919/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2936 - acc: 0.8814 - val_loss: 0.4341 - val_acc: 0.8562\n",
            "Epoch 2920/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.3007 - acc: 0.8784 - val_loss: 0.4384 - val_acc: 0.8555\n",
            "Epoch 2921/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2870 - acc: 0.8844 - val_loss: 0.4442 - val_acc: 0.8509\n",
            "Epoch 2922/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2903 - acc: 0.8800 - val_loss: 0.4494 - val_acc: 0.8499\n",
            "Epoch 2923/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8829 - val_loss: 0.4543 - val_acc: 0.8519\n",
            "Epoch 2924/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2977 - acc: 0.8798 - val_loss: 0.4494 - val_acc: 0.8532\n",
            "Epoch 2925/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2844 - acc: 0.8865 - val_loss: 0.4394 - val_acc: 0.8512\n",
            "Epoch 2926/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2931 - acc: 0.8816 - val_loss: 0.4361 - val_acc: 0.8505\n",
            "Epoch 2927/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2880 - acc: 0.8853 - val_loss: 0.4366 - val_acc: 0.8492\n",
            "Epoch 2928/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2905 - acc: 0.8843 - val_loss: 0.4415 - val_acc: 0.8509\n",
            "Epoch 2929/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.8805 - val_loss: 0.4468 - val_acc: 0.8528\n",
            "Epoch 2930/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2981 - acc: 0.8832 - val_loss: 0.4516 - val_acc: 0.8542\n",
            "Epoch 2931/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2907 - acc: 0.8833 - val_loss: 0.4503 - val_acc: 0.8548\n",
            "Epoch 2932/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2917 - acc: 0.8830 - val_loss: 0.4472 - val_acc: 0.8545\n",
            "Epoch 2933/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2915 - acc: 0.8829 - val_loss: 0.4420 - val_acc: 0.8525\n",
            "Epoch 2934/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2941 - acc: 0.8793 - val_loss: 0.4385 - val_acc: 0.8525\n",
            "Epoch 2935/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2966 - acc: 0.8800 - val_loss: 0.4365 - val_acc: 0.8525\n",
            "Epoch 2936/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2925 - acc: 0.8819 - val_loss: 0.4360 - val_acc: 0.8552\n",
            "Epoch 2937/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2946 - acc: 0.8817 - val_loss: 0.4382 - val_acc: 0.8562\n",
            "Epoch 2938/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2849 - acc: 0.8864 - val_loss: 0.4388 - val_acc: 0.8562\n",
            "Epoch 2939/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2975 - acc: 0.8802 - val_loss: 0.4369 - val_acc: 0.8581\n",
            "Epoch 2940/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2883 - acc: 0.8829 - val_loss: 0.4358 - val_acc: 0.8568\n",
            "Epoch 2941/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2932 - acc: 0.8843 - val_loss: 0.4368 - val_acc: 0.8548\n",
            "Epoch 2942/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2888 - acc: 0.8847 - val_loss: 0.4378 - val_acc: 0.8542\n",
            "Epoch 2943/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.4408 - val_acc: 0.8538\n",
            "Epoch 2944/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2943 - acc: 0.8822 - val_loss: 0.4448 - val_acc: 0.8532\n",
            "Epoch 2945/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2903 - acc: 0.8814 - val_loss: 0.4485 - val_acc: 0.8545\n",
            "Epoch 2946/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2901 - acc: 0.8843 - val_loss: 0.4471 - val_acc: 0.8512\n",
            "Epoch 2947/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2892 - acc: 0.8858 - val_loss: 0.4406 - val_acc: 0.8548\n",
            "Epoch 2948/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2895 - acc: 0.8814 - val_loss: 0.4365 - val_acc: 0.8558\n",
            "Epoch 2949/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2910 - acc: 0.8862 - val_loss: 0.4354 - val_acc: 0.8555\n",
            "Epoch 2950/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2877 - acc: 0.8873 - val_loss: 0.4369 - val_acc: 0.8542\n",
            "Epoch 2951/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8821 - val_loss: 0.4383 - val_acc: 0.8545\n",
            "Epoch 2952/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2850 - acc: 0.8858 - val_loss: 0.4410 - val_acc: 0.8552\n",
            "Epoch 2953/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2935 - acc: 0.8816 - val_loss: 0.4434 - val_acc: 0.8552\n",
            "Epoch 2954/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2879 - acc: 0.8846 - val_loss: 0.4444 - val_acc: 0.8545\n",
            "Epoch 2955/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8836 - val_loss: 0.4452 - val_acc: 0.8505\n",
            "Epoch 2956/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2882 - acc: 0.8867 - val_loss: 0.4423 - val_acc: 0.8505\n",
            "Epoch 2957/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2906 - acc: 0.8847 - val_loss: 0.4397 - val_acc: 0.8519\n",
            "Epoch 2958/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2869 - acc: 0.8833 - val_loss: 0.4384 - val_acc: 0.8528\n",
            "Epoch 2959/7500\n",
            "12096/12096 [==============================] - 0s 17us/step - loss: 0.2835 - acc: 0.8859 - val_loss: 0.4391 - val_acc: 0.8528\n",
            "Epoch 2960/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2955 - acc: 0.8838 - val_loss: 0.4350 - val_acc: 0.8535\n",
            "Epoch 2961/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2904 - acc: 0.8827 - val_loss: 0.4340 - val_acc: 0.8552\n",
            "Epoch 2962/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2890 - acc: 0.8816 - val_loss: 0.4382 - val_acc: 0.8528\n",
            "Epoch 2963/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2873 - acc: 0.8869 - val_loss: 0.4437 - val_acc: 0.8532\n",
            "Epoch 2964/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2916 - acc: 0.8829 - val_loss: 0.4476 - val_acc: 0.8528\n",
            "Epoch 2965/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2882 - acc: 0.8866 - val_loss: 0.4514 - val_acc: 0.8522\n",
            "Epoch 2966/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2860 - acc: 0.8851 - val_loss: 0.4501 - val_acc: 0.8535\n",
            "Epoch 2967/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2900 - acc: 0.8824 - val_loss: 0.4463 - val_acc: 0.8542\n",
            "Epoch 2968/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2863 - acc: 0.8847 - val_loss: 0.4431 - val_acc: 0.8535\n",
            "Epoch 2969/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2933 - acc: 0.8848 - val_loss: 0.4400 - val_acc: 0.8535\n",
            "Epoch 2970/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2890 - acc: 0.8857 - val_loss: 0.4381 - val_acc: 0.8499\n",
            "Epoch 2971/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2838 - acc: 0.8844 - val_loss: 0.4374 - val_acc: 0.8535\n",
            "Epoch 2972/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2964 - acc: 0.8791 - val_loss: 0.4375 - val_acc: 0.8545\n",
            "Epoch 2973/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2867 - acc: 0.8842 - val_loss: 0.4379 - val_acc: 0.8578\n",
            "Epoch 2974/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2849 - acc: 0.8849 - val_loss: 0.4381 - val_acc: 0.8552\n",
            "Epoch 2975/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2898 - acc: 0.8859 - val_loss: 0.4388 - val_acc: 0.8565\n",
            "Epoch 2976/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2900 - acc: 0.8843 - val_loss: 0.4386 - val_acc: 0.8565\n",
            "Epoch 2977/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2830 - acc: 0.8867 - val_loss: 0.4369 - val_acc: 0.8548\n",
            "Epoch 2978/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2925 - acc: 0.8850 - val_loss: 0.4382 - val_acc: 0.8545\n",
            "Epoch 2979/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2817 - acc: 0.8872 - val_loss: 0.4423 - val_acc: 0.8545\n",
            "Epoch 2980/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2849 - acc: 0.8833 - val_loss: 0.4439 - val_acc: 0.8548\n",
            "Epoch 2981/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2852 - acc: 0.8847 - val_loss: 0.4401 - val_acc: 0.8535\n",
            "Epoch 2982/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2905 - acc: 0.8824 - val_loss: 0.4364 - val_acc: 0.8548\n",
            "Epoch 2983/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2924 - acc: 0.8819 - val_loss: 0.4369 - val_acc: 0.8568\n",
            "Epoch 2984/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2832 - acc: 0.8844 - val_loss: 0.4396 - val_acc: 0.8591\n",
            "Epoch 2985/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2794 - acc: 0.8871 - val_loss: 0.4455 - val_acc: 0.8555\n",
            "Epoch 2986/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2859 - acc: 0.8879 - val_loss: 0.4512 - val_acc: 0.8555\n",
            "Epoch 2987/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2886 - acc: 0.8822 - val_loss: 0.4538 - val_acc: 0.8525\n",
            "Epoch 2988/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2906 - acc: 0.8838 - val_loss: 0.4509 - val_acc: 0.8538\n",
            "Epoch 2989/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.8804 - val_loss: 0.4511 - val_acc: 0.8525\n",
            "Epoch 2990/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2906 - acc: 0.8827 - val_loss: 0.4507 - val_acc: 0.8558\n",
            "Epoch 2991/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2918 - acc: 0.8821 - val_loss: 0.4492 - val_acc: 0.8545\n",
            "Epoch 2992/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2867 - acc: 0.8881 - val_loss: 0.4490 - val_acc: 0.8558\n",
            "Epoch 2993/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2846 - acc: 0.8851 - val_loss: 0.4463 - val_acc: 0.8528\n",
            "Epoch 2994/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2942 - acc: 0.8843 - val_loss: 0.4411 - val_acc: 0.8568\n",
            "Epoch 2995/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2919 - acc: 0.8836 - val_loss: 0.4391 - val_acc: 0.8555\n",
            "Epoch 2996/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2878 - acc: 0.8843 - val_loss: 0.4424 - val_acc: 0.8532\n",
            "Epoch 2997/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2898 - acc: 0.8851 - val_loss: 0.4470 - val_acc: 0.8495\n",
            "Epoch 2998/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2926 - acc: 0.8814 - val_loss: 0.4471 - val_acc: 0.8535\n",
            "Epoch 2999/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2862 - acc: 0.8850 - val_loss: 0.4452 - val_acc: 0.8575\n",
            "Epoch 3000/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2855 - acc: 0.8870 - val_loss: 0.4438 - val_acc: 0.8565\n",
            "Epoch 3001/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2931 - acc: 0.8838 - val_loss: 0.4434 - val_acc: 0.8571\n",
            "Epoch 3002/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2932 - acc: 0.8824 - val_loss: 0.4414 - val_acc: 0.8555\n",
            "Epoch 3003/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2970 - acc: 0.8797 - val_loss: 0.4376 - val_acc: 0.8542\n",
            "Epoch 3004/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2894 - acc: 0.8835 - val_loss: 0.4376 - val_acc: 0.8555\n",
            "Epoch 3005/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2937 - acc: 0.8835 - val_loss: 0.4428 - val_acc: 0.8548\n",
            "Epoch 3006/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2892 - acc: 0.8858 - val_loss: 0.4485 - val_acc: 0.8532\n",
            "Epoch 3007/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2876 - acc: 0.8862 - val_loss: 0.4503 - val_acc: 0.8492\n",
            "Epoch 3008/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2823 - acc: 0.8839 - val_loss: 0.4477 - val_acc: 0.8528\n",
            "Epoch 3009/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2837 - acc: 0.8865 - val_loss: 0.4454 - val_acc: 0.8509\n",
            "Epoch 3010/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2851 - acc: 0.8880 - val_loss: 0.4448 - val_acc: 0.8535\n",
            "Epoch 3011/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2930 - acc: 0.8829 - val_loss: 0.4453 - val_acc: 0.8502\n",
            "Epoch 3012/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2779 - acc: 0.8892 - val_loss: 0.4482 - val_acc: 0.8515\n",
            "Epoch 3013/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2888 - acc: 0.8797 - val_loss: 0.4533 - val_acc: 0.8515\n",
            "Epoch 3014/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2935 - acc: 0.8830 - val_loss: 0.4507 - val_acc: 0.8525\n",
            "Epoch 3015/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2829 - acc: 0.8870 - val_loss: 0.4456 - val_acc: 0.8538\n",
            "Epoch 3016/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2872 - acc: 0.8877 - val_loss: 0.4430 - val_acc: 0.8558\n",
            "Epoch 3017/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2839 - acc: 0.8872 - val_loss: 0.4415 - val_acc: 0.8562\n",
            "Epoch 3018/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2867 - acc: 0.8840 - val_loss: 0.4416 - val_acc: 0.8535\n",
            "Epoch 3019/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2954 - acc: 0.8839 - val_loss: 0.4438 - val_acc: 0.8532\n",
            "Epoch 3020/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2805 - acc: 0.8843 - val_loss: 0.4462 - val_acc: 0.8515\n",
            "Epoch 3021/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2914 - acc: 0.8798 - val_loss: 0.4426 - val_acc: 0.8555\n",
            "Epoch 3022/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2836 - acc: 0.8860 - val_loss: 0.4390 - val_acc: 0.8542\n",
            "Epoch 3023/7500\n",
            "12096/12096 [==============================] - 0s 14us/step - loss: 0.2967 - acc: 0.8786 - val_loss: 0.4406 - val_acc: 0.8555\n",
            "Epoch 3024/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2919 - acc: 0.8834 - val_loss: 0.4447 - val_acc: 0.8535\n",
            "Epoch 3025/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8838 - val_loss: 0.4483 - val_acc: 0.8499\n",
            "Epoch 3026/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2921 - acc: 0.8830 - val_loss: 0.4506 - val_acc: 0.8512\n",
            "Epoch 3027/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2844 - acc: 0.8869 - val_loss: 0.4486 - val_acc: 0.8528\n",
            "Epoch 3028/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2951 - acc: 0.8821 - val_loss: 0.4426 - val_acc: 0.8528\n",
            "Epoch 3029/7500\n",
            "12096/12096 [==============================] - 0s 16us/step - loss: 0.2874 - acc: 0.8837 - val_loss: 0.4399 - val_acc: 0.8542\n",
            "Epoch 3030/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2919 - acc: 0.8827 - val_loss: 0.4405 - val_acc: 0.8535\n",
            "Epoch 3031/7500\n",
            "12096/12096 [==============================] - 0s 15us/step - loss: 0.2903 - acc: 0.8823 - val_loss: 0.4432 - val_acc: 0.8552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3GC650b9MuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    #classes = classes[unique_labels(y_true, y_pred)]\n",
        "    classes = [1,2,3,4,5,6,7]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTSttv-8-dmA",
        "colab_type": "text"
      },
      "source": [
        "# View results\n",
        "\n",
        "Confusion matrix and accuracy \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5RxOvqtDahc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_pred = encoder.inverse_transform(np.argmax(model.predict(X_train),axis=1))\n",
        "y_valid_pred = encoder.inverse_transform(np.argmax(model.predict(X_valid),axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTrhhtKO9Q34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "outputId": "e87516cf-8f67-4067-a9b1-01735e4bda76"
      },
      "source": [
        "plot_confusion_matrix(y_train_pred, encoder.inverse_transform(np.argmax(y_train,axis=1)),7,normalize=True)\n",
        "plot_confusion_matrix(y_valid_pred, encoder.inverse_transform(np.argmax(y_valid,axis=1)),7,normalize=True)\n",
        "plot_confusion_matrix(y_valid_pred, encoder.inverse_transform(np.argmax(y_valid,axis=1)),7,normalize=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized confusion matrix\n",
            "Normalized confusion matrix\n",
            "Confusion matrix, without normalization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f58fdf50898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEYCAYAAAAnEYFiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nznGcVPN/skBZXBiwnTkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEYCAYAAAAnEYFiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njMkZDIagxpicwWAIav4fNux42jiO3OMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT0AAAEYCAYAAAAu+iEYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiBoK8LoBUHQVvwHdDQt/qh5SmwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKr49Bxt9e2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ca6c1f1-7b3a-4ae8-8c56-740fb67662e8"
      },
      "source": [
        "#With Adam Best historic, dropout 0.2 \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# summarize history for accuracy\n",
        "print(history2.history.keys())\n",
        "plt.plot(history2.history['acc'],color='orange',label=\"Acc\")\n",
        "plt.plot(history2.history['loss'],color='lightblue',label=\"loss\")\n",
        "plt.legend()\n",
        "plt.title('Data set')\n",
        "plt.ylabel('accuracy/loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim([0.2, 1.00])\n",
        "plt.xlim([0, 3000])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history2.history['val_acc'],color='orange',label=\"Acc\")\n",
        "plt.plot(history2.history['val_loss'],color='lightblue',label=\"loss\")\n",
        "plt.title('Validation set')\n",
        "plt.ylabel('accuracy/loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim([0.2,.6])\n",
        "plt.ylim([0.2, 1.00])\n",
        "plt.xlim([0, 3000])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "validation_result = encoder.inverse_transform(np.argmax(model.predict(X_valid),axis=1))\n",
        "print(classification_report(encoder.inverse_transform(np.argmax(y_valid,axis=1)), validation_result))\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.78      0.74      0.76       439\n",
            "           2       0.76      0.69      0.72       411\n",
            "           3       0.88      0.79      0.83       418\n",
            "           4       0.92      0.97      0.95       438\n",
            "           5       0.86      0.93      0.90       428\n",
            "           6       0.85      0.90      0.87       471\n",
            "           7       0.92      0.95      0.93       419\n",
            "\n",
            "    accuracy                           0.86      3024\n",
            "   macro avg       0.85      0.85      0.85      3024\n",
            "weighted avg       0.85      0.86      0.85      3024\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 100)               4900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 707       \n",
            "=================================================================\n",
            "Total params: 27,007\n",
            "Trainable params: 26,407\n",
            "Non-trainable params: 600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqgfYekw9rb2",
        "colab_type": "text"
      },
      "source": [
        "# Generate submission file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1XNPcu19lQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.predict(X_test_norm)\n",
        "np.argmax(result,axis=1).shape\n",
        "np.sum(result,axis=0)\n",
        "output = pd.DataFrame({'Cover_Type': encoder.inverse_transform(np.argmax(result,axis=1)), 'Id': X_test.index}, columns=['Id','Cover_Type'])\n",
        "\n",
        "output.to_csv('submission_model_dropingcols.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjWY81eBKelK",
        "colab_type": "text"
      },
      "source": [
        "# References and links\n",
        "\n",
        "[Use Colab with kaggle](https://colab.research.google.com/drive/17MsV4f8Bap8y2MU8BcKBNXRdTNE2ftP3)\n"
      ]
    }
  ]
}